\chapter{Conclusion}
This thesis aimed to explore the development and evaluation of a multimodal emotion recognition system that uses facial and audio inputs, with a focus on achieving a balance between speed and accuracy in emotion detection, particularly in real-time robotic applications. The evaluation of different face detection models revealed significant performance differences in various metrics. 

For overall accuracy, YOLO exhibited the best performance, attaining an F1 score of 0.63 in the wider face dataset and 0.94 in the single face dataset. In contrast, Tiny YOLO faced challenges with the more intricate Wider Face data set, which produced an F1 score of 0.46. However, it showed similar performance to YOLO on the simpler single-face data, achieving an F1 score of 0.93. Notably, Tiny YOLO outperformed YOLO in inference times, averaging 25 ms compared to YOLO's 161 ms. This observation was further validated during robotic platform tests, where Tiny YOLO required 697.4 ms and YOLO required 6846.5 ms. Although dlib and the Haar cascade did not achieve the same levels of accuracy as the two YOLO models, they did achieve significant results in terms of inference speeds on the robotic platform, outperforming Tiny YOLO with 216.2 ms and 491.7 ms, respectively. This presents the choice between prioritising speed or accuracy based on the selected face recognition model.

For emotion recognition, ResNet50 demonstrated the highest accuracy, delivering the best results in the confusion matrices, closely followed by MobileNetV2. MobileNetV2 significantly outpaced other models in terms of speed, with 13.6 ms per detection, while ResNet50 required 95.3 ms. These performance differences were evident in the TurtleBot tests, with MobileNetV2 increasing to 124.7 ms and ResNet50 to 1334.3 ms for inference, highlighting the trade-off between speed and accuracy.

When combining models, YOLO+ResNet50 proved to be the best, achieving an overall precision of 59.02\%, with TinyYOLO + ResNet50 closely following at 57.36\%. For efficiency, the fastest combination of models when running on a PC, is TinyYOLO + MobileNetV2, which requires only 38.6ms per emotion detection. However, if running on a Turtlebot, the dlib+MobileNetV2 combination offers the best speed, albeit at the expense of accuracy, with only 37.06\%.

The facial and audio emotion recognition findings show the promise of a multimodal system that can harness the strengths of each method. While facial recognition demonstrates greater speed in specific scenarios, audio recognition can offer emotion recognition when facial recognition is not feasible. These combined systems lay the foundation for more resilient and adaptive human-robot interaction. 

Although this study provides valuable information on multimodal emotion recognition using both facial emotion recognition (FER) and audio emotion recognition (AER), there are several limitations that must be recognised.

First, the face detection models were tested on a dataset that varies significantly in complexity. Although the results for face detection and emotion recognition were promising, the generalisability of these findings to real-world environments, especially those with dynamic and unstructured scenes, remains a challenge. The models performed well under controlled conditions, but their accuracy and inference times can be degraded in settings where multiple faces, occlusions, or extreme lighting conditions are present. This limitation is particularly relevant when deploying these models on robotic platforms that operate in uncontrolled environments.

The facial emotion recognition system has only been tested in the Expressions-in-the-Wild database. Although this data set attempts to simulate real-world scenarios, the images do not present problems such as poor lighting conditions and obstructed faces. They may not fully capture the complexities and variability of emotions expressed in truly dynamic and unstructured real-world conditions.

It was decided early on that the system would use an off-board-based analysis to perform audio emotion recognition. This resulted in, after careful research, that the audio emotion recognition capabilities of OpenSMILE were not tested. However, with the efficiency that was achieved by the facial emotion recognition system, it could be possible to run this system on the robot as well.

Future research will focus on testing the system in live environments with human participants to gain a deeper understanding of its real-world performance. Conducting trials in uncontrolled dynamic environments will allow evaluation under conditions where lighting, background noise, and other factors that affect detection accuracy are not as easily managed. This will provide more robust data on how well the system performs in daily human-robot interactions and will help identify any performance bottlenecks, usability issues, or edge cases that were not encountered in the initial tests.

In addition to live testing, integrating a full audio emotion recognition system, such as OpenSMILE, directly onto the robot will be a key step. Currently, reliance on cloud tools limits the flexibility of the system and its ability to process audio data in real time, particularly in environments with background noise or multiple speakers. By embedding a native AER system on the robot, this aim is to improve both processing speed and accuracy, enabling more seamless and interactive communication between the robot and its users.

Exploring techniques such as Generative Adversarial Networks (GANs), temporal feature analysis, and the Facial Action Coding System (FACS) could further enhance the performance of emotion recognition systems. GANs could be leveraged for data augmentation, addressing the challenge of limited and imbalanced datasets by generating synthetic examples, which would improve the model’s robustness and generalisation to real-world scenarios. Temporal feature analysis, on the other hand, allows for the recognition of emotions over time, making it possible to capture subtle shifts in facial expressions that may not be evident in static images, providing a deeper understanding of emotional states. Finally, FACS offers a systematic way to encode facial movements into Action Units (AUs), which are the fundamental components of facial expressions. By focusing on these underlying muscle movements, models could achieve more accurate emotion classification, particularly for complex or nuanced expressions. Each of these approaches could contribute uniquely to improving the overall performance of emotion recognition systems.

Moreover, exploring the fusion of facial and audio modalities represents a critical next step in advancing the system's emotional understanding. By combining data from facial expressions and vocal tones, the system could achieve more nuanced and context-sensitive emotion detection. Multimodal fusion has the potential to improve accuracy, especially in cases where one modality may be ambiguous or unavailable, such as situations where the face is partially obscured or the audio is noisy. Different fusion techniques, including early, late, or hybrid fusion, should be investigated to determine which method yields the most reliable and consistent results under various conditions.

Further optimisations for real-time performance on robotic platforms will also be pursued. As evidenced by the current study, inference times can increase substantially when models are deployed on hardware-constrained systems like the TurtleBot4. Future work will involve refining the system’s computational efficiency through model optimisations, to ensure that real-time emotion recognition remains feasible without sacrificing accuracy.

The integration of multimodal emotion recognition systems represents a substantial advancement toward developing robots capable of understanding and responding to human emotions in real-time, thereby paving the way for more empathetic and engaging human-robot interactions.
