@ARTICLE{Dautenhahn2007-wl,
  title     = "Socially intelligent robots: dimensions of human-robot
               interaction",
  author    = "Dautenhahn, Kerstin",
  abstract  = "Social intelligence in robots has a quite recent history in
               artificial intelligence and robotics. However, it has become
               increasingly apparent that social and interactive skills are
               necessary requirements in many application areas and contexts
               where robots need to interact and collaborate with other robots
               or humans. Research on human-robot interaction (HRI) poses many
               challenges regarding the nature of interactivity and 'social
               behaviour' in robot and humans. The first part of this paper
               addresses dimensions of HRI, discussing requirements on social
               skills for robots and introducing the conceptual space of HRI
               studies. In order to illustrate these concepts, two examples of
               HRI research are presented. First, research is surveyed which
               investigates the development of a cognitive robot companion. The
               aim of this work is to develop social rules for robot behaviour
               (a 'robotiquette') that is comfortable and acceptable to humans.
               Second, robots are discussed as possible educational or
               therapeutic toys for children with autism. The concept of
               interactive emergence in human-child interactions is
               highlighted. Different types of play among children are
               discussed in the light of their potential investigation in
               human-robot experiments. The paper concludes by examining
               different paradigms regarding 'social relationships' of robots
               and people interacting with them.",
  journal   = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  publisher = "The Royal Society",
  volume    =  362,
  number    =  1480,
  pages     = "679--704",
  month     =  apr,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Breazeal2003-sa,
  title     = "Emotion and sociable humanoid robots",
  author    = "Breazeal, Cynthia",
  abstract  = "This paper focuses on the role of emotion and expressive
               behavior in regulating social interaction between humans and
               expressive anthropomorphic robots, either in communicative or
               teaching scenarios. We present the scientific basis underlying
               our humanoid robot's emotion models and expressive behavior, and
               then show how these scientific viewpoints have been adapted to
               the current implementation. Our robot is also able to recognize
               affective intent through tone of voice, the implementation of
               which is inspired by the scientific findings of the
               developmental psycholinguistics community. We first evaluate the
               robot's expressive displays in isolation. Next, we evaluate the
               robot's overall emotive behavior (i.e. the coordination of the
               affective recognition system, the emotion and motivation
               systems, and the expression system) as it socially engages nave
               human subjects face-to-face.",
  journal   = "Int. J. Hum. Comput. Stud.",
  publisher = "Elsevier BV",
  volume    =  59,
  number    = "1-2",
  pages     = "119--155",
  month     =  jul,
  year      =  2003,
  language  = "en"
}

@INPROCEEDINGS{Castellano2009-cv,
  title           = "Detecting user engagement with a robot companion using
                     task and social interaction-based features",
  booktitle       = "Proceedings of the 2009 international conference on
                     Multimodal interfaces",
  author          = "Castellano, Ginevra and Pereira, Andr{\'e} and Leite,
                     Iolanda and Paiva, Ana and McOwan, Peter W",
  abstract        = "Affect sensitivity is of the utmost importance for a robot
                     companion to be able to display socially intelligent
                     behaviour, a key requirement for sustaining long-term
                     interactions with humans. This paper explores a
                     naturalistic scenario in which children play chess with
                     the iCat, a robot companion. A person-independent,
                     Bayesian approach to detect the user's engagement with the
                     iCat robot is presented. Our framework models both causes
                     and effects of engagement: features related to the user's
                     non-verbal behaviour, the task and the companion's
                     affective reactions are identified to predict the
                     children's level of engagement. An experiment was carried
                     out to train and validate our model. Results show that our
                     approach based on multimodal integration of task and
                     social interaction-based features outperforms those based
                     solely on non-verbal behaviour or contextual information
                     (94.79 \% vs. 93.75 \% and 78.13 \%).",
  publisher       = "ACM",
  month           =  nov,
  year            =  2009,
  address         = "New York, NY, USA",
  conference      = "ICMI-MLMI '09: INTERNATIONAL CONFERENCE ON MULTIMODAL
                     INTERFACES/WORKSHOP ON MACHINE LEARNING FOR MULTIMODAL
                     INTERFACES",
  location        = "Cambridge Massachusetts USA"
}


@ARTICLE{Rangulov2020-pd,
  title         = "Emotion Recognition on large video dataset based on
                   Convolutional Feature Extractor and Recurrent Neural Network",
  author        = "Rangulov, Denis and Fahim, Muhammad",
  abstract      = "For many years, the emotion recognition task has remained
                   one of the most interesting and important problems in the
                   field of human-computer interaction. In this study, we
                   consider the emotion recognition task as a classification as
                   well as a regression task by processing encoded emotions in
                   different datasets using deep learning models. Our model
                   combines convolutional neural network (CNN) with recurrent
                   neural network (RNN) to predict dimensional emotions on
                   video data. At the first step, CNN extracts feature vectors
                   from video frames. In the second step, we fed these feature
                   vectors to train RNN for exploiting the temporal dynamics of
                   video. Furthermore, we analyzed how each neural network
                   contributes to the system's overall performance. The
                   experiments are performed on publicly available datasets
                   including the largest modern Aff-Wild2 database. It contains
                   over sixty hours of video data. We discovered the problem of
                   overfitting of the model on an unbalanced dataset with an
                   illustrative example using confusion matrices. The problem
                   is solved by downsampling technique to balance the dataset.
                   By significantly decreasing training data, we balance the
                   dataset, thereby, the overall performance of the model is
                   improved. Hence, the study qualitatively describes the
                   abilities of deep learning models exploring enough amount of
                   data to predict facial emotions. Our proposed method is
                   implemented using Tensorflow Keras.",
  month         =  jun,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2006.11168"
}

@MISC{Alexey_2021-lf,
  title       = "darknet: {YOLOv4} / {Scaled-YOLOv4} / {YOLO} - Neural Networks
                 for Object Detection (Windows and Linux version of Darknet )",
  author      = "{Alexey}",
  abstract    = "YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object
                 Detection (Windows and Linux version of Darknet ) -
                 AlexeyAB/darknet",
  institution = "Github",
  language    = "en",
  URL         =  {https://github.com/AlexeyAB/darknet},
  urldate     =  {2022-09-04},
  year = {2021}
}

@inproceedings{HaqJackson_AVSP09,
  author = {Haq, S. and Jackson, P.J.B.},
  year = {2009},
  title = "Speaker-dependent audio-visual emotion recognition",
  booktitle = "Proc.\ Int.\ Conf. on Auditory-Visual Speech Processing (AVSP'08), Norwich, UK",
  month = {Sept.},
  abstract = 
"This paper explores the recognition of expressed emotion from
speech and facial gestures for the speaker-dependent case. Experiments
were performed on an English audio-visual emotional
database consisting of 480 utterances from 4 English male actors
in 7 emotions. A total of 106 audio and 240 visual features
were extracted and features were selected with Plus l-Take Away
r algorithm based on Bhattacharyya distance criterion. Linear
transformation methods, principal component analysis (PCA) and
linear discriminant analysis (LDA), were applied to the selected
features and Gaussian classifiers were used for classification. The
performance was higher for LDA features compared to PCA features.
The visual features performed better than the audio features
and overall performance improved for the audio-visual features.
In case of 7 emotion classes, an average recognition rate of 56%
was achieved with the audio features, 95% with the visual features
and 98% with the audio-visual features selected by Bhattacharyya
distance and transformed by LDA. Grouping emotions
into 4 classes, an average recognition rate of 69% was achieved
with the audio features, 98% with the visual features and 98%
with the audio-visual features fused at decision level. The results
were comparable to the measured human recognition rate with
this multimodal data set.",
  keyword = "audio-visual emotion",
  keyword = "data evaluation",
  keyword = "linear transformation",
  keyword = "speaker-dependent"
}

@INPROCEEDINGS{Viola990517,
  author={Viola, P. and Jones, M.},
  booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}, 
  title={Rapid object detection using a boosted cascade of simple features}, 
  year={2001},
  volume={1},
  number={},
  pages={I-I},
  keywords={Object detection;Face detection;Pixel;Detectors;Filters;Machine learning;Image representation;Focusing;Skin;Robustness},
  doi={10.1109/CVPR.2001.990517}
}

@ARTICLE{Redmon2015-eb,
  title     = "You only look once: Unified, real-time object detection",
  author    = "Redmon, Joseph and Divvala, Santosh and Girshick, Ross and
               Farhadi, Ali",
  abstract  = "We present YOLO, a new approach to object detection. Prior work
               on object detection repurposes classifiers to perform detection.
               Instead, we frame object detection as a regression problem to
               spatially separated bounding boxes and associated class
               probabilities. A single neural network predicts bounding boxes
               and class probabilities directly from full images in one
               evaluation. Since the whole detection pipeline is a single
               network, it can be optimized end-to-end directly on detection
               performance. Our unified architecture is extremely fast. Our
               base YOLO model processes images in real-time at 45 frames per
               second. A smaller version of the network, Fast YOLO, processes
               an astounding 155 frames per second while still achieving double
               the mAP of other real-time detectors. Compared to
               state-of-the-art detection systems, YOLO makes more localization
               errors but is far less likely to predict false detections where
               nothing exists. Finally, YOLO learns very general
               representations of objects. It outperforms all other detection
               methods, including DPM and R-CNN, by a wide margin when
               generalizing from natural images to artwork on both the Picasso
               Dataset and the People-Art Dataset.",
  publisher = "arXiv",
  year      =  2015
}

@INPROCEEDINGS{1467360,
  author={Dalal, N. and Triggs, B.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Histograms of oriented gradients for human detection}, 
  year={2005},
  volume={1},
  number={},
  pages={886-893 vol. 1},
  keywords={Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases},
  doi={10.1109/CVPR.2005.177}}


@ARTICLE{Dhuheir2021-ii,
  title         = "Emotion recognition for healthcare surveillance systems
                   using neural networks: A survey",
  author        = "Dhuheir, Marwan and Albaseer, Abdullatif and Baccour, Emna
                   and Erbad, Aiman and Abdallah, Mohamed and Hamdi, Mounir",
  abstract      = "Recognizing the patient's emotions using deep learning
                   techniques has attracted significant attention recently due
                   to technological advancements. Automatically identifying the
                   emotions can help build smart healthcare centers that can
                   detect depression and stress among the patients in order to
                   start the medication early. Using advanced technology to
                   identify emotions is one of the most exciting topics as it
                   defines the relationships between humans and machines.
                   Machines learned how to predict emotions by adopting various
                   methods. In this survey, we present recent research in the
                   field of using neural networks to recognize emotions. We
                   focus on studying emotions' recognition from speech, facial
                   expressions, and audio-visual input and show the different
                   techniques of deploying these algorithms in the real world.
                   These three emotion recognition techniques can be used as a
                   surveillance system in healthcare centers to monitor
                   patients. We conclude the survey with a presentation of the
                   challenges and the related future work to provide an insight
                   into the applications of using emotion recognition.",
  month         =  jul,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2107.05989"
}
@ARTICLE{R2024-jk,
  title         = "Multimodal emotion recognition using audio-Video Transformer
                   Fusion with Cross Attention",
  author        = "Joe Dhanith and Venkatraman, Shravan and Narendra,
                   Modigari and Sharma, Vigya and Malarvannan, Santhosh and
                   Gandomi, Amir H",
  abstract      = "Understanding emotions is a fundamental aspect of human
                   communication. Integrating audio and video signals offers a
                   more comprehensive understanding of emotional states
                   compared to traditional methods that rely on a single data
                   source, such as speech or facial expressions. Despite its
                   potential, multimodal emotion recognition faces significant
                   challenges, particularly in synchronization, feature
                   extraction, and fusion of diverse data sources. To address
                   these issues, this paper introduces a novel
                   transformer-based model named Audio-Video Transformer Fusion
                   with Cross Attention (AVT-CA). The AVT-CA model employs a
                   transformer fusion approach to effectively capture and
                   synchronize interlinked features from both audio and video
                   inputs, thereby resolving synchronization problems.
                   Additionally, the Cross Attention mechanism within AVT-CA
                   selectively extracts and emphasizes critical features while
                   discarding irrelevant ones from both modalities, addressing
                   feature extraction and fusion challenges. Extensive
                   experimental analysis conducted on the CMU-MOSEI, RAVDESS
                   and CREMA-D datasets demonstrates the efficacy of the
                   proposed model. The results underscore the importance of
                   AVT-CA in developing precise and reliable multimodal emotion
                   recognition systems for practical applications.",
  month         =  jul,
  year          =  2024,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.MM",
  eprint        = "2407.18552"
}
@article{Abreu:2010,
  title={Enhancing identity prediction using a novel approach to combining hard-and soft-biometric information},
  author={Da Costa-Abreu, M. and Fairhurst, M.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={41},
  number={5},
  pages={599--607},
  year={2010},
  publisher={IEEE}
}
@INPROCEEDINGS{Mistry2020-gr,
  title           = "A {Multi-Population} {FA} for Automatic Facial Emotion
                     Recognition",
  booktitle       = "2020 International Joint Conference on Neural Networks
                     ({IJCNN})",
  author          = "Mistry, Kamlesh and Rizvi, Baqar and Rook, Chris and
                     Iqbal, Sadaf and Zhang, Li and Joy, Colin Paul",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2020,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2020 International Joint Conference on Neural Networks
                     (IJCNN)",
  location        = "Glasgow, United Kingdom"
}
@INPROCEEDINGS{Faria2017-lg,
  title           = "Affective facial expressions recognition for human-robot
                     interaction",
  booktitle       = "2017 26th {IEEE} International Symposium on Robot and
                     Human Interactive Communication ({RO-MAN})",
  author          = "Faria, Diego R and Vieira, Mario and Faria, Fernanda C C
                     and Premebida, Cristiano",
  publisher       = "IEEE",
  month           =  aug,
  year            =  2017,
  conference      = "2017 26th IEEE International Symposium on Robot and Human
                     Interactive Communication (RO-MAN)",
  location        = "Lisbon"
}
@INPROCEEDINGS{Mistry2018-og,
  title           = "Extended {LBP} based facial expression recognition system
                     for adaptive {AI} agent behaviour",
  booktitle       = "2018 International Joint Conference on Neural Networks
                     ({IJCNN})",
  author          = "Mistry, Kamlesh and Jasekar, Jyoti and Issac, Biju and
                     Zhang, Li",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2018,
  conference      = "2018 International Joint Conference on Neural Networks
                     (IJCNN)",
  location        = "Rio de Janeiro"
}
@INPROCEEDINGS{Appuhamy2018-dc,
  title           = "Development of a {GPU-based} human emotion recognition
                     robot eye for service robot by using convolutional neural
                     network",
  booktitle       = "2018 {IEEE/ACIS} 17th International Conference on Computer
                     and Information Science ({ICIS})",
  author          = "Appuhamy, E J G S and Madhusanka, B G D A",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2018,
  conference      = "2018 IEEE/ACIS 17th International Conference on Computer
                     and Information Science (ICIS)",
  location        = "Singapore"
}
@INPROCEEDINGS{Lopez-Rincon2019-et,
  title           = "Emotion Recognition using Facial Expressions in Children
                     using the {NAO} Robot",
  booktitle       = "2019 International Conference on Electronics,
                     Communications and Computers ({CONIELECOMP})",
  author          = "Lopez-Rincon, Alejandro",
  publisher       = "IEEE",
  month           =  feb,
  year            =  2019,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2019 International Conference on Electronics,
                     Communications and Computers (CONIELECOMP)",
  location        = "Cholula, Mexico"
}
@INPROCEEDINGS{Rosula_Reyes2020-yz,
  title           = "Face detection and recognition of the seven emotions via
                     facial expression: Integration of machine learning
                     algorithm into the {NAO} robot",
  booktitle       = "2020 5th International Conference on Control and Robotics
                     Engineering ({ICCRE})",
  author          = "Rosula Reyes, S J and Depano, Keanu M and Velasco, Aaron
                     Matthew A and Kwong, John Chris T and Oppus, Carlos M",
  publisher       = "IEEE",
  month           =  apr,
  year            =  2020,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2020 5th International Conference on Control and Robotics
                     Engineering (ICCRE)",
  location        = "Osaka, Japan"
}
@INPROCEEDINGS{Gupta2018-af,
  title           = "Facial emotion recognition in real-time and static images",
  booktitle       = "2018 2nd International Conference on Inventive Systems and
                     Control ({ICISC})",
  author          = "Gupta, Shivam",
  publisher       = "IEEE",
  month           =  jan,
  year            =  2018,
  conference      = "2018 2nd International Conference on Inventive Systems and
                     Control (ICISC)",
  location        = "Coimbatore"
}
@ARTICLE{Allognon2020-um,
  title         = "Continuous emotion recognition via deep convolutional
                   autoencoder and support vector regressor",
  author        = "Allognon, Sevegni Odilon Clement and Koerich, Alessandro L
                   and Britto, Jr, Alceu de S",
  abstract      = "Automatic facial expression recognition is an important
                   research area in the emotion recognition and computer
                   vision. Applications can be found in several domains such as
                   medical treatment, driver fatigue surveillance, sociable
                   robotics, and several other human-computer interaction
                   systems. Therefore, it is crucial that the machine should be
                   able to recognize the emotional state of the user with high
                   accuracy. In recent years, deep neural networks have been
                   used with great success in recognizing emotions. In this
                   paper, we present a new model for continuous emotion
                   recognition based on facial expression recognition by using
                   an unsupervised learning approach based on transfer learning
                   and autoencoders. The proposed approach also includes
                   preprocessing and post-processing techniques which
                   contribute favorably to improving the performance of
                   predicting the concordance correlation coefficient for
                   arousal and valence dimensions. Experimental results for
                   predicting spontaneous and natural emotions on the RECOLA
                   2016 dataset have shown that the proposed approach based on
                   visual information can achieve CCCs of 0.516 and 0.264 for
                   valence and arousal, respectively.",
  month         =  jan,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2001.11976"
}
@INPROCEEDINGS{Brandizzi2021AutomaticRI,
  title={Automatic RGB Inference Based on Facial Emotion Recognition},
  author={Nicolo’ Brandizzi and Valerio Bianco and Giulia Castro and Samuele Russo and Agata Wajda},
  booktitle={System (Link{\"o}ping)},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:247582848}
}
@ARTICLE{Melinte2020-ky,
  title     = "Facial expressions recognition for human-robot interaction using
               deep convolutional neural networks with rectified Adam optimizer",
  author    = "Melinte, Daniel Octavian and Vladareanu, Luige",
  abstract  = "The interaction between humans and an NAO robot using deep
               convolutional neural networks (CNN) is presented in this paper
               based on an innovative end-to-end pipeline method that applies
               two optimized CNNs, one for face recognition (FR) and another
               one for the facial expression recognition (FER) in order to
               obtain real-time inference speed for the entire process. Two
               different models for FR are considered, one known to be very
               accurate, but has low inference speed (faster region-based
               convolutional neural network), and one that is not as accurate
               but has high inference speed (single shot detector convolutional
               neural network). For emotion recognition transfer learning and
               fine-tuning of three CNN models (VGG, Inception V3 and ResNet)
               has been used. The overall results show that single shot
               detector convolutional neural network (SSD CNN) and faster
               region-based convolutional neural network (Faster R-CNN) models
               for face detection share almost the same accuracy: 97.8\% for
               Faster R-CNN on PASCAL visual object classes (PASCAL VOCs)
               evaluation metrics and 97.42\% for SSD Inception. In terms of
               FER, ResNet obtained the highest training accuracy (90.14\%),
               while the visual geometry group (VGG) network had 87\% accuracy
               and Inception V3 reached 81\%. The results show improvements
               over 10\% when using two serialized CNN, instead of using only
               the FER CNN, while the recent optimization model, called
               rectified adaptive moment optimization (RAdam), lead to a better
               generalization and accuracy improvement of 3\%-4\% on each
               emotion recognition CNN.",
  journal   = "Sensors (Basel)",
  publisher = "MDPI AG",
  volume    =  20,
  number    =  8,
  pages     = "2393",
  month     =  apr,
  year      =  2020,
  keywords  = "NAO robot; advanced intelligent control; computer vision;
               convolutional neural networks; deep learning; face recognition;
               facial emotion recognition",
  language  = "en"
}
@ARTICLE{Devaram2022-qc,
  title    = "{LEMON}: A lightweight facial Emotion Recognition system for
              Assistive Robotics based on Dilated Residual Convolutional Neural
              Networks",
  author   = "Devaram, Rami Reddy and Beraldo, Gloria and De Benedictis,
              Riccardo and Mongiov{\`\i}, Misael and Cesta, Amedeo",
  abstract = "The development of a Social Intelligence System based on
              artificial intelligence is one of the cutting edge technologies
              in Assistive Robotics. Such systems need to create an empathic
              interaction with the users; therefore, it os required to include
              an Emotion Recognition (ER) framework which has to run, in near
              real-time, together with several other intelligent services. Most
              of the low-cost commercial robots, however, although more
              accessible by users and healthcare facilities, have to balance
              costs and effectiveness, resulting in under-performing hardware
              in terms of memory and processing unit. This aspect makes the
              design of the systems challenging, requiring a trade-off between
              the accuracy and the complexity of the adopted models. This paper
              proposes a compact and robust service for Assistive Robotics,
              called Lightweight EMotion recognitiON (LEMON), which uses image
              processing, Computer Vision and Deep Learning (DL) algorithms to
              recognize facial expressions. Specifically, the proposed DL model
              is based on Residual Convolutional Neural Networks with the
              combination of Dilated and Standard Convolution Layers. The first
              remarkable result is the few numbers (i.e., 1.6 Million) of
              parameters characterizing our model. In addition, Dilated
              Convolutions expand receptive fields exponentially with
              preserving resolution, less computation and memory cost to
              recognize the distinction among facial expressions by capturing
              the displacement of the pixels. Finally, to reduce the dying ReLU
              problem and improve the stability of the model, we apply an
              Exponential Linear Unit (ELU) activation function in the initial
              layers of the model. We have performed training and evaluation
              (via one- and five-fold cross validation) of the model with five
              datasets available in the community and one mixed dataset created
              by taking samples from all of them. With respect to the other
              approaches, our model achieves comparable results with a
              significant reduction in terms of the number of parameters.",
  journal  = "Sensors (Basel)",
  volume   =  22,
  number   =  9,
  month    =  apr,
  year     =  2022,
  keywords = "assistive robotics; computer vision; deep convolutional neural
              networks; emotion recognition; face recognition",
  language = "en"
}

@ARTICLE{Ruiz-Garcia2018-zq,
  title     = "A hybrid deep learning neural approach for emotion recognition
               from facial expressions for socially assistive robots",
  author    = "Ruiz-Garcia, Ariel and Elshaw, Mark and Altahhan, Abdulrahman
               and Palade, Vasile",
  journal   = "Neural Comput. Appl.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  29,
  number    =  7,
  pages     = "359--373",
  month     =  apr,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Saxena2022-sr,
  title     = "An intelligent facial expression recognition system with emotion
               intensity classification",
  author    = "Saxena, Suchitra and Tripathi, Shikha and Sudarshan, T S B",
  journal   = "Cogn. Syst. Res.",
  publisher = "Elsevier BV",
  volume    =  74,
  pages     = "39--52",
  month     =  aug,
  year      =  2022,
  language  = "en"
}

@INCOLLECTION{Ruiz-Garcia2018-uy,
  title     = "Deep learning for real time facial expression recognition in
               social robots",
  booktitle = "Neural Information Processing",
  author    = "Ruiz-Garcia, Ariel and Webb, Nicola and Palade, Vasile and
               Eastwood, Mark and Elshaw, Mark",
  publisher = "Springer International Publishing",
  pages     = "392--402",
  series    = "Lecture notes in computer science",
  year      =  2018,
  address   = "Cham"
}

@INPROCEEDINGS{Song2019-bo,
  title           = "Accuracy improvement of facial expression recognition in
                     speech acts: Confirmation of effectiveness of information
                     around a mouth and {GAN-based} data augmentation",
  booktitle       = "2019 28th {IEEE} International Conference on Robot and
                     Human Interactive Communication ({RO-MAN})",
  author          = "Song, Kyu-Seob and Kwon, Dong-Soo",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2019,
  conference      = "2019 28th IEEE International Conference on Robot and Human
                     Interactive Communication (RO-MAN)",
  location        = "New Delhi, India"
}
@INPROCEEDINGS{Mohammadpour2017-xk,
  title           = "Facial emotion recognition using deep convolutional
                     networks",
  booktitle       = "2017 {IEEE} 4th International Conference on
                     {Knowledge-Based} Engineering and Innovation ({KBEI})",
  author          = "Mohammadpour, Mostafa and Khaliliardali, Hossein and
                     Hashemi, Seyyed Mohammad R and AlyanNezhadi, Mohammad M",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2017,
  conference      = "2017 IEEE 4th International Conference on Knowledge-Based
                     Engineering and Innovation (KBEI)",
  location        = "Tehran"
}
@INPROCEEDINGS{Udeh2022-me,
  title           = "A co-regularization facial emotion recognition based on
                     multi-task facial action unit recognition",
  booktitle       = "2022 41st Chinese Control Conference ({CCC})",
  author          = "Udeh, Chinonso Paschal and Chen, Luefeng and Du, Sheng and
                     Li, Min and Wu, Min",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2022,
  conference      = "2022 41st Chinese Control Conference (CCC)",
  location        = "Hefei, China"
}

@ARTICLE{Kusuma2020-oa,
  title     = "Emotion recognition on {FER-2013} face images using fine-tuned
               {VGG-16}",
  author    = "Kusuma, Gede Putra and Jonathan, Jonathan and Lim, Andreas
               Pangestu",
  journal   = "Adv. Sci. Technol. Eng. Syst. J.",
  publisher = "ASTES Journal",
  volume    =  5,
  number    =  6,
  pages     = "315--322",
  year      =  2020
}

@ARTICLE{Pramerdorfer2016-xx,
  title         = "Facial expression recognition using Convolutional Neural
                   Networks: State of the art",
  author        = "Pramerdorfer, Christopher and Kampel, Martin",
  abstract      = "The ability to recognize facial expressions automatically
                   enables novel applications in human-computer interaction and
                   other areas. Consequently, there has been active research in
                   this field, with several recent works utilizing
                   Convolutional Neural Networks (CNNs) for feature extraction
                   and inference. These works differ significantly in terms of
                   CNN architectures and other factors. Based on the reported
                   results alone, the performance impact of these factors is
                   unclear. In this paper, we review the state of the art in
                   image-based facial expression recognition using CNNs and
                   highlight algorithmic differences and their performance
                   impact. On this basis, we identify existing bottlenecks and
                   consequently directions for advancing this research field.
                   Furthermore, we demonstrate that overcoming one of these
                   bottlenecks - the comparatively basic architectures of the
                   CNNs utilized in this field - leads to a substantial
                   performance increase. By forming an ensemble of modern deep
                   CNNs, we obtain a FER2013 test accuracy of 75.2\%,
                   outperforming previous works without requiring auxiliary
                   training data or face registration.",
  month         =  dec,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1612.02903"
}

@ARTICLE{Zhu2024-gy,
  title    = "A study on expression recognition based on improved mobilenetV2
              network",
  author   = "Zhu, Qiming and Zhuang, Hongwei and Zhao, Mi and Xu, Shuangchao
              and Meng, Rui",
  abstract = "This paper proposes an improved strategy for the MobileNetV2
              neural network(I-MobileNetV2) in response to problems such as
              large parameter quantities in existing deep convolutional neural
              networks and the shortcomings of the lightweight neural network
              MobileNetV2 such as easy loss of feature information, poor
              real-time performance, and low accuracy rate in facial emotion
              recognition tasks. The network inherits the characteristics of
              MobilenetV2 depthwise separated convolution, signifying a
              reduction in computational load while maintaining a lightweight
              profile. It utilizes a reverse fusion mechanism to retain
              negative features, which makes the information less likely to be
              lost. The SELU activation function is used to replace the RELU6
              activation function to avoid gradient vanishing. Meanwhile, to
              improve the feature recognition capability, the channel attention
              mechanism (Squeeze-and-Excitation Networks (SE-Net)) is
              integrated into the MobilenetV2 network. Experiments conducted on
              the facial expression datasets FER2013 and CK + showed that the
              proposed network model achieved facial expression recognition
              accuracies of 68.62\% and 95.96\%, improving upon the MobileNetV2
              model by 0.72\% and 6.14\% respectively, and the parameter count
              decreased by 83.8\%. These results empirically verify the
              effectiveness of the improvements made to the network model.",
  journal  = "Sci. Rep.",
  volume   =  14,
  number   =  1,
  pages    = "8121",
  month    =  apr,
  year     =  2024,
  keywords = "Attention mechanism; Expression recognition; MobileNetV2; Reverse
              fusion; SELU",
  language = "en"
}

@INPROCEEDINGS{Ma2019-ng,
  title      = "{ElderReact}: A multimodal dataset for recognizing emotional
                response in aging adults",
  booktitle  = "2019 International Conference on Multimodal Interaction",
  author     = "Ma, Kaixin and Wang, Xinyu and Yang, Xinru and Zhang, Mingtong
                and Girard, Jeffrey M and Morency, Louis-Philippe",
  publisher  = "ACM",
  month      =  oct,
  year       =  2019,
  address    = "New York, NY, USA",
  copyright  = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference = "ICMI '19: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION",
  location   = "Suzhou China"
}
@INPROCEEDINGS{Webb2020-bq,
  title           = "Emotion recognition from face images in an unconstrained
                     environment for usage on social robots",
  booktitle       = "2020 International Joint Conference on Neural Networks
                     ({IJCNN})",
  author          = "Webb, Nicola and Ruiz-Garcia, Ariel and Elshaw, Mark and
                     Palade, Vasile",
  publisher       = "IEEE",
  month           =  jul,
  year            =  2020,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2020 International Joint Conference on Neural Networks
                     (IJCNN)",
  location        = "Glasgow, United Kingdom"
}
@ARTICLE{Esfandbod2023-eq,
  title     = "Utilizing an emotional robot capable of lip-syncing in
               robot-assisted speech therapy sessions for children with
               language disorders",
  author    = "Esfandbod, Alireza and Rokhi, Zeynab and Meghdari, Ali F and
               Taheri, Alireza and Alemi, Minoo and Karimi, Mahdieh",
  abstract  = "This study scrutinizes the impacts of utilizing a socially
               assistive robot, the RASA robot, during speech therapy sessions
               for children with language disorders. Two capabilities were
               developed for the robotic platform to enhance children-robot
               interactions during speech therapy interventions: facial
               expression communication (containing recognition and expression)
               and lip-syncing. Facial expression recognition was conducted by
               training several well-known CNN architectures on one of the most
               extensive facial expressions databases, the AffectNet database,
               and then modifying them using the transfer learning strategy
               performed on the CK+ dataset. The robot's lip-syncing capability
               was designed in two steps. The first step was concerned with
               designing precise schemes of the articulatory elements needed
               during the pronunciation of the Persian phonemes (i.e.,
               consonants and vowels). The second step included developing an
               algorithm to pronounce words by disassembling them into their
               components (including consonants and vowels) and then morphing
               them into each other successively. To pursue the study's primary
               goal, two comparable groups of children with language disorders
               were considered, the intervention and control groups. The
               intervention group attended therapy sessions in which the robot
               acted as the therapist's assistant, while the control group only
               communicated with the human therapist. The study's first purpose
               was to compare the children's engagement while playing a mimic
               game with the affective robot and the therapist, conducted via
               video coding. The second objective was to assess the efficacy of
               the robot's presence in the speech therapy sessions alongside
               the therapist, accomplished by administering the Persian Test of
               Language Development, Persian TOLD. According to the first
               scenario, playing with the affective robot is more engaging than
               playing with the therapist. Furthermore, the statistical
               analysis of the study's results indicates that participating in
               robot-assisted speech therapy (RAST) sessions enhances children
               with language disorders' achievements in comparison with taking
               part in conventional speech therapy interventions.",
  journal   = "Int. J. Soc. Robot.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  15,
  number    =  2,
  pages     = "165--183",
  year      =  2023,
  keywords  = "Child-robot interaction (CRI); Children with language disorders;
               Convolutional neural network (CNN); Facial expression
               recognition; Lip-syncing; Social robots; Speech therapy",
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en"
}
@ARTICLE{Chen2021-ra,
  title     = "Adaptive feature selection-based {AdaBoost-KNN} with direct
               optimization for dynamic emotion recognition in human--robot
               interaction",
  author    = "Chen, Luefeng and Li, Min and Su, Wanjuan and Wu, Min and
               Hirota, Kaoru and Pedrycz, Witold",
  journal   = "IEEE Trans. Emerg. Top. Comput. Intell.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  5,
  number    =  2,
  pages     = "205--213",
  month     =  apr,
  year      =  2021,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}
@BOOK{Carolis2016-ig,
  title  = "{Emotion-Recognition} from Speech-based Interaction in {AAL}
            Environment",
  author = "Carolis, B D and Ferilli, S and Palestra, G and Redavid, D",
  year   =  2016
}
@INPROCEEDINGS{Lakomkin2018-ws,
  title           = "On the robustness of speech emotion recognition for
                     human-robot interaction with deep neural networks",
  booktitle       = "2018 {IEEE/RSJ} International Conference on Intelligent
                     Robots and Systems ({IROS})",
  author          = "Lakomkin, Egor and Zamani, Mohammad Ali and Weber,
                     Cornelius and Magg, Sven and Wermter, Stefan",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2018,
  conference      = "2018 IEEE/RSJ International Conference on Intelligent
                     Robots and Systems (IROS)",
  location        = "Madrid, Spain"
}
@ARTICLE{Kim2009-in,
  title     = "Design and development of an emotional interaction robot, mung",
  author    = "Kim, Eun Ho and Kwak, Sonya S and Hyun, Kyung Hak and Kim, Soo
               Hyun and Kwak, Yoon Keun",
  journal   = "Adv. Robot.",
  publisher = "Informa UK Limited",
  volume    =  23,
  number    =  6,
  pages     = "767--784",
  month     =  jan,
  year      =  2009,
  language  = "en"
}
@ARTICLE{Kim2018-dh,
  title     = "Speaker-independent emotion recognition for interstate measuring
               of user based on separation and rejection",
  author    = "Kim, Bo Seong and {Korea Institute of Industrial Technology,
               Ansan-si, Gyeongi-do, South Korea} and Kim, Eun Ho",
  journal   = "Int. J. Mach. Learn. Comput.",
  publisher = "EJournal Publishing",
  volume    =  8,
  number    =  2,
  pages     = "152--157",
  month     =  apr,
  year      =  2018
}

@ARTICLE{Hajarolasvadi2019-nz,
  title     = "{3D} {CNN-based} speech emotion recognition using K-means
               clustering and spectrograms",
  author    = "Hajarolasvadi, Noushin and Demirel, Hasan",
  abstract  = "Detecting human intentions and emotions helps improve
               human-robot interactions. Emotion recognition has been a
               challenging research direction in the past decade. This paper
               proposes an emotion recognition system based on analysis of
               speech signals. Firstly, we split each speech signal into
               overlapping frames of the same length. Next, we extract an
               88-dimensional vector of audio features including Mel Frequency
               Cepstral Coefficients (MFCC), pitch, and intensity for each of
               the respective frames. In parallel, the spectrogram of each
               frame is generated. In the final preprocessing step, by applying
               k-means clustering on the extracted features of all frames of
               each audio signal, we select k most discriminant frames, namely
               keyframes, to summarize the speech signal. Then, the sequence of
               the corresponding spectrograms of keyframes is encapsulated in a
               3D tensor. These tensors are used to train and test a 3D
               Convolutional Neural network using a 10-fold cross-validation
               approach. The proposed 3D CNN has two convolutional layers and
               one fully connected layer. Experiments are conducted on the
               Surrey Audio-Visual Expressed Emotion (SAVEE), Ryerson
               Multimedia Laboratory (RML), and eNTERFACE'05 databases. The
               results are superior to the state-of-the-art methods reported in
               the literature.",
  journal   = "Entropy (Basel)",
  publisher = "MDPI AG",
  volume    =  21,
  number    =  5,
  pages     = "479",
  month     =  may,
  year      =  2019,
  keywords  = "3D convolutional neural networks; deep learning; k-means
               clustering; spectrograms; speech emotion recognition",
  language  = "en"
}
@INPROCEEDINGS{Jaiswal2020-vi,
  title           = "Image based emotional state prediction from multiparty
                     audio conversation",
  booktitle       = "2020 {IEEE} Pune Section International Conference
                     ({PuneCon})",
  author          = "Jaiswal, Shruti and Jain, Ayush and Nandi, G C",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2020,
  conference      = "2020 IEEE Pune Section International Conference (PuneCon)",
  location        = "Pune, India"
}
@ARTICLE{Mohammed2020-ig,
  title     = "Speech emotion recognition using {MELBP} variants of spectrogram
               image",
  author    = "Mohammed, Suhaila and Alia, Hassan",
  journal   = "Int. J. Intell. Eng. Syst.",
  publisher = "The Intelligent Networks and Systems Society",
  volume    =  13,
  number    =  5,
  pages     = "257--266",
  month     =  oct,
  year      =  2020
}
@ARTICLE{Mustaqeem2020-ax,
  title     = "Clustering-based speech emotion recognition by incorporating
               learned features and deep {BiLSTM}",
  author    = "{Mustaqeem} and Sajjad, Muhammad and Kwon, Soonil",
  journal   = "IEEE Access",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  8,
  pages     = "79861--79875",
  year      =  2020,
  copyright = "https://creativecommons.org/licenses/by/4.0/legalcode"
}
@ARTICLE{Busso2008-qj,
  title     = "{IEMOCAP}: interactive emotional dyadic motion capture database",
  author    = "Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and
               Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang,
               Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S",
  journal   = "Lang. Resour. Eval.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  42,
  number    =  4,
  pages     = "335--359",
  month     =  dec,
  year      =  2008,
  language  = "en"
}
@INPROCEEDINGS{Shanta2021-af,
  title           = "A comparative analysis of different approach for basic
                     emotions recognition from speech",
  booktitle       = "2021 International Conference on Electronics,
                     Communications and Information Technology ({ICECIT})",
  author          = "Shanta, Shammy Shikder and Sham-E-Ansari, Md and
                     Chowdhury, Atiqul Islam and Shahriar, Mohammad Munem and
                     Hasan, Md Khairul",
  publisher       = "IEEE",
  month           =  sep,
  year            =  2021,
  conference      = "2021 International Conference on Electronics,
                     Communications and Information Technology (ICECIT)",
  location        = "Khulna, Bangladesh"
}

@BOOK{Qayyum2019-bt,
  title  = "Convolutional Neural Network ({CNN}) Based {Speech-Emotion}
            Recognition",
  author = "Qayyum, Abdul and Arefeen, A B and Shahnaz, A and Ieee Xplore, C",
  year   =  2019
}

@INPROCEEDINGS{Zhu2019-iq,
  title           = "Emotion recognition from speech to improve human-robot
                     interaction",
  booktitle       = "2019 {IEEE} Intl Conf on Dependable, Autonomic and Secure
                     Computing, Intl Conf on Pervasive Intelligence and
                     Computing, Intl Conf on Cloud and Big Data Computing, Intl
                     Conf on Cyber Science and Technology Congress
                     ({DASC/PiCom/CBDCom/CyberSciTech})",
  author          = "Zhu, Changrui and Ahmad, Wasim",
  publisher       = "IEEE",
  month           =  aug,
  year            =  2019,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2019 IEEE Intl Conf on Dependable, Autonomic and Secure
                     Computing, Intl Conf on Pervasive Intelligence and
                     Computing, Intl Conf on Cloud and Big Data Computing, Intl
                     Conf on Cyber Science and Technology Congress
                     (DASC/PiCom/CBDCom/CyberSciTech)",
  location        = "Fukuoka, Japan"
}

@BOOK{Chattopadhyay2022-kj,
  title  = "A feature selection model for speech emotion recognition using
            clustering-based population generation with hybrid of equilibrium
            optimizer and atom search optimization algorithm. Multimedia Tools
            and Applications",
  author = "Chattopadhyay, S and Dey, A and Singh, P K and Ahmadian, A and
            Sarkar, R",
  year   =  2022
}

@INPROCEEDINGS{Anjum2019-ks,
  title           = "Emotion Recognition from Speech for an Interactive Robot
                     Agent",
  booktitle       = "2019 {IEEE/SICE} International Symposium on System
                     Integration ({SII})",
  author          = "Anjum, Madiha",
  publisher       = "IEEE",
  month           =  jan,
  year            =  2019,
  copyright       = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  conference      = "2019 IEEE/SICE International Symposium on System
                     Integration (SII)",
  location        = "Paris, France"
}

@ARTICLE{Chen2020-bt,
  title     = "Two-layer fuzzy multiple random forest for speech emotion
               recognition in human-robot interaction",
  author    = "Chen, Luefeng and Su, Wanjuan and Feng, Yu and Wu, Min and She,
               Jinhua and Hirota, Kaoru",
  journal   = "Inf. Sci. (Ny)",
  publisher = "Elsevier BV",
  volume    =  509,
  pages     = "150--163",
  month     =  jan,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Peng2020-wv,
  title     = "Speech emotion recognition using {3D} convolutions and
               attention-based sliding recurrent networks with auditory
               front-ends",
  author    = "Peng, Zhichao and Li, Xingfeng and Zhu, Zhi and Unoki, Masashi
               and Dang, Jianwu and Akagi, Masato",
  journal   = "IEEE Access",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  8,
  pages     = "16560--16572",
  year      =  2020,
  copyright = "https://creativecommons.org/licenses/by/4.0/legalcode"
}

@INPROCEEDINGS{Zhichao2020-bf,
  title           = "Attention-based sequence modeling for categorical emotion
                     recognition with modulation spectral feature",
  booktitle       = "2020 7th International Conference on Information Science
                     and Control Engineering ({ICISCE})",
  author          = "Zhichao, Peng and Wenhua, He and Hongji, Tang and Minlei,
                     Xiao and Ruwei, Luo",
  publisher       = "IEEE",
  month           =  dec,
  year            =  2020,
  conference      = "2020 7th International Conference on Information Science
                     and Control Engineering (ICISCE)",
  location        = "Changsha, China"
}

@ARTICLE{Nie2022-fo,
  title     = "{I-GCN}: Incremental graph convolution network for conversation
               emotion detection",
  author    = "Nie, Weizhi and Chang, Rihao and Ren, Minjie and Su, Yuting and
               Liu, Anan",
  journal   = "IEEE Trans. Multimedia",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  24,
  pages     = "4471--4481",
  year      =  2022,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@INPROCEEDINGS{Rasendrasoa2022-nf,
  title      = "Real-time multimodal emotion recognition in conversation for
                multi-party interactions",
  booktitle  = "Proceedings of the 2022 International Conference on Multimodal
                Interaction",
  author     = "Rasendrasoa, Sandratra and Pauchet, Alexandre and Saunier,
                Julien and Adam, S{\'e}bastien",
  publisher  = "ACM",
  month      =  nov,
  year       =  2022,
  address    = "New York, NY, USA",
  conference = "ICMI '22: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION",
  location   = "Bengaluru India"
}

@ARTICLE{Chen2023-ss,
  title     = "Coupled multimodal emotional feature analysis based on
               broad-deep fusion networks in human-robot interaction",
  author    = "Chen, Luefeng and Li, Min and Wu, Min and Pedrycz, Witold and
               Hirota, Kaoru",
  abstract  = "A coupled multimodal emotional feature analysis (CMEFA) method
               based on broad-deep fusion networks, which divide multimodal
               emotion recognition into two layers, is proposed. First, facial
               emotional features and gesture emotional features are extracted
               using the broad and deep learning fusion network (BDFN).
               Considering that the bi-modal emotion is not completely
               independent of each other, canonical correlation analysis (CCA)
               is used to analyze and extract the correlation between the
               emotion features, and a coupling network is established for
               emotion recognition of the extracted bi-modal features. Both
               simulation and application experiments are completed. According
               to the simulation experiments completed on the bimodal face and
               body gesture database (FABO), the recognition rate of the
               proposed method has increased by 1.15\% compared to that of the
               support vector machine recursive feature elimination (SVMRFE)
               (without considering the unbalanced contribution of features).
               Moreover, by using the proposed method, the multimodal
               recognition rate is 21.22\%, 2.65\%, 1.61\%, 1.54\%, and 0.20\%
               higher than those of the fuzzy deep neural network with sparse
               autoencoder (FDNNSA), ResNet-101 + GFK, C3D + MCB + DBN, the
               hierarchical classification fusion strategy (HCFS), and
               cross-channel convolutional neural network (CCCNN),
               respectively. In addition, preliminary application experiments
               are carried out on our developed emotional social robot system,
               where emotional robot recognizes the emotions of eight
               volunteers based on their facial expressions and body gestures.",
  journal   = "IEEE Trans. Neural Netw. Learn. Syst.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    = "PP",
  pages     = "1--11",
  month     =  jan,
  year      =  2023,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  language  = "en"
}

@ARTICLE{Wang2022-eq,
  title     = "Multitask touch gesture and emotion recognition using multiscale
               spatiotemporal convolutions with attention mechanism",
  author    = "Wang, Ya-Xin and Li, Yun-Kai and Yang, Tian-Hao and Meng,
               Qing-Hao",
  journal   = "IEEE Sens. J.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  22,
  number    =  16,
  pages     = "16190--16201",
  month     =  aug,
  year      =  2022,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@ARTICLE{Lyu2022-vd,
  title    = "Global and local feature fusion via long and short-term memory
              mechanism for dance emotion recognition in robot",
  author   = "Lyu, Yin and Sun, Yang",
  abstract = "In recent years, there are more and more intelligent machines in
              people's life, such as intelligent wristbands, sweeping robots,
              intelligent learning machines and so on, which can simply
              complete a single execution task. We want robots to be as
              emotional as humans. In this way, human-computer interaction can
              be more natural, smooth and intelligent. Therefore, emotion
              research has become a hot topic that researchers pay close
              attention to. In this paper, we propose a new dance emotion
              recognition based on global and local feature fusion method. If
              the single feature of audio is extracted, the global information
              of dance cannot be reflected. And the dimension of data features
              is very high. In this paper, an improved long and short-term
              memory (LSTM) method is used to extract global dance information.
              Linear prediction coefficient is used to extract local
              information. Considering the complementarity of different
              features, a global and local feature fusion method based on
              discriminant multi-canonical correlation analysis is proposed in
              this paper. Experimental results on public data sets show that
              the proposed method can effectively identify dance emotion
              compared with other state-of-the-art emotion recognition methods.",
  journal  = "Front. Neurorobot.",
  volume   =  16,
  pages    = "998568",
  month    =  aug,
  year     =  2022,
  keywords = "LSTM; dance emotion recognition; feature fusion; linear
              prediction coefficient; robot",
  language = "en"
}

@INPROCEEDINGS{Elfaramawy2017-ab,
  title           = "Emotion recognition from body expressions with a neural
                     network architecture",
  booktitle       = "Proceedings of the 5th International Conference on Human
                     Agent Interaction",
  author          = "Elfaramawy, Nourhan and Barros, Pablo and Parisi, German I
                     and Wermter, Stefan",
  abstract        = "The recognition of emotions plays an important role in our
                     daily life and is essential for social communication.
                     Although multiple studies have shown that body expressions
                     can strongly convey emotional states, emotion recognition
                     from body motion patterns has received less attention than
                     the use of facial expressions. In this paper, we propose a
                     self-organizing neural architecture that can effectively
                     recognize affective states from full-body motion patterns.
                     To evaluate our system, we designed and collected a data
                     corpus named the Body Expressions of Emotion (BEE) dataset
                     using a depth sensor in a human-robot interaction
                     scenario. For our recordings, nineteen participants were
                     asked to perform six different emotions:anger, fear,
                     happiness, neutral, sadness, and surprise. In order to
                     compare our system with human-like performance, we
                     conducted an additional experiment by asking fifteen
                     annotators to label depth map video sequences as one of
                     the six emotion classes. The labeling results from human
                     annotators were compared to the results predicted by our
                     system. Experimental results showed that the recognition
                     accuracy of the system was competitive with human
                     performance when exposed to body motion patterns from the
                     same dataset.",
  publisher       = "ACM",
  month           =  oct,
  year            =  2017,
  address         = "New York, NY, USA",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "HAI '17: The Fifth International Conference on Human-Agent
                     Interaction",
  location        = "Bielefeld Germany"
}
@INPROCEEDINGS{Adiga2020-wv,
  title           = "Multimodal Emotion Recognition for Human Robot Interaction",
  booktitle       = "2020 7th International Conference on Soft Computing \&
                     Machine Intelligence ({ISCMI})",
  author          = "Adiga, Sharvari and Vaishnavi, D V and Saxena, Suchitra
                     and Tripathi, Shikha",
  publisher       = "IEEE",
  month           =  nov,
  year            =  2020,
  conference      = "2020 7th International Conference on Soft Computing \&
                     Machine Intelligence (ISCMI)",
  location        = "Stockholm, Sweden"
}
@INPROCEEDINGS{Song2018-vu,
  title           = "Decision-level fusion method for emotion recognition using
                     multimodal emotion recognition information",
  booktitle       = "2018 15th International Conference on Ubiquitous Robots
                     ({UR})",
  author          = "Song, Kyu-Seob and Nho, Young-Hoon and Seo, Ju-Hwan and
                     Kwon, Dong-Soo",
  publisher       = "IEEE",
  month           =  jun,
  year            =  2018,
  conference      = "2018 15th International Conference on Ubiquitous Robots
                     (UR)",
  location        = "Honolulu, HI"
}
@ARTICLE{Kansizoglou2022-ih,
  title     = "An active learning paradigm for online audio-visual emotion
               recognition",
  author    = "Kansizoglou, Ioannis and Bampis, Loukas and Gasteratos, Antonios",
  journal   = "IEEE Trans. Affect. Comput.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  13,
  number    =  2,
  pages     = "756--768",
  month     =  apr,
  year      =  2022,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}
@INCOLLECTION{Yu2019-ku,
  title     = "Interactive Robot Learning for Multimodal Emotion Recognition",
  booktitle = "Social Robotics",
  author    = "Yu, Chuang and Tapus, Adriana",
  publisher = "Springer International Publishing",
  pages     = "633--642",
  series    = "Lecture notes in computer science",
  year      =  2019,
  address   = "Cham"
}

@BOOK{Aqdus2021-xr,
  title  = "Deep Emotion Recognition through Upper Body Movements and Facial
            Expression",
  author = "Aqdus, Chaudhary and Nunes, Rita and {Kamal} and Rehm, Matthias and
            Moeslund, Thomas",
  year   =  2021
}
@ARTICLE{Chen2023-dn,
  title     = "Coupled multimodal emotional feature analysis based on
               broad-deep fusion networks in human-robot interaction",
  author    = "Chen, Luefeng and Li, Min and Wu, Min and Pedrycz, Witold and
               Hirota, Kaoru",
  abstract  = "A coupled multimodal emotional feature analysis (CMEFA) method
               based on broad-deep fusion networks, which divide multimodal
               emotion recognition into two layers, is proposed. First, facial
               emotional features and gesture emotional features are extracted
               using the broad and deep learning fusion network (BDFN).
               Considering that the bi-modal emotion is not completely
               independent of each other, canonical correlation analysis (CCA)
               is used to analyze and extract the correlation between the
               emotion features, and a coupling network is established for
               emotion recognition of the extracted bi-modal features. Both
               simulation and application experiments are completed. According
               to the simulation experiments completed on the bimodal face and
               body gesture database (FABO), the recognition rate of the
               proposed method has increased by 1.15\% compared to that of the
               support vector machine recursive feature elimination (SVMRFE)
               (without considering the unbalanced contribution of features).
               Moreover, by using the proposed method, the multimodal
               recognition rate is 21.22\%, 2.65\%, 1.61\%, 1.54\%, and 0.20\%
               higher than those of the fuzzy deep neural network with sparse
               autoencoder (FDNNSA), ResNet-101 + GFK, C3D + MCB + DBN, the
               hierarchical classification fusion strategy (HCFS), and
               cross-channel convolutional neural network (CCCNN),
               respectively. In addition, preliminary application experiments
               are carried out on our developed emotional social robot system,
               where emotional robot recognizes the emotions of eight
               volunteers based on their facial expressions and body gestures.",
  journal   = "IEEE Trans. Neural Netw. Learn. Syst.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    = "PP",
  pages     = "1--11",
  month     =  jan,
  year      =  2023,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  language  = "en"
}
@ARTICLE{Augello2022-fu,
  title     = "Multimodal mood recognition for assistive scenarios",
  author    = "Augello, Agnese and Bella, Giulia Di and Infantino, Ignazio and
               Pilato, Giovanni and Vitale, Gianpaolo",
  journal   = "Procedia Comput. Sci.",
  publisher = "Elsevier BV",
  volume    =  213,
  pages     = "510--517",
  year      =  2022,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}
@ARTICLE{Heredia2022-gy,
  title     = "Adaptive multimodal emotion detection architecture for social
               robots",
  author    = "Heredia, Juanpablo and Lopes-Silva, Edmundo and Cardinale,
               Yudith and Diaz-Amado, Jose and Dongo, Irvin and Graterol,
               Wilfredo and Aguilera, Ana",
  journal   = "IEEE Access",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  10,
  pages     = "20727--20744",
  year      =  2022,
  copyright = "https://creativecommons.org/licenses/by/4.0/legalcode"
}
@INPROCEEDINGS{Hung2020-lr,
  title      = "Multiple models using temporal feature learning for emotion
                recognition",
  booktitle  = "The 9th International Conference on Smart Media and
                Applications",
  author     = "Hung, Hoang Manh and Kim, Soo-Hyung and Yang, Hyung-Jeong and
                Lee, Guee-Sang",
  publisher  = "ACM",
  month      =  sep,
  year       =  2020,
  address    = "New York, NY, USA",
  conference = "SMA 2020: The 9th International Conference on Smart Media and
                Applications",
  location   = "Jeju Republic of Korea"
}

@ARTICLE{Augello2022-zy,
  title     = "Multimodal mood recognition for assistive scenarios",
  author    = "Augello, Agnese and Bella, Giulia Di and Infantino, Ignazio and
               Pilato, Giovanni and Vitale, Gianpaolo",
  journal   = "Procedia Comput. Sci.",
  publisher = "Elsevier BV",
  volume    =  213,
  pages     = "510--517",
  year      =  2022,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Heredia2022-dt,
  title     = "Adaptive multimodal emotion detection architecture for social
               robots",
  author    = "Heredia, Juanpablo and Lopes-Silva, Edmundo and Cardinale,
               Yudith and Diaz-Amado, Jose and Dongo, Irvin and Graterol,
               Wilfredo and Aguilera, Ana",
  journal   = "IEEE Access",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  10,
  pages     = "20727--20744",
  year      =  2022,
  copyright = "https://creativecommons.org/licenses/by/4.0/legalcode"
}

@INPROCEEDINGS{Hung2020-gm,
  title      = "Multiple models using temporal feature learning for emotion
                recognition",
  booktitle  = "The 9th International Conference on Smart Media and
                Applications",
  author     = "Hung, Hoang Manh and Kim, Soo-Hyung and Yang, Hyung-Jeong and
                Lee, Guee-Sang",
  publisher  = "ACM",
  month      =  sep,
  year       =  2020,
  address    = "New York, NY, USA",
  conference = "SMA 2020: The 9th International Conference on Smart Media and
                Applications",
  location   = "Jeju Republic of Korea"
}

@INPROCEEDINGS{Yu2020-zq,
  title      = "Multimodal emotion recognition with thermal and {RGB-D} cameras
                for human-robot interaction",
  booktitle  = "Companion of the 2020 {ACM/IEEE} International Conference on
                {Human-Robot} Interaction",
  author     = "Yu, Chuang and Tapus, Adriana",
  publisher  = "ACM",
  month      =  mar,
  year       =  2020,
  address    = "New York, NY, USA",
  conference = "HRI '20: ACM/IEEE International Conference on Human-Robot
                Interaction",
  location   = "Cambridge United Kingdom"
}

@inproceedings{opensmile-2010,
author = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
title = {Opensmile: the munich versatile and fast open-source audio feature extractor},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874246},
doi = {10.1145/1873951.1874246},
abstract = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1459–1462},
numpages = {4},
keywords = {audio feature extraction, emotion, music, signal processing, speech, statistical functionals},
location = {Firenze, Italy},
series = {MM '10}
}

@ARTICLE{Kumar2022-gd,
  title     = "A survey on {IBM} Watson and its services",
  author    = "Kumar, Avinash and Tejaswini, Pallapothala and Nayak, Omprakash
               and Kujur, Anurag Deep and Gupta, Rajkiran and Rajanand, Ashish
               and Sahu, Mridu",
  abstract  = "Abstract Artificial Intelligence (AI) is changing the modern way
               of lifestyle by helping the person do their jobs in an efficient
               manner. The AI is currently in its starting phase and from now
               on it is of great use. IBM Watson is an AI which is used
               globally by different organizations, institutes and
               corporations. In this paper we have created a chatbot using IBM
               Watson Assistance which is helpful in querying about the disease
               and hospitals related query. This paper also discusses IBM
               Watson in detail, its applications, its working and case studies
               on the use of IBM Watson in the field of healthcare, visual
               recognition and a software company named BOX.",
  journal   = "J. Phys. Conf. Ser.",
  publisher = "IOP Publishing",
  volume    =  2273,
  number    =  1,
  pages     = "012022",
  month     =  may,
  year      =  2022,
  copyright = "http://creativecommons.org/licenses/by/3.0/"
}

@MISC{chatgpt,
  title        = "{ChatGPT}",
  howpublished = "\url{https://chat.openai.com/chat}",
  note         = "Accessed: 2023-9-10",
  year         = 2022,
  author       = "OpenAI"
}

@ARTICLE{Tan2021-ai,
  title     = "A multimodal emotion recognition method based on facial
               expressions and electroencephalography",
  author    = "Tan, Ying and Sun, Zhe and Duan, Feng and Sol{\'e}-Casals, Jordi
               and Caiafa, Cesar F",
  journal   = "Biomed. Signal Process. Control",
  publisher = "Elsevier BV",
  volume    =  70,
  number    =  103029,
  pages     = "103029",
  month     =  sep,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Younis2022-bs,
  title     = "Evaluating ensemble learning methods for multi-modal emotion
               recognition using sensor data fusion",
  author    = "Younis, Eman M G and Zaki, Someya Mohsen and Kanjo, Eiman and
               Houssein, Essam H",
  abstract  = "Automatic recognition of human emotions is not a trivial
               process. There are many factors affecting emotions internally
               and externally. Expressing emotions could also be performed in
               many ways such as text, speech, body gestures or even
               physiologically by physiological body responses. Emotion
               detection enables many applications such as adaptive user
               interfaces, interactive games, and human robot interaction and
               many more. The availability of advanced technologies such as
               mobiles, sensors, and data analytics tools led to the ability to
               collect data from various sources, which enabled researchers to
               predict human emotions accurately. Most current research uses
               them in the lab experiments for data collection. In this work,
               we use direct and real time sensor data to construct a
               subject-independent (generic) multi-modal emotion prediction
               model. This research integrates both on-body physiological
               markers, surrounding sensory data, and emotion measurements to
               achieve the following goals: (1) Collecting a multi-modal data
               set including environmental, body responses, and emotions. (2)
               Creating subject-independent Predictive models of emotional
               states based on fusing environmental and physiological
               variables. (3) Assessing ensemble learning methods and comparing
               their performance for creating a generic subject-independent
               model for emotion recognition with high accuracy and comparing
               the results with previous similar research. To achieve that, we
               conducted a real-world study ``in the wild'' with physiological
               and mobile sensors. Collecting the data-set is coming from
               participants walking around Minia university campus to create
               accurate predictive models. Various ensemble learning models
               (Bagging, Boosting, and Stacking) have been used, combining the
               following base algorithms (K Nearest Neighbor KNN, Decision Tree
               DT, Random Forest RF, and Support Vector Machine SVM) as base
               learners and DT as a meta-classifier. The results showed that,
               the ensemble stacking learner technique gave the best accuracy
               of 98.2\% compared with other variants of ensemble learning
               methods. On the contrary, bagging and boosting methods gave
               (96.4\%) and (96.6\%) accuracy levels respectively.",
  journal   = "Sensors (Basel)",
  publisher = "MDPI AG",
  volume    =  22,
  number    =  15,
  pages     = "5611",
  month     =  jul,
  year      =  2022,
  keywords  = "emotion recognition; ensemble learning; multi-modal emotion
               recognition; physiological and environmental; subject
               independent predictive models for emotion",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Dzedzickis2020-hf,
  title     = "Human emotion recognition: Review of sensors and methods",
  author    = "Dzedzickis, Andrius and Kaklauskas, Art{\=u}ras and Bucinskas,
               Vytautas",
  abstract  = "Automated emotion recognition (AEE) is an important issue in
               various fields of activities which use human emotional reactions
               as a signal for marketing, technical equipment, or human-robot
               interaction. This paper analyzes scientific research and
               technical papers for sensor use analysis, among various methods
               implemented or researched. This paper covers a few classes of
               sensors, using contactless methods as well as contact and
               skin-penetrating electrodes for human emotion detection and the
               measurement of their intensity. The results of the analysis
               performed in this paper present applicable methods for each type
               of emotion and their intensity and propose their classification.
               The classification of emotion sensors is presented to reveal
               area of application and expected outcomes from each method, as
               well as their limitations. This paper should be relevant for
               researchers using human emotion evaluation and analysis, when
               there is a need to choose a proper method for their purposes or
               to find alternative decisions. Based on the analyzed human
               emotion recognition sensors and methods, we developed some
               practical applications for humanizing the Internet of Things
               (IoT) and affective computing systems.",
  journal   = "Sensors (Basel)",
  publisher = "MDPI AG",
  volume    =  20,
  number    =  3,
  pages     = "592",
  month     =  jan,
  year      =  2020,
  keywords  = "emotion perception; human emotions; physiologic sensors",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@INCOLLECTION{Stanley2023-ai,
  title     = "Age and emotion",
  booktitle = "Encyclopedia of Mental Health",
  author    = "Stanley, Jennifer Tehan and Villalba, Anthony",
  publisher = "Elsevier",
  pages     = "35--43",
  year      =  2023
}

@ARTICLE{Pal2021-eq,
  title     = "Development and progress in sensors and technologies for human
               emotion recognition",
  author    = "Pal, Shantanu and Mukhopadhyay, Subhas and Suryadevara, Nagender",
  abstract  = "With the advancement of human-computer interaction, robotics,
               and especially humanoid robots, there is an increasing trend for
               human-to-human communications over online platforms (e.g.,
               zoom). This has become more significant in recent years due to
               the Covid-19 pandemic situation. The increased use of online
               platforms for communication signifies the need to build
               efficient and more interactive human emotion recognition
               systems. In a human emotion recognition system, the
               physiological signals of human beings are collected, analyzed,
               and processed with the help of dedicated learning techniques and
               algorithms. With the proliferation of emerging technologies,
               e.g., the Internet of Things (IoT), future Internet, and
               artificial intelligence, there is a high demand for building
               scalable, robust, efficient, and trustworthy human recognition
               systems. In this paper, we present the development and progress
               in sensors and technologies to detect human emotions. We review
               the state-of-the-art sensors used for human emotion recognition
               and different types of activity monitoring. We present the
               design challenges and provide practical references of such human
               emotion recognition systems in the real world. Finally, we
               discuss the current trends in applications and explore the
               future research directions to address issues, e.g., scalability,
               security, trust, privacy, transparency, and decentralization.",
  journal   = "Sensors (Basel)",
  publisher = "MDPI AG",
  volume    =  21,
  number    =  16,
  pages     = "5554",
  month     =  aug,
  year      =  2021,
  keywords  = "Internet of Things; human emotion; motion analysis;
               physiological parameters monitoring; sensors; wearable sensors;
               wireless communications",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}


@MISC{Nawasalkar2017-fx,
  title     = "Study of comparison of human bio-signals for emotion detection
               using {HCI}",
  author    = "Nawasalkar, Ram K and Butey, Pradeep K",
  publisher = "Unpublished",
  year      =  2017
}


@ARTICLE{Suhaila2021-dh,
  title     = "A survey on emotion recognition for human Robot Interaction",
  abstract  = "With the recent developments of technology and the advances in
               artificial intelligent and machine learning techniques, it
               becomes possible for the robot to acquire and show the emotions
               as a part of Human-Robot Interaction (HRI). An emotional robot
               can recognize the emotional states of humans so that it will be
               able to interact more naturally with its human counterpart in
               different environments. In this article, a survey on emotion
               recognition for HRI systems has been presented. The survey aims
               to achieve two objectives. Firstly, it aims to discuss the main
               challenges that face researchers when building emotional HRI
               systems. Secondly, it seeks to identify sensing channels that
               can be used to detect emotions and provides a literature review
               about recent researches published within each channel, along
               with the used methodologies and achieved results. Finally, some
               of the existing emotion recognition issues and recommendations
               for future works have been outlined.",
  journal   = "J. Comput. Inf. Technol.",
  publisher = "Faculty of Electrical Engineering and Computing, Univ. of Zagreb",
  volume    =  28,
  number    =  2,
  pages     = "125--146",
  month     =  jun,
  year      =  2021,
  author  = "Suhaila N. Mohammed and Alia Karmin"
}

@INPROCEEDINGS{Lubecke-4751478,
  author={Boric-Lubecke, Olga and Massagram, Wansuree and Lubecke, Victor M. and Host-Madsen, Anders and Jokanovic, Branka},
  booktitle={2008 38th European Microwave Conference}, 
  title={Heart Rate Variability Assessment Using Doppler Radar with Linear Demodulation}, 
  year={2008},
  volume={},
  number={},
  pages={420-423},
  keywords={Heart rate variability;Doppler radar;Demodulation;Heart rate;Remote monitoring;Cardiology;Stress;Radar detection;Particle measurements;Fluctuations},
  doi={10.1109/EUMC.2008.4751478}}

@article{CHEN201849,
title = {Softmax regression based deep sparse autoencoder network for facial emotion recognition in human-robot interaction},
journal = {Information Sciences},
volume = {428},
pages = {49-61},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.10.044},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517310496},
author = {Luefeng Chen and Mengtian Zhou and Wanjuan Su and Min Wu and Jinhua She and Kaoru Hirota},
keywords = {Facial emotion recognition, Deep sparse autoencoder network, Softmax regression, Human-robot interaction},
abstract = {Deep neural network (DNN) has been used as a learning model for modeling the hierarchical architecture of human brain. However, DNN suffers from problems of learning efficiency and computational complexity. To address these problems, deep sparse autoencoder network (DSAN) is used for learning facial features, which considers the sparsity of hidden units for learning high-level structures. Meanwhile, Softmax regression (SR) is used to classify expression feature. In this paper, Softmax regression-based deep sparse autoencoder network (SRDSAN) is proposed to recognize facial emotion in human-robot interaction. It aims to handle large data in the output of deep learning by using SR, moreover, to overcome local extrema and gradient diffusion problems in the training process, the overall network weights are fine-tuned to reach the global optimum, which makes the entire depth of the neural network more robust, thereby enhancing the performance of facial emotion recognition. Results show that the average recognition accuracy of SRDSAN is higher than that of the SR and the convolutional neural network. The preliminarily application experiments are performed in the developing emotional social robot system (ESRS) with two mobile robots, where emotional social robot is able to recognize emotions such as happiness and angry.}
}

@article{CHEN202010236,
title = {CNN-based Broad Learning with Efficient Incremental Reconstruction Model for Facial Emotion Recognition},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {10236-10241},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2754},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320335175},
author = {Luefeng Chen and Min Li and Xuzhi Lai and Kaoru Hirota and Witold Pedrycz},
keywords = {Convolution Neural Networks, Broad Learning, Emotion Recognition, Human-Robot Interaction},
abstract = {Convolutional neural network-based broad learning with efficient incremental reconstruction model (CNNBL) is proposed to recognize emotions in human-robot interaction. It aims to extract deep and abstract features from facial emotional images, and reduce the influence of the complex structure and slow network updates on facial emotion recognition in deep learning. Feature extraction is carried out by convolution and maximum pooling, and then the ridge regression algorithm is used for emotion recognition. When the network needs to expand, the network is dynamically updated by incremental learning algorithm. We verified the experimental performance through k-fold cross validation. According to the recognition results, the accuracy on JAFFE database of our proposal is greater than that of the state of the art, such as the Local Binary Patterns with Softmax and Deep Attentive Multi-path convolutional neural network.}
}

@INPROCEEDINGS{10008155,
  author={Yang, Ping and Cao, Li Mei and Zhu, Lin Ling and Luo, Shun Nian},
  booktitle={2022 10th International Conference on Orange Technology (ICOT)}, 
  title={Design of Attendance System Based on NAO Face, Speech and Emotion Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1-3},
  keywords={Human computer interaction;Emotion recognition;Face recognition;Speech recognition;User experience;Social implications of technology;Robots;NAO;Face recognition;Attendance system;Emotion recognition},
  doi={10.1109/ICOT56925.2022.10008155}}

@article{10.1007/s11042-022-12794-3,
author = {Jaiswal, Shruti and Nandi, Gora Chand},
title = {Optimized, robust, real-time emotion prediction for human-robot interactions using deep learning},
year = {2022},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {82},
number = {4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-022-12794-3},
doi = {10.1007/s11042-022-12794-3},
abstract = {To enable humanoid robots to share the social space,development in technology is required for natural interaction with the robots using multiple modes of communication such as speech, gestures, and share emotions with them. This research is targeted towards addressing the core issue of emotion recognition problem, which would require fewer computation resources and a much lesser number of network parameters, which will be more adaptive to compute on social robots for real-time communication. Any robots will have limited computation capability for run time actions and decisions. In the present investigation, Inception based Convolution Neural Network(CNN) Architecture is proposed to improve the emotion prediction. The proposed model has achieved improved accuracy of up to 6\% improvement over the existing network architecture for emotion classification. The model was tested over seven different datasets to verify its robustness. In addition, real-time implementation capability is verified on humanoid robot NAO, which depicts its social behavior in real-time. The proposed model is reducing the trainable parameters to the extent of 94\% as compared to vanilla CNN model, which indicates that its implementation ability in a real-time based application such as human-robot interaction. Rigorous experiments have been performed to validate the methodology, which is sufficiently robust and could achieve a high level of accuracy. Seven datasets are used to build a robust model. Finally, the model is integrated in a humanoid robot, NAO, in real-time. When averaged over all the emotions, the reduction in response time by 60\% and 61\% and improvement in prediction rate by 42\% and 21\% when compared in real-time environment with Vanilla CNN and state of the art model respectively.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {5495–5519},
numpages = {25},
keywords = {Emotion classification, Optimized deep learning, Convolution neural network, Inception module, Human-robot interaction, Social robotics}
}

@INPROCEEDINGS{9900581,
  author={Shenoy, Sudhir and Jiang, Yusheng and Lynch, Tyler and Manuel, Lauren Isabelle and Doryab, Afsaneh},
  booktitle={2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={A Self Learning System for Emotion Awareness and Adaptation in Humanoid Robots}, 
  year={2022},
  volume={},
  number={},
  pages={912-919},
  keywords={Emotion recognition;Adaptation models;Adaptive learning;Transfer learning;Humanoid robots;Real-time systems;Data models;emotion recognition;engagement;user-adaptive;social robot;deep learning},
  doi={10.1109/RO-MAN53752.2022.9900581}}

@ARTICLE{9982640,
  author={Hwang, Chih-Lyang and Deng, Yu-Chen and Pu, Shih-En},
  journal={IEEE Access}, 
  title={Human–Robot Collaboration Using Sequential-Recurrent-Convolution-Network-Based Dynamic Face Emotion and Wireless Speech Command Recognitions}, 
  year={2023},
  volume={11},
  number={},
  pages={37269-37282},
  keywords={Face recognition;Speech recognition;Emotion recognition;Service robots;Human-robot interaction;Image recognition;Collaboration;Human--robot collaboration;CNN;LSTM;human and face detection;dynamic face emotion recognition;wireless speech command recognition;omnidirectional service robot;visual searching and tracking;adaptive stratified finite-time saturated control.},
  doi={10.1109/ACCESS.2022.3228825}}

@INPROCEEDINGS{8588580,
  author={Mazzoni Ranieri, Caetano and Vicentim Nardari, Guilherme and Moreira Pinto, Adam Henrique and Carnieto Tozadore, Daniel and Francelin Romero, Roseli Aparecida},
  booktitle={2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)}, 
  title={LARa: A Robotic Framework for Human-Robot Interaction on Indoor Environments}, 
  year={2018},
  volume={},
  number={},
  pages={376-382},
  keywords={Robot sensing systems;Face;Speech recognition;Face recognition;Human-robot interaction;Human Robot Interaction;Social Robot;Control Architecture;Robotic Framework},
  doi={10.1109/LARS/SBR/WRE.2018.00074}}

@ARTICLE{8760246,
  author={Li, Tzuu-Hseng S. and Kuo, Ping-Huan and Tsai, Ting-Nan and Luan, Po-Chien},
  journal={IEEE Access}, 
  title={CNN and LSTM Based Facial Expression Analysis Model for a Humanoid Robot}, 
  year={2019},
  volume={7},
  number={},
  pages={93998-94011},
  keywords={Emotion recognition;Feature extraction;Face recognition;Image recognition;Task analysis;Humanoid robots;Convolutional neural network;long short-term memory;transfer learning;facial expression analysis},
  doi={10.1109/ACCESS.2019.2928364}}

@INPROCEEDINGS{8578328,
  author={Marinoiu, Elisabeta and Zanfir, Mihai and Olaru, Vlad and Sminchisescu, Cristian},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism}, 
  year={2018},
  volume={},
  number={},
  pages={2158-2167},
  keywords={Medical treatment;Autism;Three-dimensional displays;Videos;Cameras;Robot vision systems},
  doi={10.1109/CVPR.2018.00230}}

@INPROCEEDINGS{Ashok2022-xp,
  title           = "Paralinguistic cues in speech to adapt robot behavior in
                     human-robot interaction",
  booktitle       = "2022 9th {IEEE} {RAS/EMBS} International Conference for
                     Biomedical Robotics and Biomechatronics ({BioRob})",
  author          = "Ashok, Ashita and Pawlak, Jakub and Paplu, Sarwar and
                     Zafar, Zuhair and Berns, Karsten",
  publisher       = "IEEE",
  month           =  aug,
  year            =  2022,
  conference      = "2022 9th IEEE RAS/EMBS International Conference for
                     Biomedical Robotics and Biomechatronics (BioRob)",
  location        = "Seoul, Korea, Republic of"
}

@ARTICLE{Augello2022-hm,
  title     = "Multimodal mood recognition for assistive scenarios",
  author    = "Augello, Agnese and Bella, Giulia Di and Infantino, Ignazio and
               Pilato, Giovanni and Vitale, Gianpaolo",
  journal   = "Procedia Comput. Sci.",
  publisher = "Elsevier BV",
  volume    =  213,
  pages     = "510--517",
  year      =  2022,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Khan2023-nz,
  title     = "Improved multi-lingual sentiment analysis and recognition using
               deep learning",
  author    = "Khan, Amjad",
  abstract  = "Speech emotion recognition (SER) is still a fresh in natural
               language processing domain since the accuracy is beyond
               targeted. Mainly due to real-time applications such as
               human--robot interaction, human behaviour evaluation and virtual
               reality rely heavily on SER. Moreover, cross-lingual SER plays a
               significant role in practical applications, especially when
               users of different cultural and linguistic backgrounds interact
               with the system. However, the existing conventional approaches
               of SER cannot be employed for real-world applications because it
               uses the same corpus for training and testing, which cannot be
               used for multi-lingual environments to detect or classify real
               emotions. In such a situation, the performance of SER is
               degraded. Therefore, the proposed work develops cross-lingual
               emotion recognition through Urdu, Italian, English and German.
               The features are extracted through the most employed audio
               feature known as MFCCs (Mel Frequency Cepstral Coefficients).
               Experimental results exhibited that the proposed deep learning
               model comes out with promising results on the URDU data set with
               91.25\% accuracy using random forest (RF) and XGBoost
               classifier.",
  journal   = "J. Inf. Sci.",
  publisher = "SAGE Publications",
  pages     = "016555152211372",
  month     =  jan,
  year      =  2023,
  language  = "en"
}

@inproceedings{yang2016wider,
    Author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
    Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    Title = {WIDER FACE: A Face Detection Benchmark},
    Year = {2016}
}

@inproceedings{BarsoumICMI2016,
    title={Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution},
    author={Barsoum, Emad and Zhang, Cha and Canton Ferrer, Cristian and Zhang, Zhengyou},
    booktitle={ACM International Conference on Multimodal Interaction (ICMI)},
    year={2016}
}

@INPROCEEDINGS{5543262,
  author={Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops}, 
  title={The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression}, 
  year={2010},
  volume={},
  number={},
  pages={94-101},
  keywords={Databases;Gold;Active appearance model;Support vector machines;Support vector machine classification;Face detection;Testing;Performance evaluation;Measurement;Code standards},
  doi={10.1109/CVPRW.2010.5543262}}

@INPROCEEDINGS{Ali2021-ie,
  title           = "Mel frequency cepstral coefficient: A review",
  booktitle       = "Proceedings of the 2nd International Conference on {ICT}
                     for Digital, Smart, and Sustainable Development, {ICIDSSD}
                     2020, 27-28 February 2020, Jamia Hamdard, New Delhi, India",
  author          = "Ali, Shalbbya and Tanweer, Safdar and Khalid, Syed and
                     Rao, Naseem",
  publisher       = "EAI",
  year            =  2021,
  conference      = "Proceedings of the 2nd International Conference on ICT for
                     Digital, Smart, and Sustainable Development, ICIDSSD 2020,
                     27-28 February 2020, Jamia Hamdard, New Delhi, India",
  location        = "New Delhi, India"
}

@INPROCEEDINGS{Shi2016-th,
  title           = "Robust speaker recognition based on improved {GFCC}",
  booktitle       = "2016 2nd {IEEE} International Conference on Computer and
                     Communications ({ICCC})",
  author          = "Shi, Xiaoyuan and Yang, Haiyan and Zhou, Ping",
  publisher       = "IEEE",
  month           =  oct,
  year            =  2016,
  conference      = "2016 2nd IEEE International Conference on Computer and
                     Communications (ICCC)",
  location        = "Chengdu, China"
}

@ARTICLE{OShaughnessy1988-ws,
  title     = "Linear predictive coding",
  author    = "O'Shaughnessy, D",
  journal   = "IEEE Potentials",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  7,
  number    =  1,
  pages     = "29--32",
  month     =  feb,
  year      =  1988,
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html"
}

@INPROCEEDINGS{6895780,
  author={Li, Penghua and Hu, Fangchao and Li, Yinguo and Xu, Yang},
  booktitle={Proceedings of the 33rd Chinese Control Conference}, 
  title={Speaker identification using linear predictive cepstral coefficients and general regression neural network}, 
  year={2014},
  volume={},
  number={},
  pages={4952-4956},
  keywords={Speech;Vectors;Partitioning algorithms;Training;Neural networks;Algorithm design and analysis;Cepstrum;General regression neural network;Speaker identification;Linear predictive cepstrum coefficients;Non-linear partition algorithm},
  doi={10.1109/ChiCC.2014.6895780}}

@BOOK{Picard2000-mt,
  title     = "Affective Computing",
  author    = "Picard, Rosalind W",
  publisher = "MIT Press",
  series    = "The MIT Press",
  month     =  jul,
  year      =  2000,
  address   = "London, England"
}

@ARTICLE{Goodfellow2013-al,
  title         = "Challenges in Representation Learning: A report on three
                   machine learning contests",
  author        = "Goodfellow, Ian J and Erhan, Dumitru and Carrier, Pierre Luc
                   and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and
                   Cukierski, Will and Tang, Yichuan and Thaler, David and Lee,
                   Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng,
                   Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis,
                   Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park,
                   John and Ionescu, Radu and Popescu, Marius and Grozea,
                   Cristian and Bergstra, James and Xie, Jingjing and Romaszko,
                   Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua",
  abstract      = "The ICML 2013 Workshop on Challenges in Representation
                   Learning focused on three challenges: the black box learning
                   challenge, the facial expression recognition challenge, and
                   the multimodal learning challenge. We describe the datasets
                   created for these challenges and summarize the results of
                   the competitions. We provide suggestions for organizers of
                   future challenges and some comments on what kind of
                   knowledge can be gained from machine learning competitions.",
  month         =  jul,
  year          =  2013,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1307.0414"
}

@ARTICLE{Mollahosseini2017-bj,
  title         = "{AffectNet}: A database for facial expression, valence, and
                   arousal computing in the wild",
  author        = "Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H",
  abstract      = "Automated affective computing in the wild setting is a
                   challenging problem in computer vision. Existing annotated
                   databases of facial expressions in the wild are small and
                   mostly cover discrete emotions (aka the categorical model).
                   There are very limited annotated facial databases for
                   affective computing in the continuous dimensional model
                   (e.g., valence and arousal). To meet this need, we
                   collected, annotated, and prepared for public distribution a
                   new database of facial emotions in the wild (called
                   AffectNet). AffectNet contains more than 1,000,000 facial
                   images from the Internet by querying three major search
                   engines using 1250 emotion related keywords in six different
                   languages. About half of the retrieved images were manually
                   annotated for the presence of seven discrete facial
                   expressions and the intensity of valence and arousal.
                   AffectNet is by far the largest database of facial
                   expression, valence, and arousal in the wild enabling
                   research in automated facial expression recognition in two
                   different emotion models. Two baseline deep neural networks
                   are used to classify images in the categorical model and
                   predict the intensity of valence and arousal. Various
                   evaluation metrics show that our deep neural network
                   baselines can perform better than conventional machine
                   learning methods and off-the-shelf facial expression
                   recognition systems.",
  month         =  aug,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1708.03985"
}

@inproceedings{jaffe,
author = {Alshamsi, Humaid and Këpuska, Veton and Meng, Hongying},
year = {2017},
month = {11},
pages = {},
title = {Real Time Automated Facial Expression Recognition App Development on Smart Phones},
doi = {10.1109/IEMCON.2017.8117150}
}

@MISC{Lundqvist2015-in,
  title     = "Karolinska Directed Emotional Faces",
  author    = "Lundqvist, D and Flykt, A and {\"O}hman, A",
  publisher = "American Psychological Association (APA)",
  month     =  may,
  year      =  2015,
  note      = "Title of the publication associated with this dataset: PsycTESTS
               Dataset"
}

@ARTICLE{Livingstone2018-li,
  title     = "The Ryerson {Audio-Visual} Database of Emotional Speech and Song
               ({RAVDESS)}: A dynamic, multimodal set of facial and vocal
               expressions in North American English",
  author    = "Livingstone, Steven R and Russo, Frank A",
  abstract  = "The RAVDESS is a validated multimodal database of emotional
               speech and song. The database is gender balanced consisting of
               24 professional actors, vocalizing lexically-matched statements
               in a neutral North American accent. Speech includes calm, happy,
               sad, angry, fearful, surprise, and disgust expressions, and song
               contains calm, happy, sad, angry, and fearful emotions. Each
               expression is produced at two levels of emotional intensity,
               with an additional neutral expression. All conditions are
               available in face-and-voice, face-only, and voice-only formats.
               The set of 7356 recordings were each rated 10 times on emotional
               validity, intensity, and genuineness. Ratings were provided by
               247 individuals who were characteristic of untrained research
               participants from North America. A further set of 72
               participants provided test-retest data. High levels of emotional
               validity and test-retest intrarater reliability were reported.
               Corrected accuracy and composite ``goodness'' measures are
               presented to assist researchers in the selection of stimuli. All
               recordings are made freely available under a Creative Commons
               license and can be downloaded at
               https://doi.org/10.5281/zenodo.1188976.",
  journal   = "PLoS One",
  publisher = "Public Library of Science (PLoS)",
  volume    =  13,
  number    =  5,
  pages     = "e0196391",
  month     =  may,
  year      =  2018,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@INPROCEEDINGS{1699093,
  author={Gunes, H. and Piccardi, M.},
  booktitle={18th International Conference on Pattern Recognition (ICPR'06)}, 
  title={A Bimodal Face and Body Gesture Database for Automatic Analysis of Human Nonverbal Affective Behavior}, 
  year={2006},
  volume={1},
  number={},
  pages={1148-1153},
  keywords={Databases;Data analysis;Emotion recognition;Face recognition;Computer vision;Pattern recognition;Human computer interaction;Face detection;Australia;System testing},
  doi={10.1109/ICPR.2006.39}}

@ARTICLE{Singh2020-ui,
  title     = "Face recognition using {HOG} feature extraction and {SVM}
               classifier",
  author    = "Singh, Swarnima and Singh, Durgesh and Yadav, Vikash",
  journal   = "Int. J. Emerg. Trends Eng. Res.",
  publisher = "The World Academy of Research in Science and Engineering",
  volume    =  8,
  number    =  9,
  pages     = "6437--6440",
  month     =  sep,
  year      =  2020
}

@ARTICLE{Shorten2019-mj,
  title     = "A survey on image data augmentation for deep learning",
  author    = "Shorten, Connor and Khoshgoftaar, Taghi M",
  abstract  = "Deep convolutional neural networks have performed remarkably
               well on many Computer Vision tasks. However, these networks are
               heavily reliant on big data to avoid overfitting. Overfitting
               refers to the phenomenon when a network learns a function with
               very high variance such as to perfectly model the training data.
               Unfortunately, many application domains do not have access to
               big data, such as medical image analysis. This survey focuses on
               Data Augmentation, a data-space solution to the problem of
               limited data. Data Augmentation encompasses a suite of
               techniques that enhance the size and quality of training
               datasets such that better Deep Learning models can be built
               using them. The image augmentation algorithms discussed in this
               survey include geometric transformations, color space
               augmentations, kernel filters, mixing images, random erasing,
               feature space augmentation, adversarial training, generative
               adversarial networks, neural style transfer, and meta-learning.
               The application of augmentation methods based on GANs are
               heavily covered in this survey. In addition to augmentation
               techniques, this paper will briefly discuss other
               characteristics of Data Augmentation such as test-time
               augmentation, resolution impact, final dataset size, and
               curriculum learning. This survey will present existing methods
               for Data Augmentation, promising developments, and meta-level
               decisions for implementing Data Augmentation. Readers will
               understand how Data Augmentation can improve the performance of
               their models and expand limited datasets to take advantage of
               the capabilities of big data.",
  journal   = "J. Big Data",
  publisher = "Springer Science and Business Media LLC",
  volume    =  6,
  number    =  1,
  month     =  dec,
  year      =  2019,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@misc{openai2024,
	author = {OpenAI},
	title = {{C}hat{G}{P}{T} {M}odels},
	howpublished = {\url{https://platform.openai.com/docs/models/o1}},
	year = {},
	note = {[Accessed 22-10-2024]},
}