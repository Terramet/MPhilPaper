@article{Sisbot2010-wy,
  title     = {Synthesizing robot motions adapted to human presence},
  author    = {Sisbot, Emrah Akin and Marin-Urias, Luis F and Broqu{\`e}re,
               Xavier and Sidobre, Daniel and Alami, Rachid},
  journal   = {Int. J. Soc. Robot.},
  publisher = {Springer Science and Business Media LLC},
  volume    = 2,
  number    = 3,
  pages     = {329--343},
  month     = sep,
  year      = 2010,
  language  = {en}
}

@article{Chuah2021-zw,
  title     = {The future of service: The power of emotion in human-robot
               interaction},
  author    = {Chuah, Stephanie Hui-Wen and Yu, Joanne},
  journal   = {J. Retail. Consum. Serv.},
  publisher = {Elsevier BV},
  volume    = 61,
  number    = 102551,
  pages     = {102551},
  month     = jul,
  year      = 2021,
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}


@article{Matsumoto1992-jf,
  title     = {More evidence for the universality of a contempt expression},
  author    = {Matsumoto, David},
  abstract  = {Since its publication in 1986, Ekman and Friesen's (1986)
               discovery of a universal facial expression unique to contempt
               has received considerable attention (e.g., see Ekman \& Friesen,
               1988; Ekman \& Heider, 1988; Ekman, O'Sullivan, \& Matsumoto,
               1991a, 1991b; Izard \& Haynes, 1988; Russell, 1991a, 1991b;
               Ricci Bitti, Brighetti, Garotti, Boggi-Cavallo, 1989). Actually,
               much of this argument began before there was adequate sampling
               of contempt photographs across many cultures. In order to
               address this concern, this study reports judgment data on all 12
               photos used in previous studies depicting the contempt
               expression from four non-American cultures. The data provide a
               strong replication of Ekman and Friesen's (1986) and Ekman and
               Heider's (1988) findings for a universal expression of contempt.},
  journal   = {Motiv. Emot.},
  publisher = {Springer Science and Business Media LLC},
  volume    = 16,
  number    = 4,
  pages     = {363--368},
  month     = dec,
  year      = 1992,
  language  = {en}
}

@article{Filippini2021-ni,
  title     = {Improving human-robot interaction by enhancing {NAO} robot
               awareness of human facial expression},
  author    = {Filippini, Chiara and Perpetuini, David and Cardone, Daniela and
               Merla, Arcangelo},
  abstract  = {An intriguing challenge in the human-robot interaction field is
               the prospect of endowing robots with emotional intelligence to
               make the interaction more genuine, intuitive, and natural. A
               crucial aspect in achieving this goal is the robot's capability
               to infer and interpret human emotions. Thanks to its design and
               open programming platform, the NAO humanoid robot is one of the
               most widely used agents for human interaction. As with
               person-to-person communication, facial expressions are the
               privileged channel for recognizing the interlocutor's emotional
               expressions. Although NAO is equipped with a facial expression
               recognition module, specific use cases may require additional
               features and affective computing capabilities that are not
               currently available. This study proposes a highly accurate
               convolutional-neural-network-based facial expression recognition
               model that is able to further enhance the NAO robot' awareness
               of human facial expressions and provide the robot with an
               interlocutor's arousal level detection capability. Indeed, the
               model tested during human-robot interactions was 91\% and 90\%
               accurate in recognizing happy and sad facial expressions,
               respectively; 75\% accurate in recognizing surprised and scared
               expressions; and less accurate in recognizing neutral and angry
               expressions. Finally, the model was successfully integrated into
               the NAO SDK, thus allowing for high-performing facial expression
               classification with an inference time of 0.34 $\pm$ 0.04 s.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 21,
  number    = 19,
  pages     = {6438},
  month     = sep,
  year      = 2021,
  keywords  = {affective computing; emotion recognition; facial expression
               recognition; human-robot interaction; machine learning},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}


@article{Szaboova2020-ru,
  title     = {Emotion analysis in human--robot interaction},
  author    = {Szab{\'o}ov{\'a}, Martina and Sarnovsk{\'y}, Martin and Maslej
               Kre{\v s}{\v n}{\'a}kov{\'a}, Viera and Machov{\'a},
               Krist{\'\i}na},
  abstract  = {This paper connects two large research areas, namely sentiment
               analysis and human--robot interaction. Emotion analysis, as a
               subfield of sentiment analysis, explores text data and, based on
               the characteristics of the text and generally known emotional
               models, evaluates what emotion is presented in it. The analysis
               of emotions in the human--robot interaction aims to evaluate the
               emotional state of the human being and on this basis to decide
               how the robot should adapt its behavior to the human being.
               There are several approaches and algorithms to detect emotions
               in the text data. We decided to apply a combined method of
               dictionary approach with machine learning algorithms. As a
               result of the ambiguity and subjectivity of labeling emotions,
               it was possible to assign more than one emotion to a sentence;
               thus, we were dealing with a multi-label problem. Based on the
               overview of the problem, we performed experiments with the Naive
               Bayes, Support Vector Machine and Neural Network classifiers.
               Results obtained from classification were subsequently used in
               human--robot experiments. Despise the lower accuracy of emotion
               classification, we proved the importance of expressing emotion
               gestures based on the words we speak.},
  journal   = {Electronics (Basel)},
  publisher = {MDPI AG},
  volume    = 9,
  number    = 11,
  pages     = {1761},
  month     = oct,
  year      = 2020,
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@article{Valagkouti2022-xf,
  title     = {Emotion recognition in human--robot interaction using the {NAO}
               robot},
  author    = {Valagkouti, Iro Athina and Troussas, Christos and Krouska,
               Akrivi and Feidakis, Michalis and Sgouropoulou, Cleo},
  abstract  = {Affective computing can be implemented across many fields in
               order to provide a unique experience by tailoring services and
               products according to each person's needs and interests. More
               specifically, digital learning and robotics in education can
               benefit from affective computing with a redesign of the
               curriculum's contents based on students' emotions during
               teaching. This key feature is observed during traditional
               learning methods, and robot tutors are adapting to it gradually.
               Following this trend, this work focused on creating a game that
               aims to raise environmental awareness by using the social robot
               NAO as a conversation agent. This quiz-like game supports
               emotion recognition with DeepFace, allowing users to review
               their answers if a negative emotion is detected. A version of
               this game was tested during real-life circumstances and produced
               favorable results, both for emotion analysis and overall user
               enjoyment.},
  journal   = {Computers},
  publisher = {MDPI AG},
  volume    = 11,
  number    = 5,
  pages     = {72},
  month     = may,
  year      = 2022,
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}
@misc{Demutti2022-vz,
  title     = {A cloud architecture for emotion recognition in human-robot
               interaction based on the appraisal theory},
  author    = {Demutti, Marco and D'Amato, Vincenzo and Oneto, Luca and
               Sgorbissa, Antonio and Recchiuto, Carmine},
  abstract  = {This work proposes a cloud system, structured as a set of REST
               API endpoints, for online human emotion recognition in
               spontaneous human-robot verbal interaction. Based on the
               appraisal theory of emotion, the system acquires data about the
               person's expected appraisal of a given situation, depending on
               their needs and goals, and combines it with sensory data, such
               as facial expressions, angles of the head, and gaze of the
               person, and distance between the person and the robot. The whole
               set of data is used to infer the emotional state of the person
               during the interaction through a Random Forest classifier,
               trained for binary classification (i.e., positive vs. negative
               emotions). Results confirmed that using both sources of data led
               to a performance improvement both in the K-fold and in the Leave
               One Person Out scenarios.},
  publisher = {Zenodo},
  year      = 2022
}
@article{Alonso-Martin2013-cv,
  title     = {A multimodal emotion detection system during human-robot
               interaction},
  author    = {Alonso-Mart{\'\i}n, Fernando and Malfaz, Mar{\'\i}a and
               Sequeira, Jo{\~a}o and Gorostiza, Javier F and Salichs, Miguel A},
  abstract  = {In this paper, a multimodal user-emotion detection system for
               social robots is presented. This system is intended to be used
               during human-robot interaction, and it is integrated as part of
               the overall interaction system of the robot: the Robotics Dialog
               System (RDS). Two modes are used to detect emotions: the voice
               and face expression analysis. In order to analyze the voice of
               the user, a new component has been developed: Gender and Emotion
               Voice Analysis (GEVA), which is written using the Chuck
               language. For emotion detection in facial expressions, the
               system, Gender and Emotion Facial Analysis (GEFA), has been also
               developed. This last system integrates two third-party
               solutions: Sophisticated High-speed Object Recognition Engine
               (SHORE) and Computer Expression Recognition Toolbox (CERT). Once
               these new components (GEVA and GEFA) give their results, a
               decision rule is applied in order to combine the information
               given by both of them. The result of this rule, the detected
               emotion, is integrated into the dialog system through
               communicative acts. Hence, each communicative act gives, among
               other things, the detected emotion of the user to the RDS so it
               can adapt its strategy in order to get a greater satisfaction
               degree during the human-robot dialog. Each of the new
               components, GEVA and GEFA, can also be used individually.
               Moreover, they are integrated with the robotic control platform
               ROS (Robot Operating System). Several experiments with real
               users were performed to determine the accuracy of each component
               and to set the final decision rule. The results obtained from
               applying this decision rule in these experiments show a high
               success rate in automatic user emotion recognition, improving
               the results given by the two information channels (audio and
               visual) separately.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 13,
  number    = 11,
  pages     = {15549--15581},
  month     = nov,
  year      = 2013,
  copyright = {https://creativecommons.org/licenses/by/3.0/},
  language  = {en}
}

@article{Stock-Homburg2022-wd,
  title     = {Survey of emotions in human--robot interactions: Perspectives
               from robotic psychology on 20 years of research},
  author    = {Stock-Homburg, Ruth},
  abstract  = {AbstractKnowledge production within the interdisciplinary field
               of human--robot interaction (HRI) with social robots has
               accelerated, despite the continued fragmentation of the research
               domain. Together, these features make it hard to remain at the
               forefront of research or assess the collective evidence
               pertaining to specific areas, such as the role of emotions in
               HRI. This systematic review of state-of-the-art research into
               humans' recognition and responses to artificial emotions of
               social robots during HRI encompasses the years 2000--2020. In
               accordance with a stimulus--organism--response framework, the
               review advances robotic psychology by revealing current
               knowledge about (1) the generation of artificial robotic
               emotions (stimulus), (2) human recognition of robotic artificial
               emotions (organism), and (3) human responses to robotic emotions
               (response), as well as (4) other contingencies that affect
               emotions as moderators.},
  journal   = {Int. J. Soc. Robot.},
  publisher = {Springer Science and Business Media LLC},
  volume    = 14,
  number    = 2,
  pages     = {389--411},
  month     = mar,
  year      = 2022,
  copyright = {https://creativecommons.org/licenses/by/4.0},
  language  = {en}
}

@article{Reyes2019-go,
  title     = {Robotics facial expression of anger in collaborative
               human--robot interaction},
  author    = {Reyes, Mauricio E and Meza, Ivan V and Pineda, Luis A},
  abstract  = {The facial expression of angry emotion can be useful to direct
               the interaction between agents, especially in unclear and
               cluttered environments. During the presence of an angry face, a
               process of analysis and diagnosis is activated in the subject
               that notices it, which could impact its behavior toward the one
               who expresses the emotion. In order to study such an effect in
               human--robot interaction, an expressive robotics face was
               designed and constructed. The influence of this face on human
               action and attention was analyzed in two collaborative tasks.
               Results of a digital survey, experimental interaction, and a
               questionnaire indicated that anger is the best recognized
               universal facial expression, has a regulatory effect in human
               action, and induces human attention when an unclear condition
               arises during the task. An additional finding was that the
               prolonged presence of an angry face reduces its impact compared
               to positive expressions.},
  journal   = {Int. J. Adv. Robot. Syst.},
  publisher = {SAGE Publications},
  volume    = 16,
  number    = 1,
  pages     = {172988141881797},
  month     = jan,
  year      = 2019,
  language  = {en}
}
@article{Ramis2020-ec,
  title     = {Using a social robot to evaluate facial expressions in the wild},
  author    = {Ramis, Silvia and Buades, Jose Maria and Perales, Francisco J},
  abstract  = {In this work an affective computing approach is used to study
               the human-robot interaction using a social robot to validate
               facial expressions in the wild. Our global goal is to evaluate
               that a social robot can be used to interact in a convincing
               manner with human users to recognize their potential emotions
               through facial expressions, contextual cues and bio-signals. In
               particular, this work is focused on analyzing facial expression.
               A social robot is used to validate a pre-trained convolutional
               neural network (CNN) which recognizes facial expressions. Facial
               expression recognition plays an important role in recognizing
               and understanding human emotion by robots. Robots equipped with
               expression recognition capabilities can also be a useful tool to
               get feedback from the users. The designed experiment allows
               evaluating a trained neural network in facial expressions using
               a social robot in a real environment. In this paper a comparison
               between the CNN accuracy and human experts is performed, in
               addition to analyze the interaction, attention and difficulty to
               perform a particular expression by 29 non-expert users. In the
               experiment, the robot leads the users to perform different
               facial expressions in motivating and entertaining way. At the
               end of the experiment, the users are quizzed about their
               experience with the robot. Finally, a set of experts and the CNN
               classify the expressions. The obtained results allow affirming
               that the use of social robot is an adequate interaction paradigm
               for the evaluation on facial expression.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 20,
  number    = 23,
  pages     = {6716},
  month     = nov,
  year      = 2020,
  keywords  = {affective computing; convolutional neural network (CNN); facial
               expression recognition; human-robot interaction; social robots},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@article{Savchenko2024-ns,
  title         = {{HSEmotion} team at the 6th {ABAW} competition: Facial
                   expressions, valence-arousal and emotion intensity
                   prediction},
  author        = {Savchenko, Andrey V},
  abstract      = {This article presents our results for the sixth Affective
                   Behavior Analysis in-the-wild (ABAW) competition. To improve
                   the trustworthiness of facial analysis, we study the
                   possibility of using pre-trained deep models that extract
                   reliable emotional features without the need to fine-tune
                   the neural networks for a downstream task. In particular, we
                   introduce several lightweight models based on MobileViT,
                   MobileFaceNet, EfficientNet, and DDAMFN architectures
                   trained in multi-task scenarios to recognize facial
                   expressions, valence, and arousal on static photos. These
                   neural networks extract frame-level features fed into a
                   simple classifier, e.g., linear feed-forward neural network,
                   to predict emotion intensity, compound expressions, action
                   units, facial expressions, and valence/arousal. Experimental
                   results for five tasks from the sixth ABAW challenge
                   demonstrate that our approach lets us significantly improve
                   quality metrics on validation sets compared to existing
                   non-ensemble techniques.},
  month         = mar,
  year          = 2024,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  eprint        = {2403.11590}
}

@article{Marmpena2018-tw,
  title     = {How does the robot feel? Perception of valence and arousal in
               emotional body language},
  author    = {Marmpena, Mina and Lim, Angelica and Dahl, Torbj{\o}rn S},
  abstract  = {AbstractHuman-robot interaction in social robotics applications
               could be greatly enhanced by robotic behaviors that incorporate
               emotional body language. Using as our starting point a set of
               pre-designed, emotion conveying animations that have been
               created by professional animators for the Pepper robot, we seek
               to explore how humans perceive their affect content, and to
               increase their usability by annotating them with reliable labels
               of valence and arousal, in a continuous interval space. We
               conducted an experiment with 20 participants who were presented
               with the animations and rated them in the two-dimensional affect
               space. An inter-rater reliability analysis was applied to
               support the aggregation of the ratings for deriving the final
               labels. The set of emotional body language animations with the
               labels of valence and arousal is available and can potentially
               be useful to other researchers as a ground truth for behavioral
               experiments on robotic expression of emotion, or for the
               automatic selection of robotic emotional behaviors with respect
               to valence and arousal. To further utilize the data we
               collected, we analyzed it with an exploratory approach and we
               present some interesting trends with regard to the human
               perception of Pepper's emotional body language, that might be
               worth further investigation.},
  journal   = {Paladyn},
  publisher = {Walter de Gruyter GmbH},
  volume    = 9,
  number    = 1,
  pages     = {168--182},
  month     = jul,
  year      = 2018,
  language  = {en}
}

@article{Tang2025-ny,
  title         = {Robot character generation and adaptive human-robot
                   interaction with personality shaping},
  author        = {Tang, Cheng and Tang, Chao and Gong, Steven and Kwok, Thomas
                   M and Hu, Yue},
  abstract      = {We present a novel framework for designing emotionally agile
                   robots with dynamic personalities and memory-based learning,
                   with the aim of performing adaptive and non-deterministic
                   interactions with humans while conforming to shared social
                   understanding. While existing work has largely focused on
                   emotion recognition and static response systems, many
                   approaches rely on sentiment analysis and action mapping
                   frameworks that are pre-defined with limited dimensionality
                   and fixed configurations, lacking the flexibility of dynamic
                   personality traits and memory-enabled adaptation. Other
                   systems are often restricted to limited modes of expression
                   and fail to develop a causal relationship between human
                   behavior and the robot's proactive physical actions,
                   resulting in constrained adaptability and reduced
                   responsiveness in complex, dynamic interactions. Our
                   methodology integrates the Big Five Personality Traits,
                   Appraisal Theory, and abstracted memory layers through Large
                   Language Models (LLMs). The LLM generates a parameterized
                   robot personality based on the Big Five, processes human
                   language and sentiments, evaluates human behavior using
                   Appraisal Theory, and generates emotions and selects
                   appropriate actions adapted by historical context over time.
                   We validated the framework by testing three robots with
                   distinct personalities in identical background contexts and
                   found that personality, appraisal, and memory influence the
                   adaptability of human-robot interactions. The impact of the
                   individual components was further validated through ablation
                   tests. We conclude that this system enables robots to engage
                   in meaningful and personalized interactions with users, and
                   holds significant potential for applications in domains such
                   as pet robots, assistive robots, educational robots, and
                   collaborative functional robots, where cultivating tailored
                   relationships and enriching user experiences are essential.},
  month         = feb,
  year          = 2025,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC},
  eprint        = {2503.15518}
}

@article{Troiano2023-if,
  title     = {Dimensional modeling of emotions in text with appraisal
               theories: Corpus creation, annotation reliability, and
               prediction},
  author    = {Troiano, Enrica and Oberl{\"a}nder, Laura and Klinger, Roman},
  abstract  = {Abstract The most prominent tasks in emotion analysis are to
               assign emotions to texts and to understand how emotions manifest
               in language. An important observation for natural language
               processing is that emotions can be communicated implicitly by
               referring to events alone, appealing to an empathetic,
               intersubjective understanding of events, even without explicitly
               mentioning an emotion name. In psychology, the class of emotion
               theories known as appraisal theories aims at explaining the link
               between events and emotions. Appraisals can be formalized as
               variables that measure a cognitive evaluation by people living
               through an event that they consider relevant. They include the
               assessment if an event is novel, if the person considers
               themselves to be responsible, if it is in line with their own
               goals, and so forth. Such appraisals explain which emotions are
               developed based on an event, for example, that a novel situation
               can induce surprise or one with uncertain consequences could
               evoke fear. We analyze the suitability of appraisal theories for
               emotion analysis in text with the goal of understanding if
               appraisal concepts can reliably be reconstructed by annotators,
               if they can be predicted by text classifiers, and if appraisal
               concepts help to identify emotion categories. To achieve that,
               we compile a corpus by asking people to textually describe
               events that triggered particular emotions and to disclose their
               appraisals. Then, we ask readers to reconstruct emotions and
               appraisals from the text. This set-up allows us to measure if
               emotions and appraisals can be recovered purely from text and
               provides a human baseline to judge a model's performance
               measures. Our comparison of text classification methods to human
               annotators shows that both can reliably detect emotions and
               appraisals with similar performance. Therefore, appraisals
               constitute an alternative computational emotion analysis
               paradigm and further improve the categorization of emotions in
               text with joint models.},
  journal   = {Comput. Linguist. Assoc. Comput. Linguist.},
  publisher = {MIT Press},
  volume    = 49,
  number    = 1,
  pages     = {1--72},
  month     = mar,
  year      = 2023,
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  language  = {en}
}

@incollection{Balahur2011-mb,
  title     = {{EmotiNet}: A knowledge base for emotion detection in text built
               on the appraisal theories},
  booktitle = {Natural Language Processing and Information Systems},
  author    = {Balahur, Alexandra and Hermida, Jes{\'u}s M and Montoyo,
               Andr{\'e}s and Mu{\~n}oz, Rafael},
  publisher = {Springer Berlin Heidelberg},
  pages     = {27--39},
  series    = {Lecture notes in computer science},
  year      = 2011,
  address   = {Berlin, Heidelberg}
}

@article{Suhaila_2021-ez,
  title     = {A survey on emotion recognition for human Robot Interaction},
  author    = {Suhaila Najim Mohammed and Alia Karim Abdul Hassan},
  abstract  = {With the recent developments of technology and the advances in
               artificial intelligent and machine learning techniques, it
               becomes possible for the robot to acquire and show the emotions
               as a part of Human-Robot Interaction (HRI). An emotional robot
               can recognize the emotional states of humans so that it will be
               able to interact more naturally with its human counterpart in
               different environments. In this article, a survey on emotion
               recognition for HRI systems has been presented. The survey aims
               to achieve two objectives. Firstly, it aims to discuss the main
               challenges that face researchers when building emotional HRI
               systems. Secondly, it seeks to identify sensing channels that
               can be used to detect emotions and provides a literature review
               about recent researches published within each channel, along
               with the used methodologies and achieved results. Finally, some
               of the existing emotion recognition issues and recommendations
               for future works have been outlined.},
  journal   = {J. Comput. Inf. Technol.},
  publisher = {Faculty of Electrical Engineering and Computing, Univ. of Zagreb},
  volume    = 28,
  number    = 2,
  pages     = {125--146},
  month     = jun,
  year      = 2021
}

@article{Spezialetti2020-ty,
  title     = {Emotion recognition for human-robot interaction: Recent advances
               and future perspectives},
  author    = {Spezialetti, Matteo and Placidi, Giuseppe and Rossi, Silvia},
  abstract  = {A fascinating challenge in the field of human-robot interaction
               is the possibility to endow robots with emotional intelligence
               in order to make the interaction more intuitive, genuine, and
               natural. To achieve this, a critical point is the capability of
               the robot to infer and interpret human emotions. Emotion
               recognition has been widely explored in the broader fields of
               human-machine interaction and affective computing. Here, we
               report recent advances in emotion recognition, with particular
               regard to the human-robot interaction context. Our aim is to
               review the state of the art of currently adopted emotional
               models, interaction modalities, and classification strategies
               and offer our point of view on future developments and critical
               issues. We focus on facial expressions, body poses and
               kinematics, voice, brain activity, and peripheral physiological
               responses, also providing a list of available datasets
               containing data from these modalities.},
  journal   = {Front. Robot. AI},
  publisher = {Frontiers Media SA},
  volume    = 7,
  pages     = {532279},
  month     = dec,
  year      = 2020,
  keywords  = {affective computing; emotion recognition (ER); human-robot
               interaction; machine learning; multimodal data},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@article{Russell1980-cd,
  title     = {A circumplex model of affect},
  author    = {Russell, James A},
  journal   = {J. Pers. Soc. Psychol.},
  publisher = {American Psychological Association (APA)},
  volume    = 39,
  number    = 6,
  pages     = {1161--1178},
  month     = dec,
  year      = 1980,
  language  = {en}
}

@inproceedings{Mohammed2020-bz,
  title      = {Machine learning with oversampling and undersampling
                techniques: Overview study and experimental results},
  booktitle  = {2020 11th International Conference on Information and
                Communication Systems ({ICICS})},
  author     = {Mohammed, Roweida and Rawashdeh, Jumanah and Abdullah,
                Malak},
  publisher  = {IEEE},
  month      = apr,
  year       = 2020,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2020 11th International Conference on Information and
                Communication Systems (ICICS)},
  location   = {Irbid, Jordan}
}

@article{Johnson2019-xc,
  title     = {Survey on deep learning with class imbalance},
  author    = {Johnson, Justin M and Khoshgoftaar, Taghi M},
  abstract  = {The purpose of this study is to examine existing deep learning
               techniques for addressing class imbalanced data. Effective
               classification with imbalanced data is an important area of
               research, as high class imbalance is naturally inherent in many
               real-world applications, e.g., fraud detection and cancer
               detection. Moreover, highly imbalanced data poses added
               difficulty, as most learners will exhibit bias towards the
               majority class, and in extreme cases, may ignore the minority
               class altogether. Class imbalance has been studied thoroughly
               over the last two decades using traditional machine learning
               models, i.e. non-deep learning. Despite recent advances in deep
               learning, along with its increasing popularity, very little
               empirical work in the area of deep learning with class imbalance
               exists. Having achieved record-breaking performance results in
               several complex domains, investigating the use of deep neural
               networks for problems containing high levels of class imbalance
               is of great interest. Available studies regarding class
               imbalance and deep learning are surveyed in order to better
               understand the efficacy of deep learning when applied to class
               imbalanced data. This survey discusses the implementation
               details and experimental results for each study, and offers
               additional insight into their strengths and weaknesses. Several
               areas of focus include: data complexity, architectures tested,
               performance interpretation, ease of use, big data application,
               and generalization to other domains. We have found that research
               in this area is very limited, that most existing work focuses on
               computer vision tasks with convolutional neural networks, and
               that the effects of big data are rarely considered. Several
               traditional methods for class imbalance, e.g. data sampling and
               cost-sensitive learning, prove to be applicable in deep
               learning, while more advanced methods that exploit neural
               network feature learning abilities show promising results. The
               survey concludes with a discussion that highlights various gaps
               in deep learning from class imbalanced data for the purpose of
               guiding future research.},
  journal   = {J. Big Data},
  publisher = {Springer Science and Business Media LLC},
  volume    = 6,
  number    = 1,
  month     = dec,
  year      = 2019,
  language  = {en}
}

@article{Dautenhahn2007-wl,
  title     = {Socially intelligent robots: dimensions of human-robot
               interaction},
  author    = {Dautenhahn, Kerstin},
  abstract  = {Social intelligence in robots has a quite recent history in
               artificial intelligence and robotics. However, it has become
               increasingly apparent that social and interactive skills are
               necessary requirements in many application areas and contexts
               where robots need to interact and collaborate with other robots
               or humans. Research on human-robot interaction (HRI) poses many
               challenges regarding the nature of interactivity and 'social
               behaviour' in robot and humans. The first part of this paper
               addresses dimensions of HRI, discussing requirements on social
               skills for robots and introducing the conceptual space of HRI
               studies. In order to illustrate these concepts, two examples of
               HRI research are presented. First, research is surveyed which
               investigates the development of a cognitive robot companion. The
               aim of this work is to develop social rules for robot behaviour
               (a 'robotiquette') that is comfortable and acceptable to humans.
               Second, robots are discussed as possible educational or
               therapeutic toys for children with autism. The concept of
               interactive emergence in human-child interactions is
               highlighted. Different types of play among children are
               discussed in the light of their potential investigation in
               human-robot experiments. The paper concludes by examining
               different paradigms regarding 'social relationships' of robots
               and people interacting with them.},
  journal   = {Philos. Trans. R. Soc. Lond. B Biol. Sci.},
  publisher = {The Royal Society},
  volume    = 362,
  number    = 1480,
  pages     = {679--704},
  month     = apr,
  year      = 2007,
  language  = {en}
}

@article{Breazeal2003-sa,
  title     = {Emotion and sociable humanoid robots},
  author    = {Breazeal, Cynthia},
  abstract  = {This paper focuses on the role of emotion and expressive
               behavior in regulating social interaction between humans and
               expressive anthropomorphic robots, either in communicative or
               teaching scenarios. We present the scientific basis underlying
               our humanoid robot's emotion models and expressive behavior, and
               then show how these scientific viewpoints have been adapted to
               the current implementation. Our robot is also able to recognize
               affective intent through tone of voice, the implementation of
               which is inspired by the scientific findings of the
               developmental psycholinguistics community. We first evaluate the
               robot's expressive displays in isolation. Next, we evaluate the
               robot's overall emotive behavior (i.e. the coordination of the
               affective recognition system, the emotion and motivation
               systems, and the expression system) as it socially engages nave
               human subjects face-to-face.},
  journal   = {Int. J. Hum. Comput. Stud.},
  publisher = {Elsevier BV},
  volume    = 59,
  number    = {1-2},
  pages     = {119--155},
  month     = jul,
  year      = 2003,
  language  = {en}
}

@inproceedings{Castellano2009-cv,
  title      = {Detecting user engagement with a robot companion using
                task and social interaction-based features},
  booktitle  = {Proceedings of the 2009 international conference on
                Multimodal interfaces},
  author     = {Castellano, Ginevra and Pereira, Andr{\'e} and Leite,
                Iolanda and Paiva, Ana and McOwan, Peter W},
  abstract   = {Affect sensitivity is of the utmost importance for a robot
                companion to be able to display socially intelligent
                behaviour, a key requirement for sustaining long-term
                interactions with humans. This paper explores a
                naturalistic scenario in which children play chess with
                the iCat, a robot companion. A person-independent,
                Bayesian approach to detect the user's engagement with the
                iCat robot is presented. Our framework models both causes
                and effects of engagement: features related to the user's
                non-verbal behaviour, the task and the companion's
                affective reactions are identified to predict the
                children's level of engagement. An experiment was carried
                out to train and validate our model. Results show that our
                approach based on multimodal integration of task and
                social interaction-based features outperforms those based
                solely on non-verbal behaviour or contextual information
                (94.79 \% vs. 93.75 \% and 78.13 \%).},
  publisher  = {ACM},
  month      = nov,
  year       = 2009,
  address    = {New York, NY, USA},
  conference = {ICMI-MLMI '09: INTERNATIONAL CONFERENCE ON MULTIMODAL
                INTERFACES/WORKSHOP ON MACHINE LEARNING FOR MULTIMODAL
                INTERFACES},
  location   = {Cambridge Massachusetts USA}
}


@article{Rangulov2020-pd,
  title         = {Emotion Recognition on large video dataset based on
                   Convolutional Feature Extractor and Recurrent Neural Network},
  author        = {Rangulov, Denis and Fahim, Muhammad},
  abstract      = {For many years, the emotion recognition task has remained
                   one of the most interesting and important problems in the
                   field of human-computer interaction. In this study, we
                   consider the emotion recognition task as a classification as
                   well as a regression task by processing encoded emotions in
                   different datasets using deep learning models. Our model
                   combines convolutional neural network (CNN) with recurrent
                   neural network (RNN) to predict dimensional emotions on
                   video data. At the first step, CNN extracts feature vectors
                   from video frames. In the second step, we fed these feature
                   vectors to train RNN for exploiting the temporal dynamics of
                   video. Furthermore, we analyzed how each neural network
                   contributes to the system's overall performance. The
                   experiments are performed on publicly available datasets
                   including the largest modern Aff-Wild2 database. It contains
                   over sixty hours of video data. We discovered the problem of
                   overfitting of the model on an unbalanced dataset with an
                   illustrative example using confusion matrices. The problem
                   is solved by downsampling technique to balance the dataset.
                   By significantly decreasing training data, we balance the
                   dataset, thereby, the overall performance of the model is
                   improved. Hence, the study qualitatively describes the
                   abilities of deep learning models exploring enough amount of
                   data to predict facial emotions. Our proposed method is
                   implemented using Tensorflow Keras.},
  month         = jun,
  year          = 2020,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  eprint        = {2006.11168}
}

@misc{Alexey_2021-lf,
  title       = {darknet: {YOLOv4} / {Scaled-YOLOv4} / {YOLO} - Neural Networks
                 for Object Detection (Windows and Linux version of Darknet )},
  author      = {{Alexey}},
  abstract    = {YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object
                 Detection (Windows and Linux version of Darknet ) -
                 AlexeyAB/darknet},
  institution = {Github},
  language    = {en},
  url         = {https://github.com/AlexeyAB/darknet},
  urldate     = {2022-09-04},
  year        = {2021}
}

@inproceedings{HaqJackson_AVSP09,
  author    = {Haq, S. and Jackson, P.J.B.},
  year      = {2009},
  title     = {Speaker-dependent audio-visual emotion recognition},
  booktitle = {Proc.\ Int.\ Conf. on Auditory-Visual Speech Processing (AVSP'08), Norwich, UK},
  month     = {Sept.},
  abstract  = {This paper explores the recognition of expressed emotion from
               speech and facial gestures for the speaker-dependent case. Experiments
               were performed on an English audio-visual emotional
               database consisting of 480 utterances from 4 English male actors
               in 7 emotions. A total of 106 audio and 240 visual features
               were extracted and features were selected with Plus l-Take Away
               r algorithm based on Bhattacharyya distance criterion. Linear
               transformation methods, principal component analysis (PCA) and
               linear discriminant analysis (LDA), were applied to the selected
               features and Gaussian classifiers were used for classification. The
               performance was higher for LDA features compared to PCA features.
               The visual features performed better than the audio features
               and overall performance improved for the audio-visual features.
               In case of 7 emotion classes, an average recognition rate of 56%
               was achieved with the audio features, 95% with the visual features
               and 98% with the audio-visual features selected by Bhattacharyya
               distance and transformed by LDA. Grouping emotions
               into 4 classes, an average recognition rate of 69% was achieved
               with the audio features, 98% with the visual features and 98%
               with the audio-visual features fused at decision level. The results
               were comparable to the measured human recognition rate with
               this multimodal data set.},
  keyword   = {audio-visual emotion},
  keyword   = {data evaluation},
  keyword   = {linear transformation},
  keyword   = {speaker-dependent}
}

@inproceedings{Viola990517,
  author    = {Viola, P. and Jones, M.},
  booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
  title     = {Rapid object detection using a boosted cascade of simple features},
  year      = {2001},
  volume    = {1},
  number    = {},
  pages     = {I-I},
  keywords  = {Object detection;Face detection;Pixel;Detectors;Filters;Machine learning;Image representation;Focusing;Skin;Robustness},
  doi       = {10.1109/CVPR.2001.990517}
}

@article{Redmon2015-eb,
  title     = {You only look once: Unified, real-time object detection},
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and
               Farhadi, Ali},
  abstract  = {We present YOLO, a new approach to object detection. Prior work
               on object detection repurposes classifiers to perform detection.
               Instead, we frame object detection as a regression problem to
               spatially separated bounding boxes and associated class
               probabilities. A single neural network predicts bounding boxes
               and class probabilities directly from full images in one
               evaluation. Since the whole detection pipeline is a single
               network, it can be optimized end-to-end directly on detection
               performance. Our unified architecture is extremely fast. Our
               base YOLO model processes images in real-time at 45 frames per
               second. A smaller version of the network, Fast YOLO, processes
               an astounding 155 frames per second while still achieving double
               the mAP of other real-time detectors. Compared to
               state-of-the-art detection systems, YOLO makes more localization
               errors but is far less likely to predict false detections where
               nothing exists. Finally, YOLO learns very general
               representations of objects. It outperforms all other detection
               methods, including DPM and R-CNN, by a wide margin when
               generalizing from natural images to artwork on both the Picasso
               Dataset and the People-Art Dataset.},
  publisher = {arXiv},
  year      = 2015
}

@inproceedings{1467360,
  author    = {Dalal, N. and Triggs, B.},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  title     = {Histograms of oriented gradients for human detection},
  year      = {2005},
  volume    = {1},
  number    = {},
  pages     = {886-893 vol. 1},
  keywords  = {Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases},
  doi       = {10.1109/CVPR.2005.177}
}


@article{Dhuheir2021-ii,
  title         = {Emotion recognition for healthcare surveillance systems
                   using neural networks: A survey},
  author        = {Dhuheir, Marwan and Albaseer, Abdullatif and Baccour, Emna
                   and Erbad, Aiman and Abdallah, Mohamed and Hamdi, Mounir},
  abstract      = {Recognizing the patient's emotions using deep learning
                   techniques has attracted significant attention recently due
                   to technological advancements. Automatically identifying the
                   emotions can help build smart healthcare centers that can
                   detect depression and stress among the patients in order to
                   start the medication early. Using advanced technology to
                   identify emotions is one of the most exciting topics as it
                   defines the relationships between humans and machines.
                   Machines learned how to predict emotions by adopting various
                   methods. In this survey, we present recent research in the
                   field of using neural networks to recognize emotions. We
                   focus on studying emotions' recognition from speech, facial
                   expressions, and audio-visual input and show the different
                   techniques of deploying these algorithms in the real world.
                   These three emotion recognition techniques can be used as a
                   surveillance system in healthcare centers to monitor
                   patients. We conclude the survey with a presentation of the
                   challenges and the related future work to provide an insight
                   into the applications of using emotion recognition.},
  month         = jul,
  year          = 2021,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  eprint        = {2107.05989}
}
@article{R2024-jk,
  title         = {Multimodal emotion recognition using audio-Video Transformer
                   Fusion with Cross Attention},
  author        = {Joe Dhanith and Venkatraman, Shravan and Narendra,
                   Modigari and Sharma, Vigya and Malarvannan, Santhosh and
                   Gandomi, Amir H},
  abstract      = {Understanding emotions is a fundamental aspect of human
                   communication. Integrating audio and video signals offers a
                   more comprehensive understanding of emotional states
                   compared to traditional methods that rely on a single data
                   source, such as speech or facial expressions. Despite its
                   potential, multimodal emotion recognition faces significant
                   challenges, particularly in synchronization, feature
                   extraction, and fusion of diverse data sources. To address
                   these issues, this paper introduces a novel
                   transformer-based model named Audio-Video Transformer Fusion
                   with Cross Attention (AVT-CA). The AVT-CA model employs a
                   transformer fusion approach to effectively capture and
                   synchronize interlinked features from both audio and video
                   inputs, thereby resolving synchronization problems.
                   Additionally, the Cross Attention mechanism within AVT-CA
                   selectively extracts and emphasizes critical features while
                   discarding irrelevant ones from both modalities, addressing
                   feature extraction and fusion challenges. Extensive
                   experimental analysis conducted on the CMU-MOSEI, RAVDESS
                   and CREMA-D datasets demonstrates the efficacy of the
                   proposed model. The results underscore the importance of
                   AVT-CA in developing precise and reliable multimodal emotion
                   recognition systems for practical applications.},
  month         = jul,
  year          = 2024,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.MM},
  eprint        = {2407.18552}
}
@article{Abreu:2010,
  title     = {Enhancing identity prediction using a novel approach to combining hard-and soft-biometric information},
  author    = {Da Costa-Abreu, M. and Fairhurst, M.},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume    = {41},
  number    = {5},
  pages     = {599--607},
  year      = {2010},
  publisher = {IEEE}
}
@inproceedings{Mistry2020-gr,
  title      = {A {Multi-Population} {FA} for Automatic Facial Emotion
                Recognition},
  booktitle  = {2020 International Joint Conference on Neural Networks
                ({IJCNN})},
  author     = {Mistry, Kamlesh and Rizvi, Baqar and Rook, Chris and
                Iqbal, Sadaf and Zhang, Li and Joy, Colin Paul},
  publisher  = {IEEE},
  month      = jul,
  year       = 2020,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2020 International Joint Conference on Neural Networks
                (IJCNN)},
  location   = {Glasgow, United Kingdom}
}
@inproceedings{Faria2017-lg,
  title      = {Affective facial expressions recognition for human-robot
                interaction},
  booktitle  = {2017 26th {IEEE} International Symposium on Robot and
                Human Interactive Communication ({RO-MAN})},
  author     = {Faria, Diego R and Vieira, Mario and Faria, Fernanda C C
                and Premebida, Cristiano},
  publisher  = {IEEE},
  month      = aug,
  year       = 2017,
  conference = {2017 26th IEEE International Symposium on Robot and Human
                Interactive Communication (RO-MAN)},
  location   = {Lisbon}
}
@inproceedings{Mistry2018-og,
  title      = {Extended {LBP} based facial expression recognition system
                for adaptive {AI} agent behaviour},
  booktitle  = {2018 International Joint Conference on Neural Networks
                ({IJCNN})},
  author     = {Mistry, Kamlesh and Jasekar, Jyoti and Issac, Biju and
                Zhang, Li},
  publisher  = {IEEE},
  month      = jul,
  year       = 2018,
  conference = {2018 International Joint Conference on Neural Networks
                (IJCNN)},
  location   = {Rio de Janeiro}
}
@inproceedings{Appuhamy2018-dc,
  title      = {Development of a {GPU-based} human emotion recognition
                robot eye for service robot by using convolutional neural
                network},
  booktitle  = {2018 {IEEE/ACIS} 17th International Conference on Computer
                and Information Science ({ICIS})},
  author     = {Appuhamy, E J G S and Madhusanka, B G D A},
  publisher  = {IEEE},
  month      = jun,
  year       = 2018,
  conference = {2018 IEEE/ACIS 17th International Conference on Computer
                and Information Science (ICIS)},
  location   = {Singapore}
}
@inproceedings{Lopez-Rincon2019-et,
  title      = {Emotion Recognition using Facial Expressions in Children
                using the {NAO} Robot},
  booktitle  = {2019 International Conference on Electronics,
                Communications and Computers ({CONIELECOMP})},
  author     = {Lopez-Rincon, Alejandro},
  publisher  = {IEEE},
  month      = feb,
  year       = 2019,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2019 International Conference on Electronics,
                Communications and Computers (CONIELECOMP)},
  location   = {Cholula, Mexico}
}
@inproceedings{Rosula_Reyes2020-yz,
  title      = {Face detection and recognition of the seven emotions via
                facial expression: Integration of machine learning
                algorithm into the {NAO} robot},
  booktitle  = {2020 5th International Conference on Control and Robotics
                Engineering ({ICCRE})},
  author     = {Rosula Reyes, S J and Depano, Keanu M and Velasco, Aaron
                Matthew A and Kwong, John Chris T and Oppus, Carlos M},
  publisher  = {IEEE},
  month      = apr,
  year       = 2020,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2020 5th International Conference on Control and Robotics
                Engineering (ICCRE)},
  location   = {Osaka, Japan}
}
@inproceedings{Gupta2018-af,
  title      = {Facial emotion recognition in real-time and static images},
  booktitle  = {2018 2nd International Conference on Inventive Systems and
                Control ({ICISC})},
  author     = {Gupta, Shivam},
  publisher  = {IEEE},
  month      = jan,
  year       = 2018,
  conference = {2018 2nd International Conference on Inventive Systems and
                Control (ICISC)},
  location   = {Coimbatore}
}
@article{Allognon2020-um,
  title         = {Continuous emotion recognition via deep convolutional
                   autoencoder and support vector regressor},
  author        = {Allognon, Sevegni Odilon Clement and Koerich, Alessandro L
                   and Britto, Jr, Alceu de S},
  abstract      = {Automatic facial expression recognition is an important
                   research area in the emotion recognition and computer
                   vision. Applications can be found in several domains such as
                   medical treatment, driver fatigue surveillance, sociable
                   robotics, and several other human-computer interaction
                   systems. Therefore, it is crucial that the machine should be
                   able to recognize the emotional state of the user with high
                   accuracy. In recent years, deep neural networks have been
                   used with great success in recognizing emotions. In this
                   paper, we present a new model for continuous emotion
                   recognition based on facial expression recognition by using
                   an unsupervised learning approach based on transfer learning
                   and autoencoders. The proposed approach also includes
                   preprocessing and post-processing techniques which
                   contribute favorably to improving the performance of
                   predicting the concordance correlation coefficient for
                   arousal and valence dimensions. Experimental results for
                   predicting spontaneous and natural emotions on the RECOLA
                   2016 dataset have shown that the proposed approach based on
                   visual information can achieve CCCs of 0.516 and 0.264 for
                   valence and arousal, respectively.},
  month         = jan,
  year          = 2020,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  eprint        = {2001.11976}
}
@inproceedings{Brandizzi2021AutomaticRI,
  title     = {Automatic RGB Inference Based on Facial Emotion Recognition},
  author    = {Nicolo’ Brandizzi and Valerio Bianco and Giulia Castro and Samuele Russo and Agata Wajda},
  booktitle = {System (Link{\"o}ping)},
  year      = {2021},
  url       = {https://api.semanticscholar.org/CorpusID:247582848}
}
@article{Melinte2020-ky,
  title     = {Facial expressions recognition for human-robot interaction using
               deep convolutional neural networks with rectified Adam optimizer},
  author    = {Melinte, Daniel Octavian and Vladareanu, Luige},
  abstract  = {The interaction between humans and an NAO robot using deep
               convolutional neural networks (CNN) is presented in this paper
               based on an innovative end-to-end pipeline method that applies
               two optimized CNNs, one for face recognition (FR) and another
               one for the facial expression recognition (FER) in order to
               obtain real-time inference speed for the entire process. Two
               different models for FR are considered, one known to be very
               accurate, but has low inference speed (faster region-based
               convolutional neural network), and one that is not as accurate
               but has high inference speed (single shot detector convolutional
               neural network). For emotion recognition transfer learning and
               fine-tuning of three CNN models (VGG, Inception V3 and ResNet)
               has been used. The overall results show that single shot
               detector convolutional neural network (SSD CNN) and faster
               region-based convolutional neural network (Faster R-CNN) models
               for face detection share almost the same accuracy: 97.8\% for
               Faster R-CNN on PASCAL visual object classes (PASCAL VOCs)
               evaluation metrics and 97.42\% for SSD Inception. In terms of
               FER, ResNet obtained the highest training accuracy (90.14\%),
               while the visual geometry group (VGG) network had 87\% accuracy
               and Inception V3 reached 81\%. The results show improvements
               over 10\% when using two serialized CNN, instead of using only
               the FER CNN, while the recent optimization model, called
               rectified adaptive moment optimization (RAdam), lead to a better
               generalization and accuracy improvement of 3\%-4\% on each
               emotion recognition CNN.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 20,
  number    = 8,
  pages     = {2393},
  month     = apr,
  year      = 2020,
  keywords  = {NAO robot; advanced intelligent control; computer vision;
               convolutional neural networks; deep learning; face recognition;
               facial emotion recognition},
  language  = {en}
}
@article{Devaram2022-qc,
  title    = {{LEMON}: A lightweight facial Emotion Recognition system for
              Assistive Robotics based on Dilated Residual Convolutional Neural
              Networks},
  author   = {Devaram, Rami Reddy and Beraldo, Gloria and De Benedictis,
              Riccardo and Mongiov{\`\i}, Misael and Cesta, Amedeo},
  abstract = {The development of a Social Intelligence System based on
              artificial intelligence is one of the cutting edge technologies
              in Assistive Robotics. Such systems need to create an empathic
              interaction with the users; therefore, it os required to include
              an Emotion Recognition (ER) framework which has to run, in near
              real-time, together with several other intelligent services. Most
              of the low-cost commercial robots, however, although more
              accessible by users and healthcare facilities, have to balance
              costs and effectiveness, resulting in under-performing hardware
              in terms of memory and processing unit. This aspect makes the
              design of the systems challenging, requiring a trade-off between
              the accuracy and the complexity of the adopted models. This paper
              proposes a compact and robust service for Assistive Robotics,
              called Lightweight EMotion recognitiON (LEMON), which uses image
              processing, Computer Vision and Deep Learning (DL) algorithms to
              recognize facial expressions. Specifically, the proposed DL model
              is based on Residual Convolutional Neural Networks with the
              combination of Dilated and Standard Convolution Layers. The first
              remarkable result is the few numbers (i.e., 1.6 Million) of
              parameters characterizing our model. In addition, Dilated
              Convolutions expand receptive fields exponentially with
              preserving resolution, less computation and memory cost to
              recognize the distinction among facial expressions by capturing
              the displacement of the pixels. Finally, to reduce the dying ReLU
              problem and improve the stability of the model, we apply an
              Exponential Linear Unit (ELU) activation function in the initial
              layers of the model. We have performed training and evaluation
              (via one- and five-fold cross validation) of the model with five
              datasets available in the community and one mixed dataset created
              by taking samples from all of them. With respect to the other
              approaches, our model achieves comparable results with a
              significant reduction in terms of the number of parameters.},
  journal  = {Sensors (Basel)},
  volume   = 22,
  number   = 9,
  month    = apr,
  year     = 2022,
  keywords = {assistive robotics; computer vision; deep convolutional neural
              networks; emotion recognition; face recognition},
  language = {en}
}

@article{Ruiz-Garcia2018-zq,
  title     = {A hybrid deep learning neural approach for emotion recognition
               from facial expressions for socially assistive robots},
  author    = {Ruiz-Garcia, Ariel and Elshaw, Mark and Altahhan, Abdulrahman
               and Palade, Vasile},
  journal   = {Neural Comput. Appl.},
  publisher = {Springer Science and Business Media LLC},
  volume    = 29,
  number    = 7,
  pages     = {359--373},
  month     = apr,
  year      = 2018,
  language  = {en}
}

@article{Saxena2022-sr,
  title     = {An intelligent facial expression recognition system with emotion
               intensity classification},
  author    = {Saxena, Suchitra and Tripathi, Shikha and Sudarshan, T S B},
  journal   = {Cogn. Syst. Res.},
  publisher = {Elsevier BV},
  volume    = 74,
  pages     = {39--52},
  month     = aug,
  year      = 2022,
  language  = {en}
}

@incollection{Ruiz-Garcia2018-uy,
  title     = {Deep learning for real time facial expression recognition in
               social robots},
  booktitle = {Neural Information Processing},
  author    = {Ruiz-Garcia, Ariel and Webb, Nicola and Palade, Vasile and
               Eastwood, Mark and Elshaw, Mark},
  publisher = {Springer International Publishing},
  pages     = {392--402},
  series    = {Lecture notes in computer science},
  year      = 2018,
  address   = {Cham}
}

@inproceedings{Song2019-bo,
  title      = {Accuracy improvement of facial expression recognition in
                speech acts: Confirmation of effectiveness of information
                around a mouth and {GAN-based} data augmentation},
  booktitle  = {2019 28th {IEEE} International Conference on Robot and
                Human Interactive Communication ({RO-MAN})},
  author     = {Song, Kyu-Seob and Kwon, Dong-Soo},
  publisher  = {IEEE},
  month      = oct,
  year       = 2019,
  conference = {2019 28th IEEE International Conference on Robot and Human
                Interactive Communication (RO-MAN)},
  location   = {New Delhi, India}
}
@inproceedings{Mohammadpour2017-xk,
  title      = {Facial emotion recognition using deep convolutional
                networks},
  booktitle  = {2017 {IEEE} 4th International Conference on
                {Knowledge-Based} Engineering and Innovation ({KBEI})},
  author     = {Mohammadpour, Mostafa and Khaliliardali, Hossein and
                Hashemi, Seyyed Mohammad R and AlyanNezhadi, Mohammad M},
  publisher  = {IEEE},
  month      = dec,
  year       = 2017,
  conference = {2017 IEEE 4th International Conference on Knowledge-Based
                Engineering and Innovation (KBEI)},
  location   = {Tehran}
}
@inproceedings{Udeh2022-me,
  title      = {A co-regularization facial emotion recognition based on
                multi-task facial action unit recognition},
  booktitle  = {2022 41st Chinese Control Conference ({CCC})},
  author     = {Udeh, Chinonso Paschal and Chen, Luefeng and Du, Sheng and
                Li, Min and Wu, Min},
  publisher  = {IEEE},
  month      = jul,
  year       = 2022,
  conference = {2022 41st Chinese Control Conference (CCC)},
  location   = {Hefei, China}
}

@article{Kusuma2020-oa,
  title     = {Emotion recognition on {FER-2013} face images using fine-tuned
               {VGG-16}},
  author    = {Kusuma, Gede Putra and Jonathan, Jonathan and Lim, Andreas
               Pangestu},
  journal   = {Adv. Sci. Technol. Eng. Syst. J.},
  publisher = {ASTES Journal},
  volume    = 5,
  number    = 6,
  pages     = {315--322},
  year      = 2020
}

@article{Pramerdorfer2016-xx,
  title         = {Facial expression recognition using Convolutional Neural
                   Networks: State of the art},
  author        = {Pramerdorfer, Christopher and Kampel, Martin},
  abstract      = {The ability to recognize facial expressions automatically
                   enables novel applications in human-computer interaction and
                   other areas. Consequently, there has been active research in
                   this field, with several recent works utilizing
                   Convolutional Neural Networks (CNNs) for feature extraction
                   and inference. These works differ significantly in terms of
                   CNN architectures and other factors. Based on the reported
                   results alone, the performance impact of these factors is
                   unclear. In this paper, we review the state of the art in
                   image-based facial expression recognition using CNNs and
                   highlight algorithmic differences and their performance
                   impact. On this basis, we identify existing bottlenecks and
                   consequently directions for advancing this research field.
                   Furthermore, we demonstrate that overcoming one of these
                   bottlenecks - the comparatively basic architectures of the
                   CNNs utilized in this field - leads to a substantial
                   performance increase. By forming an ensemble of modern deep
                   CNNs, we obtain a FER2013 test accuracy of 75.2\%,
                   outperforming previous works without requiring auxiliary
                   training data or face registration.},
  month         = dec,
  year          = 2016,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  eprint        = {1612.02903}
}

@article{Zhu2024-gy,
  title    = {A study on expression recognition based on improved mobilenetV2
              network},
  author   = {Zhu, Qiming and Zhuang, Hongwei and Zhao, Mi and Xu, Shuangchao
              and Meng, Rui},
  abstract = {This paper proposes an improved strategy for the MobileNetV2
              neural network(I-MobileNetV2) in response to problems such as
              large parameter quantities in existing deep convolutional neural
              networks and the shortcomings of the lightweight neural network
              MobileNetV2 such as easy loss of feature information, poor
              real-time performance, and low accuracy rate in facial emotion
              recognition tasks. The network inherits the characteristics of
              MobilenetV2 depthwise separated convolution, signifying a
              reduction in computational load while maintaining a lightweight
              profile. It utilizes a reverse fusion mechanism to retain
              negative features, which makes the information less likely to be
              lost. The SELU activation function is used to replace the RELU6
              activation function to avoid gradient vanishing. Meanwhile, to
              improve the feature recognition capability, the channel attention
              mechanism (Squeeze-and-Excitation Networks (SE-Net)) is
              integrated into the MobilenetV2 network. Experiments conducted on
              the facial expression datasets FER2013 and CK + showed that the
              proposed network model achieved facial expression recognition
              accuracies of 68.62\% and 95.96\%, improving upon the MobileNetV2
              model by 0.72\% and 6.14\% respectively, and the parameter count
              decreased by 83.8\%. These results empirically verify the
              effectiveness of the improvements made to the network model.},
  journal  = {Sci. Rep.},
  volume   = 14,
  number   = 1,
  pages    = {8121},
  month    = apr,
  year     = 2024,
  keywords = {Attention mechanism; Expression recognition; MobileNetV2; Reverse
              fusion; SELU},
  language = {en}
}

@inproceedings{Ma2019-ng,
  title      = {{ElderReact}: A multimodal dataset for recognizing emotional
                response in aging adults},
  booktitle  = {2019 International Conference on Multimodal Interaction},
  author     = {Ma, Kaixin and Wang, Xinyu and Yang, Xinru and Zhang, Mingtong
                and Girard, Jeffrey M and Morency, Louis-Philippe},
  publisher  = {ACM},
  month      = oct,
  year       = 2019,
  address    = {New York, NY, USA},
  copyright  = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  conference = {ICMI '19: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION},
  location   = {Suzhou China}
}
@inproceedings{Webb2020-bq,
  title      = {Emotion recognition from face images in an unconstrained
                environment for usage on social robots},
  booktitle  = {2020 International Joint Conference on Neural Networks
                ({IJCNN})},
  author     = {Webb, Nicola and Ruiz-Garcia, Ariel and Elshaw, Mark and
                Palade, Vasile},
  publisher  = {IEEE},
  month      = jul,
  year       = 2020,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2020 International Joint Conference on Neural Networks
                (IJCNN)},
  location   = {Glasgow, United Kingdom}
}
@article{Esfandbod2023-eq,
  title     = {Utilizing an emotional robot capable of lip-syncing in
               robot-assisted speech therapy sessions for children with
               language disorders},
  author    = {Esfandbod, Alireza and Rokhi, Zeynab and Meghdari, Ali F and
               Taheri, Alireza and Alemi, Minoo and Karimi, Mahdieh},
  abstract  = {This study scrutinizes the impacts of utilizing a socially
               assistive robot, the RASA robot, during speech therapy sessions
               for children with language disorders. Two capabilities were
               developed for the robotic platform to enhance children-robot
               interactions during speech therapy interventions: facial
               expression communication (containing recognition and expression)
               and lip-syncing. Facial expression recognition was conducted by
               training several well-known CNN architectures on one of the most
               extensive facial expressions databases, the AffectNet database,
               and then modifying them using the transfer learning strategy
               performed on the CK+ dataset. The robot's lip-syncing capability
               was designed in two steps. The first step was concerned with
               designing precise schemes of the articulatory elements needed
               during the pronunciation of the Persian phonemes (i.e.,
               consonants and vowels). The second step included developing an
               algorithm to pronounce words by disassembling them into their
               components (including consonants and vowels) and then morphing
               them into each other successively. To pursue the study's primary
               goal, two comparable groups of children with language disorders
               were considered, the intervention and control groups. The
               intervention group attended therapy sessions in which the robot
               acted as the therapist's assistant, while the control group only
               communicated with the human therapist. The study's first purpose
               was to compare the children's engagement while playing a mimic
               game with the affective robot and the therapist, conducted via
               video coding. The second objective was to assess the efficacy of
               the robot's presence in the speech therapy sessions alongside
               the therapist, accomplished by administering the Persian Test of
               Language Development, Persian TOLD. According to the first
               scenario, playing with the affective robot is more engaging than
               playing with the therapist. Furthermore, the statistical
               analysis of the study's results indicates that participating in
               robot-assisted speech therapy (RAST) sessions enhances children
               with language disorders' achievements in comparison with taking
               part in conventional speech therapy interventions.},
  journal   = {Int. J. Soc. Robot.},
  publisher = {Springer Science and Business Media LLC},
  volume    = 15,
  number    = 2,
  pages     = {165--183},
  year      = 2023,
  keywords  = {Child-robot interaction (CRI); Children with language disorders;
               Convolutional neural network (CNN); Facial expression
               recognition; Lip-syncing; Social robots; Speech therapy},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  language  = {en}
}
@article{Chen2021-ra,
  title     = {Adaptive feature selection-based {AdaBoost-KNN} with direct
               optimization for dynamic emotion recognition in human--robot
               interaction},
  author    = {Chen, Luefeng and Li, Min and Su, Wanjuan and Wu, Min and
               Hirota, Kaoru and Pedrycz, Witold},
  journal   = {IEEE Trans. Emerg. Top. Comput. Intell.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 5,
  number    = 2,
  pages     = {205--213},
  month     = apr,
  year      = 2021,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}
@book{Carolis2016-ig,
  title  = {{Emotion-Recognition} from Speech-based Interaction in {AAL}
            Environment},
  author = {Carolis, B D and Ferilli, S and Palestra, G and Redavid, D},
  year   = 2016
}
@inproceedings{Lakomkin2018-ws,
  title      = {On the robustness of speech emotion recognition for
                human-robot interaction with deep neural networks},
  booktitle  = {2018 {IEEE/RSJ} International Conference on Intelligent
                Robots and Systems ({IROS})},
  author     = {Lakomkin, Egor and Zamani, Mohammad Ali and Weber,
                Cornelius and Magg, Sven and Wermter, Stefan},
  publisher  = {IEEE},
  month      = oct,
  year       = 2018,
  conference = {2018 IEEE/RSJ International Conference on Intelligent
                Robots and Systems (IROS)},
  location   = {Madrid, Spain}
}
@article{Kim2009-in,
  title     = {Design and development of an emotional interaction robot, mung},
  author    = {Kim, Eun Ho and Kwak, Sonya S and Hyun, Kyung Hak and Kim, Soo
               Hyun and Kwak, Yoon Keun},
  journal   = {Adv. Robot.},
  publisher = {Informa UK Limited},
  volume    = 23,
  number    = 6,
  pages     = {767--784},
  month     = jan,
  year      = 2009,
  language  = {en}
}
@article{Kim2018-dh,
  title     = {Speaker-independent emotion recognition for interstate measuring
               of user based on separation and rejection},
  author    = {Kim, Bo Seong and {Korea Institute of Industrial Technology,
               Ansan-si, Gyeongi-do, South Korea} and Kim, Eun Ho},
  journal   = {Int. J. Mach. Learn. Comput.},
  publisher = {EJournal Publishing},
  volume    = 8,
  number    = 2,
  pages     = {152--157},
  month     = apr,
  year      = 2018
}

@article{Hajarolasvadi2019-nz,
  title     = {{3D} {CNN-based} speech emotion recognition using K-means
               clustering and spectrograms},
  author    = {Hajarolasvadi, Noushin and Demirel, Hasan},
  abstract  = {Detecting human intentions and emotions helps improve
               human-robot interactions. Emotion recognition has been a
               challenging research direction in the past decade. This paper
               proposes an emotion recognition system based on analysis of
               speech signals. Firstly, we split each speech signal into
               overlapping frames of the same length. Next, we extract an
               88-dimensional vector of audio features including Mel Frequency
               Cepstral Coefficients (MFCC), pitch, and intensity for each of
               the respective frames. In parallel, the spectrogram of each
               frame is generated. In the final preprocessing step, by applying
               k-means clustering on the extracted features of all frames of
               each audio signal, we select k most discriminant frames, namely
               keyframes, to summarize the speech signal. Then, the sequence of
               the corresponding spectrograms of keyframes is encapsulated in a
               3D tensor. These tensors are used to train and test a 3D
               Convolutional Neural network using a 10-fold cross-validation
               approach. The proposed 3D CNN has two convolutional layers and
               one fully connected layer. Experiments are conducted on the
               Surrey Audio-Visual Expressed Emotion (SAVEE), Ryerson
               Multimedia Laboratory (RML), and eNTERFACE'05 databases. The
               results are superior to the state-of-the-art methods reported in
               the literature.},
  journal   = {Entropy (Basel)},
  publisher = {MDPI AG},
  volume    = 21,
  number    = 5,
  pages     = {479},
  month     = may,
  year      = 2019,
  keywords  = {3D convolutional neural networks; deep learning; k-means
               clustering; spectrograms; speech emotion recognition},
  language  = {en}
}
@inproceedings{Jaiswal2020-vi,
  title      = {Image based emotional state prediction from multiparty
                audio conversation},
  booktitle  = {2020 {IEEE} Pune Section International Conference
                ({PuneCon})},
  author     = {Jaiswal, Shruti and Jain, Ayush and Nandi, G C},
  publisher  = {IEEE},
  month      = dec,
  year       = 2020,
  conference = {2020 IEEE Pune Section International Conference (PuneCon)},
  location   = {Pune, India}
}
@article{Mohammed2020-ig,
  title     = {Speech emotion recognition using {MELBP} variants of spectrogram
               image},
  author    = {Mohammed, Suhaila and Alia, Hassan},
  journal   = {Int. J. Intell. Eng. Syst.},
  publisher = {The Intelligent Networks and Systems Society},
  volume    = 13,
  number    = 5,
  pages     = {257--266},
  month     = oct,
  year      = 2020
}
@article{Mustaqeem2020-ax,
  title     = {Clustering-based speech emotion recognition by incorporating
               learned features and deep {BiLSTM}},
  author    = {{Mustaqeem} and Sajjad, Muhammad and Kwon, Soonil},
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 8,
  pages     = {79861--79875},
  year      = 2020,
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode}
}
@article{Busso2008-qj,
  title     = {{IEMOCAP}: interactive emotional dyadic motion capture database},
  author    = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and
               Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang,
               Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal   = {Lang. Resour. Eval.},
  publisher = {Springer Science and Business Media LLC},
  volume    = 42,
  number    = 4,
  pages     = {335--359},
  month     = dec,
  year      = 2008,
  language  = {en}
}
@inproceedings{Shanta2021-af,
  title      = {A comparative analysis of different approach for basic
                emotions recognition from speech},
  booktitle  = {2021 International Conference on Electronics,
                Communications and Information Technology ({ICECIT})},
  author     = {Shanta, Shammy Shikder and Sham-E-Ansari, Md and
                Chowdhury, Atiqul Islam and Shahriar, Mohammad Munem and
                Hasan, Md Khairul},
  publisher  = {IEEE},
  month      = sep,
  year       = 2021,
  conference = {2021 International Conference on Electronics,
                Communications and Information Technology (ICECIT)},
  location   = {Khulna, Bangladesh}
}

@book{Qayyum2019-bt,
  title  = {Convolutional Neural Network ({CNN}) Based {Speech-Emotion}
            Recognition},
  author = {Qayyum, Abdul and Arefeen, A B and Shahnaz, A and Ieee Xplore, C},
  year   = 2019
}

@inproceedings{Zhu2019-iq,
  title      = {Emotion recognition from speech to improve human-robot
                interaction},
  booktitle  = {2019 {IEEE} Intl Conf on Dependable, Autonomic and Secure
                Computing, Intl Conf on Pervasive Intelligence and
                Computing, Intl Conf on Cloud and Big Data Computing, Intl
                Conf on Cyber Science and Technology Congress
                ({DASC/PiCom/CBDCom/CyberSciTech})},
  author     = {Zhu, Changrui and Ahmad, Wasim},
  publisher  = {IEEE},
  month      = aug,
  year       = 2019,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2019 IEEE Intl Conf on Dependable, Autonomic and Secure
                Computing, Intl Conf on Pervasive Intelligence and
                Computing, Intl Conf on Cloud and Big Data Computing, Intl
                Conf on Cyber Science and Technology Congress
                (DASC/PiCom/CBDCom/CyberSciTech)},
  location   = {Fukuoka, Japan}
}

@book{Chattopadhyay2022-kj,
  title  = {A feature selection model for speech emotion recognition using
            clustering-based population generation with hybrid of equilibrium
            optimizer and atom search optimization algorithm. Multimedia Tools
            and Applications},
  author = {Chattopadhyay, S and Dey, A and Singh, P K and Ahmadian, A and
            Sarkar, R},
  year   = 2022
}

@inproceedings{Anjum2019-ks,
  title      = {Emotion Recognition from Speech for an Interactive Robot
                Agent},
  booktitle  = {2019 {IEEE/SICE} International Symposium on System
                Integration ({SII})},
  author     = {Anjum, Madiha},
  publisher  = {IEEE},
  month      = jan,
  year       = 2019,
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  conference = {2019 IEEE/SICE International Symposium on System
                Integration (SII)},
  location   = {Paris, France}
}

@article{Chen2020-bt,
  title     = {Two-layer fuzzy multiple random forest for speech emotion
               recognition in human-robot interaction},
  author    = {Chen, Luefeng and Su, Wanjuan and Feng, Yu and Wu, Min and She,
               Jinhua and Hirota, Kaoru},
  journal   = {Inf. Sci. (Ny)},
  publisher = {Elsevier BV},
  volume    = 509,
  pages     = {150--163},
  month     = jan,
  year      = 2020,
  language  = {en}
}

@article{Peng2020-wv,
  title     = {Speech emotion recognition using {3D} convolutions and
               attention-based sliding recurrent networks with auditory
               front-ends},
  author    = {Peng, Zhichao and Li, Xingfeng and Zhu, Zhi and Unoki, Masashi
               and Dang, Jianwu and Akagi, Masato},
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 8,
  pages     = {16560--16572},
  year      = 2020,
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode}
}

@inproceedings{Zhichao2020-bf,
  title      = {Attention-based sequence modeling for categorical emotion
                recognition with modulation spectral feature},
  booktitle  = {2020 7th International Conference on Information Science
                and Control Engineering ({ICISCE})},
  author     = {Zhichao, Peng and Wenhua, He and Hongji, Tang and Minlei,
                Xiao and Ruwei, Luo},
  publisher  = {IEEE},
  month      = dec,
  year       = 2020,
  conference = {2020 7th International Conference on Information Science
                and Control Engineering (ICISCE)},
  location   = {Changsha, China}
}

@article{Nie2022-fo,
  title     = {{I-GCN}: Incremental graph convolution network for conversation
               emotion detection},
  author    = {Nie, Weizhi and Chang, Rihao and Ren, Minjie and Su, Yuting and
               Liu, Anan},
  journal   = {IEEE Trans. Multimedia},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 24,
  pages     = {4471--4481},
  year      = 2022,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@inproceedings{Rasendrasoa2022-nf,
  title      = {Real-time multimodal emotion recognition in conversation for
                multi-party interactions},
  booktitle  = {Proceedings of the 2022 International Conference on Multimodal
                Interaction},
  author     = {Rasendrasoa, Sandratra and Pauchet, Alexandre and Saunier,
                Julien and Adam, S{\'e}bastien},
  publisher  = {ACM},
  month      = nov,
  year       = 2022,
  address    = {New York, NY, USA},
  conference = {ICMI '22: INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION},
  location   = {Bengaluru India}
}

@article{Chen2023-ss,
  title     = {Coupled multimodal emotional feature analysis based on
               broad-deep fusion networks in human-robot interaction},
  author    = {Chen, Luefeng and Li, Min and Wu, Min and Pedrycz, Witold and
               Hirota, Kaoru},
  abstract  = {A coupled multimodal emotional feature analysis (CMEFA) method
               based on broad-deep fusion networks, which divide multimodal
               emotion recognition into two layers, is proposed. First, facial
               emotional features and gesture emotional features are extracted
               using the broad and deep learning fusion network (BDFN).
               Considering that the bi-modal emotion is not completely
               independent of each other, canonical correlation analysis (CCA)
               is used to analyze and extract the correlation between the
               emotion features, and a coupling network is established for
               emotion recognition of the extracted bi-modal features. Both
               simulation and application experiments are completed. According
               to the simulation experiments completed on the bimodal face and
               body gesture database (FABO), the recognition rate of the
               proposed method has increased by 1.15\% compared to that of the
               support vector machine recursive feature elimination (SVMRFE)
               (without considering the unbalanced contribution of features).
               Moreover, by using the proposed method, the multimodal
               recognition rate is 21.22\%, 2.65\%, 1.61\%, 1.54\%, and 0.20\%
               higher than those of the fuzzy deep neural network with sparse
               autoencoder (FDNNSA), ResNet-101 + GFK, C3D + MCB + DBN, the
               hierarchical classification fusion strategy (HCFS), and
               cross-channel convolutional neural network (CCCNN),
               respectively. In addition, preliminary application experiments
               are carried out on our developed emotional social robot system,
               where emotional robot recognizes the emotions of eight
               volunteers based on their facial expressions and body gestures.},
  journal   = {IEEE Trans. Neural Netw. Learn. Syst.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = {PP},
  pages     = {1--11},
  month     = jan,
  year      = 2023,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  language  = {en}
}

@article{Wang2022-eq,
  title     = {Multitask touch gesture and emotion recognition using multiscale
               spatiotemporal convolutions with attention mechanism},
  author    = {Wang, Ya-Xin and Li, Yun-Kai and Yang, Tian-Hao and Meng,
               Qing-Hao},
  journal   = {IEEE Sens. J.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 22,
  number    = 16,
  pages     = {16190--16201},
  month     = aug,
  year      = 2022,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{Lyu2022-vd,
  title    = {Global and local feature fusion via long and short-term memory
              mechanism for dance emotion recognition in robot},
  author   = {Lyu, Yin and Sun, Yang},
  abstract = {In recent years, there are more and more intelligent machines in
              people's life, such as intelligent wristbands, sweeping robots,
              intelligent learning machines and so on, which can simply
              complete a single execution task. We want robots to be as
              emotional as humans. In this way, human-computer interaction can
              be more natural, smooth and intelligent. Therefore, emotion
              research has become a hot topic that researchers pay close
              attention to. In this paper, we propose a new dance emotion
              recognition based on global and local feature fusion method. If
              the single feature of audio is extracted, the global information
              of dance cannot be reflected. And the dimension of data features
              is very high. In this paper, an improved long and short-term
              memory (LSTM) method is used to extract global dance information.
              Linear prediction coefficient is used to extract local
              information. Considering the complementarity of different
              features, a global and local feature fusion method based on
              discriminant multi-canonical correlation analysis is proposed in
              this paper. Experimental results on public data sets show that
              the proposed method can effectively identify dance emotion
              compared with other state-of-the-art emotion recognition methods.},
  journal  = {Front. Neurorobot.},
  volume   = 16,
  pages    = {998568},
  month    = aug,
  year     = 2022,
  keywords = {LSTM; dance emotion recognition; feature fusion; linear
              prediction coefficient; robot},
  language = {en}
}

@inproceedings{Elfaramawy2017-ab,
  title      = {Emotion recognition from body expressions with a neural
                network architecture},
  booktitle  = {Proceedings of the 5th International Conference on Human
                Agent Interaction},
  author     = {Elfaramawy, Nourhan and Barros, Pablo and Parisi, German I
                and Wermter, Stefan},
  abstract   = {The recognition of emotions plays an important role in our
                daily life and is essential for social communication.
                Although multiple studies have shown that body expressions
                can strongly convey emotional states, emotion recognition
                from body motion patterns has received less attention than
                the use of facial expressions. In this paper, we propose a
                self-organizing neural architecture that can effectively
                recognize affective states from full-body motion patterns.
                To evaluate our system, we designed and collected a data
                corpus named the Body Expressions of Emotion (BEE) dataset
                using a depth sensor in a human-robot interaction
                scenario. For our recordings, nineteen participants were
                asked to perform six different emotions:anger, fear,
                happiness, neutral, sadness, and surprise. In order to
                compare our system with human-like performance, we
                conducted an additional experiment by asking fifteen
                annotators to label depth map video sequences as one of
                the six emotion classes. The labeling results from human
                annotators were compared to the results predicted by our
                system. Experimental results showed that the recognition
                accuracy of the system was competitive with human
                performance when exposed to body motion patterns from the
                same dataset.},
  publisher  = {ACM},
  month      = oct,
  year       = 2017,
  address    = {New York, NY, USA},
  copyright  = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  conference = {HAI '17: The Fifth International Conference on Human-Agent
                Interaction},
  location   = {Bielefeld Germany}
}
@inproceedings{Adiga2020-wv,
  title      = {Multimodal Emotion Recognition for Human Robot Interaction},
  booktitle  = {2020 7th International Conference on Soft Computing \&
                Machine Intelligence ({ISCMI})},
  author     = {Adiga, Sharvari and Vaishnavi, D V and Saxena, Suchitra
                and Tripathi, Shikha},
  publisher  = {IEEE},
  month      = nov,
  year       = 2020,
  conference = {2020 7th International Conference on Soft Computing \&
                Machine Intelligence (ISCMI)},
  location   = {Stockholm, Sweden}
}
@inproceedings{Song2018-vu,
  title      = {Decision-level fusion method for emotion recognition using
                multimodal emotion recognition information},
  booktitle  = {2018 15th International Conference on Ubiquitous Robots
                ({UR})},
  author     = {Song, Kyu-Seob and Nho, Young-Hoon and Seo, Ju-Hwan and
                Kwon, Dong-Soo},
  publisher  = {IEEE},
  month      = jun,
  year       = 2018,
  conference = {2018 15th International Conference on Ubiquitous Robots
                (UR)},
  location   = {Honolulu, HI}
}
@article{Kansizoglou2022-ih,
  title     = {An active learning paradigm for online audio-visual emotion
               recognition},
  author    = {Kansizoglou, Ioannis and Bampis, Loukas and Gasteratos, Antonios},
  journal   = {IEEE Trans. Affect. Comput.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 13,
  number    = 2,
  pages     = {756--768},
  month     = apr,
  year      = 2022,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}
@incollection{Yu2019-ku,
  title     = {Interactive Robot Learning for Multimodal Emotion Recognition},
  booktitle = {Social Robotics},
  author    = {Yu, Chuang and Tapus, Adriana},
  publisher = {Springer International Publishing},
  pages     = {633--642},
  series    = {Lecture notes in computer science},
  year      = 2019,
  address   = {Cham}
}

@book{Aqdus2021-xr,
  title  = {Deep Emotion Recognition through Upper Body Movements and Facial
            Expression},
  author = {Aqdus, Chaudhary and Nunes, Rita and {Kamal} and Rehm, Matthias and
            Moeslund, Thomas},
  year   = 2021
}
@article{Chen2023-dn,
  title     = {Coupled multimodal emotional feature analysis based on
               broad-deep fusion networks in human-robot interaction},
  author    = {Chen, Luefeng and Li, Min and Wu, Min and Pedrycz, Witold and
               Hirota, Kaoru},
  abstract  = {A coupled multimodal emotional feature analysis (CMEFA) method
               based on broad-deep fusion networks, which divide multimodal
               emotion recognition into two layers, is proposed. First, facial
               emotional features and gesture emotional features are extracted
               using the broad and deep learning fusion network (BDFN).
               Considering that the bi-modal emotion is not completely
               independent of each other, canonical correlation analysis (CCA)
               is used to analyze and extract the correlation between the
               emotion features, and a coupling network is established for
               emotion recognition of the extracted bi-modal features. Both
               simulation and application experiments are completed. According
               to the simulation experiments completed on the bimodal face and
               body gesture database (FABO), the recognition rate of the
               proposed method has increased by 1.15\% compared to that of the
               support vector machine recursive feature elimination (SVMRFE)
               (without considering the unbalanced contribution of features).
               Moreover, by using the proposed method, the multimodal
               recognition rate is 21.22\%, 2.65\%, 1.61\%, 1.54\%, and 0.20\%
               higher than those of the fuzzy deep neural network with sparse
               autoencoder (FDNNSA), ResNet-101 + GFK, C3D + MCB + DBN, the
               hierarchical classification fusion strategy (HCFS), and
               cross-channel convolutional neural network (CCCNN),
               respectively. In addition, preliminary application experiments
               are carried out on our developed emotional social robot system,
               where emotional robot recognizes the emotions of eight
               volunteers based on their facial expressions and body gestures.},
  journal   = {IEEE Trans. Neural Netw. Learn. Syst.},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = {PP},
  pages     = {1--11},
  month     = jan,
  year      = 2023,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  language  = {en}
}
@article{Augello2022-fu,
  title     = {Multimodal mood recognition for assistive scenarios},
  author    = {Augello, Agnese and Bella, Giulia Di and Infantino, Ignazio and
               Pilato, Giovanni and Vitale, Gianpaolo},
  journal   = {Procedia Comput. Sci.},
  publisher = {Elsevier BV},
  volume    = 213,
  pages     = {510--517},
  year      = 2022,
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  language  = {en}
}
@article{Heredia2022-gy,
  title     = {Adaptive multimodal emotion detection architecture for social
               robots},
  author    = {Heredia, Juanpablo and Lopes-Silva, Edmundo and Cardinale,
               Yudith and Diaz-Amado, Jose and Dongo, Irvin and Graterol,
               Wilfredo and Aguilera, Ana},
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 10,
  pages     = {20727--20744},
  year      = 2022,
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode}
}
@inproceedings{Hung2020-lr,
  title      = {Multiple models using temporal feature learning for emotion
                recognition},
  booktitle  = {The 9th International Conference on Smart Media and
                Applications},
  author     = {Hung, Hoang Manh and Kim, Soo-Hyung and Yang, Hyung-Jeong and
                Lee, Guee-Sang},
  publisher  = {ACM},
  month      = sep,
  year       = 2020,
  address    = {New York, NY, USA},
  conference = {SMA 2020: The 9th International Conference on Smart Media and
                Applications},
  location   = {Jeju Republic of Korea}
}

@article{Augello2022-zy,
  title     = {Multimodal mood recognition for assistive scenarios},
  author    = {Augello, Agnese and Bella, Giulia Di and Infantino, Ignazio and
               Pilato, Giovanni and Vitale, Gianpaolo},
  journal   = {Procedia Comput. Sci.},
  publisher = {Elsevier BV},
  volume    = 213,
  pages     = {510--517},
  year      = 2022,
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  language  = {en}
}

@article{Heredia2022-dt,
  title     = {Adaptive multimodal emotion detection architecture for social
               robots},
  author    = {Heredia, Juanpablo and Lopes-Silva, Edmundo and Cardinale,
               Yudith and Diaz-Amado, Jose and Dongo, Irvin and Graterol,
               Wilfredo and Aguilera, Ana},
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 10,
  pages     = {20727--20744},
  year      = 2022,
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode}
}

@inproceedings{Hung2020-gm,
  title      = {Multiple models using temporal feature learning for emotion
                recognition},
  booktitle  = {The 9th International Conference on Smart Media and
                Applications},
  author     = {Hung, Hoang Manh and Kim, Soo-Hyung and Yang, Hyung-Jeong and
                Lee, Guee-Sang},
  publisher  = {ACM},
  month      = sep,
  year       = 2020,
  address    = {New York, NY, USA},
  conference = {SMA 2020: The 9th International Conference on Smart Media and
                Applications},
  location   = {Jeju Republic of Korea}
}

@inproceedings{Yu2020-zq,
  title      = {Multimodal emotion recognition with thermal and {RGB-D} cameras
                for human-robot interaction},
  booktitle  = {Companion of the 2020 {ACM/IEEE} International Conference on
                {Human-Robot} Interaction},
  author     = {Yu, Chuang and Tapus, Adriana},
  publisher  = {ACM},
  month      = mar,
  year       = 2020,
  address    = {New York, NY, USA},
  conference = {HRI '20: ACM/IEEE International Conference on Human-Robot
                Interaction},
  location   = {Cambridge United Kingdom}
}

@inproceedings{opensmile-2010,
  author    = {Eyben, Florian and W\"{o}llmer, Martin and Schuller, Bj\"{o}rn},
  title     = {Opensmile: the munich versatile and fast open-source audio feature extractor},
  year      = {2010},
  isbn      = {9781605589336},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1873951.1874246},
  doi       = {10.1145/1873951.1874246},
  abstract  = {We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.},
  booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
  pages     = {1459–1462},
  numpages  = {4},
  keywords  = {audio feature extraction, emotion, music, signal processing, speech, statistical functionals},
  location  = {Firenze, Italy},
  series    = {MM '10}
}

@article{Kumar2022-gd,
  title     = {A survey on {IBM} Watson and its services},
  author    = {Kumar, Avinash and Tejaswini, Pallapothala and Nayak, Omprakash
               and Kujur, Anurag Deep and Gupta, Rajkiran and Rajanand, Ashish
               and Sahu, Mridu},
  abstract  = {Abstract Artificial Intelligence (AI) is changing the modern way
               of lifestyle by helping the person do their jobs in an efficient
               manner. The AI is currently in its starting phase and from now
               on it is of great use. IBM Watson is an AI which is used
               globally by different organizations, institutes and
               corporations. In this paper we have created a chatbot using IBM
               Watson Assistance which is helpful in querying about the disease
               and hospitals related query. This paper also discusses IBM
               Watson in detail, its applications, its working and case studies
               on the use of IBM Watson in the field of healthcare, visual
               recognition and a software company named BOX.},
  journal   = {J. Phys. Conf. Ser.},
  publisher = {IOP Publishing},
  volume    = 2273,
  number    = 1,
  pages     = {012022},
  month     = may,
  year      = 2022,
  copyright = {http://creativecommons.org/licenses/by/3.0/}
}

@misc{chatgpt,
  title        = {{ChatGPT}},
  howpublished = {\url{https://chat.openai.com/chat}},
  note         = {Accessed: 2023-9-10},
  year         = 2022,
  author       = {OpenAI}
}

@article{Tan2021-ai,
  title     = {A multimodal emotion recognition method based on facial
               expressions and electroencephalography},
  author    = {Tan, Ying and Sun, Zhe and Duan, Feng and Sol{\'e}-Casals, Jordi
               and Caiafa, Cesar F},
  journal   = {Biomed. Signal Process. Control},
  publisher = {Elsevier BV},
  volume    = 70,
  number    = 103029,
  pages     = {103029},
  month     = sep,
  year      = 2021,
  language  = {en}
}

@article{Younis2022-bs,
  title     = {Evaluating ensemble learning methods for multi-modal emotion
               recognition using sensor data fusion},
  author    = {Younis, Eman M G and Zaki, Someya Mohsen and Kanjo, Eiman and
               Houssein, Essam H},
  abstract  = {Automatic recognition of human emotions is not a trivial
               process. There are many factors affecting emotions internally
               and externally. Expressing emotions could also be performed in
               many ways such as text, speech, body gestures or even
               physiologically by physiological body responses. Emotion
               detection enables many applications such as adaptive user
               interfaces, interactive games, and human robot interaction and
               many more. The availability of advanced technologies such as
               mobiles, sensors, and data analytics tools led to the ability to
               collect data from various sources, which enabled researchers to
               predict human emotions accurately. Most current research uses
               them in the lab experiments for data collection. In this work,
               we use direct and real time sensor data to construct a
               subject-independent (generic) multi-modal emotion prediction
               model. This research integrates both on-body physiological
               markers, surrounding sensory data, and emotion measurements to
               achieve the following goals: (1) Collecting a multi-modal data
               set including environmental, body responses, and emotions. (2)
               Creating subject-independent Predictive models of emotional
               states based on fusing environmental and physiological
               variables. (3) Assessing ensemble learning methods and comparing
               their performance for creating a generic subject-independent
               model for emotion recognition with high accuracy and comparing
               the results with previous similar research. To achieve that, we
               conducted a real-world study ``in the wild'' with physiological
               and mobile sensors. Collecting the data-set is coming from
               participants walking around Minia university campus to create
               accurate predictive models. Various ensemble learning models
               (Bagging, Boosting, and Stacking) have been used, combining the
               following base algorithms (K Nearest Neighbor KNN, Decision Tree
               DT, Random Forest RF, and Support Vector Machine SVM) as base
               learners and DT as a meta-classifier. The results showed that,
               the ensemble stacking learner technique gave the best accuracy
               of 98.2\% compared with other variants of ensemble learning
               methods. On the contrary, bagging and boosting methods gave
               (96.4\%) and (96.6\%) accuracy levels respectively.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 22,
  number    = 15,
  pages     = {5611},
  month     = jul,
  year      = 2022,
  keywords  = {emotion recognition; ensemble learning; multi-modal emotion
               recognition; physiological and environmental; subject
               independent predictive models for emotion},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@article{Dzedzickis2020-hf,
  title     = {Human emotion recognition: Review of sensors and methods},
  author    = {Dzedzickis, Andrius and Kaklauskas, Art{\=u}ras and Bucinskas,
               Vytautas},
  abstract  = {Automated emotion recognition (AEE) is an important issue in
               various fields of activities which use human emotional reactions
               as a signal for marketing, technical equipment, or human-robot
               interaction. This paper analyzes scientific research and
               technical papers for sensor use analysis, among various methods
               implemented or researched. This paper covers a few classes of
               sensors, using contactless methods as well as contact and
               skin-penetrating electrodes for human emotion detection and the
               measurement of their intensity. The results of the analysis
               performed in this paper present applicable methods for each type
               of emotion and their intensity and propose their classification.
               The classification of emotion sensors is presented to reveal
               area of application and expected outcomes from each method, as
               well as their limitations. This paper should be relevant for
               researchers using human emotion evaluation and analysis, when
               there is a need to choose a proper method for their purposes or
               to find alternative decisions. Based on the analyzed human
               emotion recognition sensors and methods, we developed some
               practical applications for humanizing the Internet of Things
               (IoT) and affective computing systems.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 20,
  number    = 3,
  pages     = {592},
  month     = jan,
  year      = 2020,
  keywords  = {emotion perception; human emotions; physiologic sensors},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@incollection{Stanley2023-ai,
  title     = {Age and emotion},
  booktitle = {Encyclopedia of Mental Health},
  author    = {Stanley, Jennifer Tehan and Villalba, Anthony},
  publisher = {Elsevier},
  pages     = {35--43},
  year      = 2023
}

@article{Pal2021-eq,
  title     = {Development and progress in sensors and technologies for human
               emotion recognition},
  author    = {Pal, Shantanu and Mukhopadhyay, Subhas and Suryadevara, Nagender},
  abstract  = {With the advancement of human-computer interaction, robotics,
               and especially humanoid robots, there is an increasing trend for
               human-to-human communications over online platforms (e.g.,
               zoom). This has become more significant in recent years due to
               the Covid-19 pandemic situation. The increased use of online
               platforms for communication signifies the need to build
               efficient and more interactive human emotion recognition
               systems. In a human emotion recognition system, the
               physiological signals of human beings are collected, analyzed,
               and processed with the help of dedicated learning techniques and
               algorithms. With the proliferation of emerging technologies,
               e.g., the Internet of Things (IoT), future Internet, and
               artificial intelligence, there is a high demand for building
               scalable, robust, efficient, and trustworthy human recognition
               systems. In this paper, we present the development and progress
               in sensors and technologies to detect human emotions. We review
               the state-of-the-art sensors used for human emotion recognition
               and different types of activity monitoring. We present the
               design challenges and provide practical references of such human
               emotion recognition systems in the real world. Finally, we
               discuss the current trends in applications and explore the
               future research directions to address issues, e.g., scalability,
               security, trust, privacy, transparency, and decentralization.},
  journal   = {Sensors (Basel)},
  publisher = {MDPI AG},
  volume    = 21,
  number    = 16,
  pages     = {5554},
  month     = aug,
  year      = 2021,
  keywords  = {Internet of Things; human emotion; motion analysis;
               physiological parameters monitoring; sensors; wearable sensors;
               wireless communications},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}


@misc{Nawasalkar2017-fx,
  title     = {Study of comparison of human bio-signals for emotion detection
               using {HCI}},
  author    = {Nawasalkar, Ram K and Butey, Pradeep K},
  publisher = {Unpublished},
  year      = 2017
}


@article{Suhaila2021-dh,
  title     = {A survey on emotion recognition for human Robot Interaction},
  abstract  = {With the recent developments of technology and the advances in
               artificial intelligent and machine learning techniques, it
               becomes possible for the robot to acquire and show the emotions
               as a part of Human-Robot Interaction (HRI). An emotional robot
               can recognize the emotional states of humans so that it will be
               able to interact more naturally with its human counterpart in
               different environments. In this article, a survey on emotion
               recognition for HRI systems has been presented. The survey aims
               to achieve two objectives. Firstly, it aims to discuss the main
               challenges that face researchers when building emotional HRI
               systems. Secondly, it seeks to identify sensing channels that
               can be used to detect emotions and provides a literature review
               about recent researches published within each channel, along
               with the used methodologies and achieved results. Finally, some
               of the existing emotion recognition issues and recommendations
               for future works have been outlined.},
  journal   = {J. Comput. Inf. Technol.},
  publisher = {Faculty of Electrical Engineering and Computing, Univ. of Zagreb},
  volume    = 28,
  number    = 2,
  pages     = {125--146},
  month     = jun,
  year      = 2021,
  author    = {Suhaila N. Mohammed and Alia Karmin}
}

@inproceedings{Lubecke-4751478,
  author    = {Boric-Lubecke, Olga and Massagram, Wansuree and Lubecke, Victor M. and Host-Madsen, Anders and Jokanovic, Branka},
  booktitle = {2008 38th European Microwave Conference},
  title     = {Heart Rate Variability Assessment Using Doppler Radar with Linear Demodulation},
  year      = {2008},
  volume    = {},
  number    = {},
  pages     = {420-423},
  keywords  = {Heart rate variability;Doppler radar;Demodulation;Heart rate;Remote monitoring;Cardiology;Stress;Radar detection;Particle measurements;Fluctuations},
  doi       = {10.1109/EUMC.2008.4751478}
}

@article{CHEN201849,
  title    = {Softmax regression based deep sparse autoencoder network for facial emotion recognition in human-robot interaction},
  journal  = {Information Sciences},
  volume   = {428},
  pages    = {49-61},
  year     = {2018},
  issn     = {0020-0255},
  doi      = {https://doi.org/10.1016/j.ins.2017.10.044},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025517310496},
  author   = {Luefeng Chen and Mengtian Zhou and Wanjuan Su and Min Wu and Jinhua She and Kaoru Hirota},
  keywords = {Facial emotion recognition, Deep sparse autoencoder network, Softmax regression, Human-robot interaction},
  abstract = {Deep neural network (DNN) has been used as a learning model for modeling the hierarchical architecture of human brain. However, DNN suffers from problems of learning efficiency and computational complexity. To address these problems, deep sparse autoencoder network (DSAN) is used for learning facial features, which considers the sparsity of hidden units for learning high-level structures. Meanwhile, Softmax regression (SR) is used to classify expression feature. In this paper, Softmax regression-based deep sparse autoencoder network (SRDSAN) is proposed to recognize facial emotion in human-robot interaction. It aims to handle large data in the output of deep learning by using SR, moreover, to overcome local extrema and gradient diffusion problems in the training process, the overall network weights are fine-tuned to reach the global optimum, which makes the entire depth of the neural network more robust, thereby enhancing the performance of facial emotion recognition. Results show that the average recognition accuracy of SRDSAN is higher than that of the SR and the convolutional neural network. The preliminarily application experiments are performed in the developing emotional social robot system (ESRS) with two mobile robots, where emotional social robot is able to recognize emotions such as happiness and angry.}
}

@article{CHEN202010236,
  title    = {CNN-based Broad Learning with Efficient Incremental Reconstruction Model for Facial Emotion Recognition},
  journal  = {IFAC-PapersOnLine},
  volume   = {53},
  number   = {2},
  pages    = {10236-10241},
  year     = {2020},
  note     = {21st IFAC World Congress},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2020.12.2754},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896320335175},
  author   = {Luefeng Chen and Min Li and Xuzhi Lai and Kaoru Hirota and Witold Pedrycz},
  keywords = {Convolution Neural Networks, Broad Learning, Emotion Recognition, Human-Robot Interaction},
  abstract = {Convolutional neural network-based broad learning with efficient incremental reconstruction model (CNNBL) is proposed to recognize emotions in human-robot interaction. It aims to extract deep and abstract features from facial emotional images, and reduce the influence of the complex structure and slow network updates on facial emotion recognition in deep learning. Feature extraction is carried out by convolution and maximum pooling, and then the ridge regression algorithm is used for emotion recognition. When the network needs to expand, the network is dynamically updated by incremental learning algorithm. We verified the experimental performance through k-fold cross validation. According to the recognition results, the accuracy on JAFFE database of our proposal is greater than that of the state of the art, such as the Local Binary Patterns with Softmax and Deep Attentive Multi-path convolutional neural network.}
}

@inproceedings{10008155,
  author    = {Yang, Ping and Cao, Li Mei and Zhu, Lin Ling and Luo, Shun Nian},
  booktitle = {2022 10th International Conference on Orange Technology (ICOT)},
  title     = {Design of Attendance System Based on NAO Face, Speech and Emotion Recognition},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {1-3},
  keywords  = {Human computer interaction;Emotion recognition;Face recognition;Speech recognition;User experience;Social implications of technology;Robots;NAO;Face recognition;Attendance system;Emotion recognition},
  doi       = {10.1109/ICOT56925.2022.10008155}
}

@article{10.1007/s11042-022-12794-3,
  author     = {Jaiswal, Shruti and Nandi, Gora Chand},
  title      = {Optimized, robust, real-time emotion prediction for human-robot interactions using deep learning},
  year       = {2022},
  issue_date = {Feb 2023},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {82},
  number     = {4},
  issn       = {1380-7501},
  url        = {https://doi.org/10.1007/s11042-022-12794-3},
  doi        = {10.1007/s11042-022-12794-3},
  abstract   = {To enable humanoid robots to share the social space,development in technology is required for natural interaction with the robots using multiple modes of communication such as speech, gestures, and share emotions with them. This research is targeted towards addressing the core issue of emotion recognition problem, which would require fewer computation resources and a much lesser number of network parameters, which will be more adaptive to compute on social robots for real-time communication. Any robots will have limited computation capability for run time actions and decisions. In the present investigation, Inception based Convolution Neural Network(CNN) Architecture is proposed to improve the emotion prediction. The proposed model has achieved improved accuracy of up to 6\% improvement over the existing network architecture for emotion classification. The model was tested over seven different datasets to verify its robustness. In addition, real-time implementation capability is verified on humanoid robot NAO, which depicts its social behavior in real-time. The proposed model is reducing the trainable parameters to the extent of 94\% as compared to vanilla CNN model, which indicates that its implementation ability in a real-time based application such as human-robot interaction. Rigorous experiments have been performed to validate the methodology, which is sufficiently robust and could achieve a high level of accuracy. Seven datasets are used to build a robust model. Finally, the model is integrated in a humanoid robot, NAO, in real-time. When averaged over all the emotions, the reduction in response time by 60\% and 61\% and improvement in prediction rate by 42\% and 21\% when compared in real-time environment with Vanilla CNN and state of the art model respectively.},
  journal    = {Multimedia Tools Appl.},
  month      = apr,
  pages      = {5495–5519},
  numpages   = {25},
  keywords   = {Emotion classification, Optimized deep learning, Convolution neural network, Inception module, Human-robot interaction, Social robotics}
}

@inproceedings{9900581,
  author    = {Shenoy, Sudhir and Jiang, Yusheng and Lynch, Tyler and Manuel, Lauren Isabelle and Doryab, Afsaneh},
  booktitle = {2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  title     = {A Self Learning System for Emotion Awareness and Adaptation in Humanoid Robots},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {912-919},
  keywords  = {Emotion recognition;Adaptation models;Adaptive learning;Transfer learning;Humanoid robots;Real-time systems;Data models;emotion recognition;engagement;user-adaptive;social robot;deep learning},
  doi       = {10.1109/RO-MAN53752.2022.9900581}
}

@article{9982640,
  author   = {Hwang, Chih-Lyang and Deng, Yu-Chen and Pu, Shih-En},
  journal  = {IEEE Access},
  title    = {Human–Robot Collaboration Using Sequential-Recurrent-Convolution-Network-Based Dynamic Face Emotion and Wireless Speech Command Recognitions},
  year     = {2023},
  volume   = {11},
  number   = {},
  pages    = {37269-37282},
  keywords = {Face recognition;Speech recognition;Emotion recognition;Service robots;Human-robot interaction;Image recognition;Collaboration;Human--robot collaboration;CNN;LSTM;human and face detection;dynamic face emotion recognition;wireless speech command recognition;omnidirectional service robot;visual searching and tracking;adaptive stratified finite-time saturated control.},
  doi      = {10.1109/ACCESS.2022.3228825}
}

@inproceedings{8588580,
  author    = {Mazzoni Ranieri, Caetano and Vicentim Nardari, Guilherme and Moreira Pinto, Adam Henrique and Carnieto Tozadore, Daniel and Francelin Romero, Roseli Aparecida},
  booktitle = {2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)},
  title     = {LARa: A Robotic Framework for Human-Robot Interaction on Indoor Environments},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {376-382},
  keywords  = {Robot sensing systems;Face;Speech recognition;Face recognition;Human-robot interaction;Human Robot Interaction;Social Robot;Control Architecture;Robotic Framework},
  doi       = {10.1109/LARS/SBR/WRE.2018.00074}
}

@article{8760246,
  author   = {Li, Tzuu-Hseng S. and Kuo, Ping-Huan and Tsai, Ting-Nan and Luan, Po-Chien},
  journal  = {IEEE Access},
  title    = {CNN and LSTM Based Facial Expression Analysis Model for a Humanoid Robot},
  year     = {2019},
  volume   = {7},
  number   = {},
  pages    = {93998-94011},
  keywords = {Emotion recognition;Feature extraction;Face recognition;Image recognition;Task analysis;Humanoid robots;Convolutional neural network;long short-term memory;transfer learning;facial expression analysis},
  doi      = {10.1109/ACCESS.2019.2928364}
}

@inproceedings{8578328,
  author    = {Marinoiu, Elisabeta and Zanfir, Mihai and Olaru, Vlad and Sminchisescu, Cristian},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {2158-2167},
  keywords  = {Medical treatment;Autism;Three-dimensional displays;Videos;Cameras;Robot vision systems},
  doi       = {10.1109/CVPR.2018.00230}
}

@inproceedings{Ashok2022-xp,
  title      = {Paralinguistic cues in speech to adapt robot behavior in
                human-robot interaction},
  booktitle  = {2022 9th {IEEE} {RAS/EMBS} International Conference for
                Biomedical Robotics and Biomechatronics ({BioRob})},
  author     = {Ashok, Ashita and Pawlak, Jakub and Paplu, Sarwar and
                Zafar, Zuhair and Berns, Karsten},
  publisher  = {IEEE},
  month      = aug,
  year       = 2022,
  conference = {2022 9th IEEE RAS/EMBS International Conference for
                Biomedical Robotics and Biomechatronics (BioRob)},
  location   = {Seoul, Korea, Republic of}
}

@article{Augello2022-hm,
  title     = {Multimodal mood recognition for assistive scenarios},
  author    = {Augello, Agnese and Bella, Giulia Di and Infantino, Ignazio and
               Pilato, Giovanni and Vitale, Gianpaolo},
  journal   = {Procedia Comput. Sci.},
  publisher = {Elsevier BV},
  volume    = 213,
  pages     = {510--517},
  year      = 2022,
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  language  = {en}
}

@article{Khan2023-nz,
  title     = {Improved multi-lingual sentiment analysis and recognition using
               deep learning},
  author    = {Khan, Amjad},
  abstract  = {Speech emotion recognition (SER) is still a fresh in natural
               language processing domain since the accuracy is beyond
               targeted. Mainly due to real-time applications such as
               human--robot interaction, human behaviour evaluation and virtual
               reality rely heavily on SER. Moreover, cross-lingual SER plays a
               significant role in practical applications, especially when
               users of different cultural and linguistic backgrounds interact
               with the system. However, the existing conventional approaches
               of SER cannot be employed for real-world applications because it
               uses the same corpus for training and testing, which cannot be
               used for multi-lingual environments to detect or classify real
               emotions. In such a situation, the performance of SER is
               degraded. Therefore, the proposed work develops cross-lingual
               emotion recognition through Urdu, Italian, English and German.
               The features are extracted through the most employed audio
               feature known as MFCCs (Mel Frequency Cepstral Coefficients).
               Experimental results exhibited that the proposed deep learning
               model comes out with promising results on the URDU data set with
               91.25\% accuracy using random forest (RF) and XGBoost
               classifier.},
  journal   = {J. Inf. Sci.},
  publisher = {SAGE Publications},
  pages     = {016555152211372},
  month     = jan,
  year      = 2023,
  language  = {en}
}

@inproceedings{yang2016wider,
  author    = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {WIDER FACE: A Face Detection Benchmark},
  year      = {2016}
}

@inproceedings{BarsoumICMI2016,
  title     = {Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution},
  author    = {Barsoum, Emad and Zhang, Cha and Canton Ferrer, Cristian and Zhang, Zhengyou},
  booktitle = {ACM International Conference on Multimodal Interaction (ICMI)},
  year      = {2016}
}

@inproceedings{5543262,
  author    = {Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
  booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops},
  title     = {The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {94-101},
  keywords  = {Databases;Gold;Active appearance model;Support vector machines;Support vector machine classification;Face detection;Testing;Performance evaluation;Measurement;Code standards},
  doi       = {10.1109/CVPRW.2010.5543262}
}

@inproceedings{Ali2021-ie,
  title      = {Mel frequency cepstral coefficient: A review},
  booktitle  = {Proceedings of the 2nd International Conference on {ICT}
                for Digital, Smart, and Sustainable Development, {ICIDSSD}
                2020, 27-28 February 2020, Jamia Hamdard, New Delhi, India},
  author     = {Ali, Shalbbya and Tanweer, Safdar and Khalid, Syed and
                Rao, Naseem},
  publisher  = {EAI},
  year       = 2021,
  conference = {Proceedings of the 2nd International Conference on ICT for
                Digital, Smart, and Sustainable Development, ICIDSSD 2020,
                27-28 February 2020, Jamia Hamdard, New Delhi, India},
  location   = {New Delhi, India}
}

@inproceedings{Shi2016-th,
  title      = {Robust speaker recognition based on improved {GFCC}},
  booktitle  = {2016 2nd {IEEE} International Conference on Computer and
                Communications ({ICCC})},
  author     = {Shi, Xiaoyuan and Yang, Haiyan and Zhou, Ping},
  publisher  = {IEEE},
  month      = oct,
  year       = 2016,
  conference = {2016 2nd IEEE International Conference on Computer and
                Communications (ICCC)},
  location   = {Chengdu, China}
}

@article{OShaughnessy1988-ws,
  title     = {Linear predictive coding},
  author    = {O'Shaughnessy, D},
  journal   = {IEEE Potentials},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume    = 7,
  number    = 1,
  pages     = {29--32},
  month     = feb,
  year      = 1988,
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@inproceedings{6895780,
  author    = {Li, Penghua and Hu, Fangchao and Li, Yinguo and Xu, Yang},
  booktitle = {Proceedings of the 33rd Chinese Control Conference},
  title     = {Speaker identification using linear predictive cepstral coefficients and general regression neural network},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {4952-4956},
  keywords  = {Speech;Vectors;Partitioning algorithms;Training;Neural networks;Algorithm design and analysis;Cepstrum;General regression neural network;Speaker identification;Linear predictive cepstrum coefficients;Non-linear partition algorithm},
  doi       = {10.1109/ChiCC.2014.6895780}
}

@book{Picard2000-mt,
  title     = {Affective Computing},
  author    = {Picard, Rosalind W},
  publisher = {MIT Press},
  series    = {The MIT Press},
  month     = jul,
  year      = 2000,
  address   = {London, England}
}

@article{Goodfellow2013-al,
  title         = {Challenges in Representation Learning: A report on three
                   machine learning contests},
  author        = {Goodfellow, Ian J and Erhan, Dumitru and Carrier, Pierre Luc
                   and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and
                   Cukierski, Will and Tang, Yichuan and Thaler, David and Lee,
                   Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng,
                   Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis,
                   Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park,
                   John and Ionescu, Radu and Popescu, Marius and Grozea,
                   Cristian and Bergstra, James and Xie, Jingjing and Romaszko,
                   Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
  abstract      = {The ICML 2013 Workshop on Challenges in Representation
                   Learning focused on three challenges: the black box learning
                   challenge, the facial expression recognition challenge, and
                   the multimodal learning challenge. We describe the datasets
                   created for these challenges and summarize the results of
                   the competitions. We provide suggestions for organizers of
                   future challenges and some comments on what kind of
                   knowledge can be gained from machine learning competitions.},
  month         = jul,
  year          = 2013,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  eprint        = {1307.0414}
}

@article{Mollahosseini2017-bj,
  title         = {{AffectNet}: A database for facial expression, valence, and
                   arousal computing in the wild},
  author        = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H},
  abstract      = {Automated affective computing in the wild setting is a
                   challenging problem in computer vision. Existing annotated
                   databases of facial expressions in the wild are small and
                   mostly cover discrete emotions (aka the categorical model).
                   There are very limited annotated facial databases for
                   affective computing in the continuous dimensional model
                   (e.g., valence and arousal). To meet this need, we
                   collected, annotated, and prepared for public distribution a
                   new database of facial emotions in the wild (called
                   AffectNet). AffectNet contains more than 1,000,000 facial
                   images from the Internet by querying three major search
                   engines using 1250 emotion related keywords in six different
                   languages. About half of the retrieved images were manually
                   annotated for the presence of seven discrete facial
                   expressions and the intensity of valence and arousal.
                   AffectNet is by far the largest database of facial
                   expression, valence, and arousal in the wild enabling
                   research in automated facial expression recognition in two
                   different emotion models. Two baseline deep neural networks
                   are used to classify images in the categorical model and
                   predict the intensity of valence and arousal. Various
                   evaluation metrics show that our deep neural network
                   baselines can perform better than conventional machine
                   learning methods and off-the-shelf facial expression
                   recognition systems.},
  month         = aug,
  year          = 2017,
  copyright     = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  eprint        = {1708.03985}
}

@inproceedings{jaffe,
  author = {Alshamsi, Humaid and Këpuska, Veton and Meng, Hongying},
  year   = {2017},
  month  = {11},
  pages  = {},
  title  = {Real Time Automated Facial Expression Recognition App Development on Smart Phones},
  doi    = {10.1109/IEMCON.2017.8117150}
}

@misc{Lundqvist2015-in,
  title     = {Karolinska Directed Emotional Faces},
  author    = {Lundqvist, D and Flykt, A and {\"O}hman, A},
  publisher = {American Psychological Association (APA)},
  month     = may,
  year      = 2015,
  note      = {Title of the publication associated with this dataset: PsycTESTS
               Dataset}
}

@article{Livingstone2018-li,
  title     = {The Ryerson {Audio-Visual} Database of Emotional Speech and Song
               ({RAVDESS)}: A dynamic, multimodal set of facial and vocal
               expressions in North American English},
  author    = {Livingstone, Steven R and Russo, Frank A},
  abstract  = {The RAVDESS is a validated multimodal database of emotional
               speech and song. The database is gender balanced consisting of
               24 professional actors, vocalizing lexically-matched statements
               in a neutral North American accent. Speech includes calm, happy,
               sad, angry, fearful, surprise, and disgust expressions, and song
               contains calm, happy, sad, angry, and fearful emotions. Each
               expression is produced at two levels of emotional intensity,
               with an additional neutral expression. All conditions are
               available in face-and-voice, face-only, and voice-only formats.
               The set of 7356 recordings were each rated 10 times on emotional
               validity, intensity, and genuineness. Ratings were provided by
               247 individuals who were characteristic of untrained research
               participants from North America. A further set of 72
               participants provided test-retest data. High levels of emotional
               validity and test-retest intrarater reliability were reported.
               Corrected accuracy and composite ``goodness'' measures are
               presented to assist researchers in the selection of stimuli. All
               recordings are made freely available under a Creative Commons
               license and can be downloaded at
               https://doi.org/10.5281/zenodo.1188976.},
  journal   = {PLoS One},
  publisher = {Public Library of Science (PLoS)},
  volume    = 13,
  number    = 5,
  pages     = {e0196391},
  month     = may,
  year      = 2018,
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  language  = {en}
}

@inproceedings{1699093,
  author    = {Gunes, H. and Piccardi, M.},
  booktitle = {18th International Conference on Pattern Recognition (ICPR'06)},
  title     = {A Bimodal Face and Body Gesture Database for Automatic Analysis of Human Nonverbal Affective Behavior},
  year      = {2006},
  volume    = {1},
  number    = {},
  pages     = {1148-1153},
  keywords  = {Databases;Data analysis;Emotion recognition;Face recognition;Computer vision;Pattern recognition;Human computer interaction;Face detection;Australia;System testing},
  doi       = {10.1109/ICPR.2006.39}
}

@article{Singh2020-ui,
  title     = {Face recognition using {HOG} feature extraction and {SVM}
               classifier},
  author    = {Singh, Swarnima and Singh, Durgesh and Yadav, Vikash},
  journal   = {Int. J. Emerg. Trends Eng. Res.},
  publisher = {The World Academy of Research in Science and Engineering},
  volume    = 8,
  number    = 9,
  pages     = {6437--6440},
  month     = sep,
  year      = 2020
}

@article{Shorten2019-mj,
  title     = {A survey on image data augmentation for deep learning},
  author    = {Shorten, Connor and Khoshgoftaar, Taghi M},
  abstract  = {Deep convolutional neural networks have performed remarkably
               well on many Computer Vision tasks. However, these networks are
               heavily reliant on big data to avoid overfitting. Overfitting
               refers to the phenomenon when a network learns a function with
               very high variance such as to perfectly model the training data.
               Unfortunately, many application domains do not have access to
               big data, such as medical image analysis. This survey focuses on
               Data Augmentation, a data-space solution to the problem of
               limited data. Data Augmentation encompasses a suite of
               techniques that enhance the size and quality of training
               datasets such that better Deep Learning models can be built
               using them. The image augmentation algorithms discussed in this
               survey include geometric transformations, color space
               augmentations, kernel filters, mixing images, random erasing,
               feature space augmentation, adversarial training, generative
               adversarial networks, neural style transfer, and meta-learning.
               The application of augmentation methods based on GANs are
               heavily covered in this survey. In addition to augmentation
               techniques, this paper will briefly discuss other
               characteristics of Data Augmentation such as test-time
               augmentation, resolution impact, final dataset size, and
               curriculum learning. This survey will present existing methods
               for Data Augmentation, promising developments, and meta-level
               decisions for implementing Data Augmentation. Readers will
               understand how Data Augmentation can improve the performance of
               their models and expand limited datasets to take advantage of
               the capabilities of big data.},
  journal   = {J. Big Data},
  publisher = {Springer Science and Business Media LLC},
  volume    = 6,
  number    = 1,
  month     = dec,
  year      = 2019,
  copyright = {https://creativecommons.org/licenses/by/4.0},
  language  = {en}
}

@misc{openai2024,
  author       = {OpenAI},
  title        = {{C}hat{G}{P}{T} {M}odels},
  howpublished = {\url{https://platform.openai.com/docs/models/o1}},
  year         = {},
  note         = {[Accessed 22-10-2024]}
}