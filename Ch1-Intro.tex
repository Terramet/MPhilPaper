\chapter{Introduction}

Emotion recognition plays a critical role in Human-Robot Interaction (HRI) by enabling robots to better understand and respond to human emotional states. This capacity is essential for creating more natural, empathetic, and socially appropriate interactions, as emotional cues are fundamental to human communication \cite{Castellano2009-cv}. When robots can recognise emotions, they can adjust their tone, behaviour, or responses to match the user's affective state \cite{Breazeal2003-sa}, thereby improving user experience and engagement particularly in sensitive domains such as healthcare and education.

In healthcare, robots capable of recognising patient emotions can offer better support and improve outcomes for groups such as individuals with autism, mental health challenges, or elderly populations \cite{Dautenhahn2007-wl}. Emotional awareness allows for personalised, adaptive interaction, and can help robots respond more appropriately to shifts in mood or stress levels \cite{Dhuheir2021-ii}.

However, existing emotion recognition systems often rely on a single input modality, such as facial expressions or speech. These approaches are inherently limited: facial-based systems fail when faces are obscured or out of frame, while audio-based systems can be disrupted by noise or speech variation. These limitations present challenges to real-world deployment, especially in unstructured environments.

To address these limitations, researchers have explored multimodal emotion recognition, which combines data from multiple sources, typically facial expressions, vocal intonation, and gestures, to achieve greater accuracy and robustness. While true multimodal fusion, where data streams are tightly integrated at the algorithmic level, has shown promise in lab settings, it also introduces high computational complexity and integration challenges that limit its use on resource-constrained robotic platforms.

This thesis is motivated by the need to enhance emotion-aware interactions on widely available, resource-limited robots, which often lack the computational power or hardware needed for complex multimodal fusion. 

The aim of this work is to evaluate and implement individual unimodal emotion recognition channels, specifically facial emotion recognition and sentiment-based emotion recognition, and to assess their effectiveness and feasibility in real-world HRI contexts. Rather than developing a fused multimodal system, this thesis focuses on understanding the strengths, weaknesses, and trade-offs of each modality when applied independently on low-power, real-time robotic systems. To achieve this, the research objectives are as follows: review current literature on facial, audio, gesture-based, and multimodal emotion recognition methods, implement and evaluate facial emotion recognition methods suitable for low-resource environments, explore the use of text-based sentiment analysis using existing cloud-based tools, and assess the performance and suitability of each method on a resource-limited robot platform, considering constraints such as computational load and response time.

The literature review chapter presents a comprehensive survey of emotion recognition methods across various modalities, including facial expressions, gestures, speech, and multimodal systems. This chapter highlights the strengths, limitations, and applicability of each approach within the context of human-robot interaction, with special attention given to systems designed for resource-constrained environments. It also considers the role of more invasive techniques, such as EEG, addressing their practical implications.

Materials and Methods outlines the technical setup used throughout this study. It describes the robotic platform, datasets, and the implementation of emotion recognition systems across both facial and audio modalities. The chapter also details the selection and configuration of software libraries and tools, emphasising their suitability for low-power robotic platforms and real-time processing constraints.

Next the results chapter combines the evaluation of both facial and sentiment analysis systems. It presents a comparative analysis of various face detection and classification algorithms, as well as response times for the sentiment analysis. The performance of each system is assessed in terms of accuracy, computational efficiency, and suitability for deployment in resource-limited environments. This chapter also includes reflections on the challenges encountered during implementation and testing.

Chapter 5, Discussion, interprets the results presented in the previous chapter, analysing their significance in relation to the research objectives. It explores the trade-offs between accuracy and efficiency, the practical challenges of integrating emotion recognition into real-world robotic systems, and the implications of using unimodal rather than multimodal approaches.

The conclusion summarises the key findings of the thesis and reflects on how the research meets its stated aims and objectives. It evaluates the effectiveness of the unimodal systems developed, and identifies areas for improvement, including model performance, hardware integration, and user validation. The chapter concludes by proposing directions for future research, such as lightweight multimodal fusion strategies and extended testing in real-world HRI scenarios.