\chapter{Introduction}

Emotion recognition plays a vital role in enhancing Human-Robot Interaction (HRI) by allowing robots to interpret and respond to the emotional states of humans in a socially intelligent manner. Effective emotional responsiveness is critical for creating natural, engaging, and empathetic interactions between humans and robots, particularly in environments such as healthcare, education, and assistive living. In these contexts, robots that can detect and appropriately react to human emotions are better equipped to provide personalised support, improve user experience, and build trust with users \cite{Breazeal2003-sa, Castellano2009-cv}.

Despite advances in emotion recognition technologies, many current systems still rely on a single mode of input, such as facial expressions, vocal cues, gestures or sentiment analysis. This single-modality approach limits robustness. For instance, facial emotion recognition (FER) may fail when the user's face is partially obscured or not facing the camera, while audio-based systems may struggle in noisy environments or when speech is unavailable. These limitations reduce the reliability and flexibility of such systems in real-world conditions.

To address this gap, this thesis aims to support the development of more resilient emotion recognition systems by evaluating the use of both facial and textual-based modalities in parallel. The core aim of this research is not to fuse these channels into a single multimodal pipeline, but to explore how each modality can individually contribute to improving the adaptability and performance of emotion recognition on robots operating in dynamic environments.

The objectives of this research are threefold. First, to evaluate and compare the performance of different face detection and facial emotion recognition models in terms of accuracy and inference time, particularly on resource-constrained robotic hardware. Second, to assess the potential of audio-based emotion recognition using off-the-shelf tools such as IBM Watson and OpenSMILE. Third, to investigate how using these two modalities side by side can enhance the flexibility of emotion-aware systems, allowing one modality to compensate when the other is unavailable or unreliable.

The objectives of this research are threefold. First, to evaluate and compare the performance of different face detection and facial emotion recognition models in terms of accuracy and inference time, particularly on resource-constrained robotic hardware. Second, to assess the potential of text-based sentiment analysis as an accessible form of audio-related emotion recognition, using off-the-shelf tools such as IBM Watson. Third, to explore the complementary use of facial emotion recognition and sentiment analysis as separate but parallel modalities, highlighting their individual strengths and practical roles in developing more flexible emotion-aware robotic systems.

The Materials and Methods chapter details the robotic platform used in this study. There is then a discussion on implementing emotion detection algorithms, utilising established software libraries and frameworks, and describing the methodologies used for data preparation and system evaluation on the resource-limited robots.

In Chapter 4: Results, the study explores a dual-model approach where separate models are used for face detection and emotion classification. For face detection, various algorithms such as Haar Cascade, Dlib, and more advanced models like Tiny YOLO and YOLO are evaluated. This division of tasks allows for more specialised processing, ensuring that each stage—detecting the face and classifying the emotions—can be optimised independently. The system's performance is carefully analysed in resource-constrained environments, where computational efficiency is critical. By comparing models, the study highlights the trade-offs between accuracy and processing speed, particularly important for systems deployed in low-power devices or real-time applications. 

Continuing futher, Chapter 4 also delves into the use of IBM Watson's capabilities for analysing emotional content in speech, with a focus on sentiment analysis. Using IBM Watson's powerful natural language understanding, this system can perform sentiment analysis detecting emotions such as happiness, sadness, anger, and fear, based on textual input. The chapter assesses Watson's performance in terms of accuracy and speed, determining whether it is suitable for real-time speech emotion detection. Additionally, the role of large language models (LLMs), like ChatGPT, is explored in complementing emotion recognition by enhancing conversational capabilities, allowing for more natural and empathetic interactions. While IBM Watson remains central to the system's speech analysis, the study also investigates other tools like OpenSMILE, known for its advanced feature extraction capabilities in audio signal processing.

The Discussion chapter reflects on the performance of the emotion recognition system by interpreting the results obtained from both facial emotion recognition and sentiment analysis components. The chapter analyses the trade-offs encountered, such as the balance between inference speed and emotion classification accuracy, and considers the practical implications of these findings for real-time human-robot interaction. The chapter also considers each part of the system's and the limitations it encountered and suggests how they might be addressed in future work.

Finally, the Conclusion reflects on how the system meets its core aims and objectives — particularly the development of emotion-aware robotic systems suited to real-world conditions — and outlines future directions, including real-world testing and deeper exploration of on-robot audio analysis.