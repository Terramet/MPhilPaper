\chapter{Introduction}

Emotion recognition plays a critical role in Human-Robot Interaction (HRI) by enabling robots to better understand and respond to the emotional states of humans. This is crucial for fostering natural and intuitive communication between robots and humans, as emotional cues guide social interactions \cite{Castellano2009-cv}. When robots can recognise emotions, they can adapt their behaviour, tone \cite{Breazeal2003-sa}, or responses that make the interaction more engaging, empathetic, and contextually appropriate. This capability is particularly important in healthcare and education where the robot’s ability to sense and respond to emotions can significantly enhance the user experience.

In healthcare, robots that can detect patient emotions are better equipped to provide support and improve outcomes, especially for people with mental health conditions, autism \cite{Dautenhahn2007-wl}, or older people. Emotional awareness helps robots engage with patients more effectively, offering personalised interactions that take into account mood fluctuations or stress \cite{Dhuheir2021-ii}.

Current approaches to emotion recognition face significant challenges, particularly when they rely on a single modality. Systems that analyse only facial expressions may struggle to accurately interpret emotions in cases where facial cues are not visible, such as when individuals are not facing the camera. Similarly, audio-based emotion recognition can be limited by variations in speech patterns, background noise, or linguistic differences, leading to potential inaccuracies.

Multimodal emotion recognition refers to identifying and interpreting human emotions by combining data from multiple sources, such as facial expressions, vocal intonations, and gestures. Unlike traditional methods that rely on a single modality, multimodal approaches offer a more comprehensive and nuanced understanding of emotional states. By integrating different types of data, these systems can achieve greater accuracy and reliability in emotion detection, overcoming the limitations inherent in relying on just one mode of input.

A more comprehensive and robust system integrating visual and auditory inputs would benefit the field. By combining face detection with audio emotion recognition, it becomes possible to cross-validate and reinforce the emotional insights derived from each modality. This multimodal approach makes the system more adaptable to real-world conditions, where emotions are often expressed through a complex interplay of visual and auditory signals \cite{R2024-jk}.

This thesis explores the integration of facial and textual data to create a more robust emotion recognition system tailored for the more widely available robots which tend to be resource-limited, beginning with a comprehensive review of existing literature on emotion recognition, focusing on the challenges and advancements in gesture recognition, facial recognition, audio/text recognition, and multimodal approaches. In addition, a critical assessment of the role of invasive technologies such as EEG in emotion detection is performed, considering their ethical implications and practical limitations.

The Materials and Methods chapter details the robotic platform used in this study. There is then a discussion on implementing emotion detection algorithms, utilising established software libraries and frameworks, and describing the methodologies used for data preparation and system evaluation on the resource-limited robots.

In Chapter 4: Facial Emotion Recognition, the study explores a dual-model approach where separate models are used for face detection and emotion classification. For face detection, various algorithms such as Haar Cascade, Dlib, and more advanced models like Tiny YOLO and YOLO are evaluated. This division of tasks allows for more specialised processing, ensuring that each stage—detecting the face and classifying the emotions—can be optimised independently. The system's performance is carefully analysed in resource-constrained environments, where computational efficiency is critical. By comparing models, the study highlights the trade-offs between accuracy and processing speed, particularly important for systems deployed in low-power devices or real-time applications. 

Chapter 5: Audio Emotion Recognition delves into the use of IBM Watson's capabilities for analysing emotional content in speech, with a focus on sentiment analysis. Using IBM Watson's powerful natural language understanding, this system can perform sentiment analysis detecting emotions such as happiness, sadness, anger, and fear, based on textual input. The chapter assesses Watson's performance in terms of accuracy and speed, determining whether it is suitable for real-time speech emotion detection. Additionally, the role of large language models (LLMs), like ChatGPT, is explored in complementing emotion recognition by enhancing conversational capabilities, allowing for more natural and empathetic interactions. While IBM Watson remains central to the system's speech analysis, the study also investigates other tools like OpenSMILE, known for its advanced feature extraction capabilities in audio signal processing.

The Conclusion chapter provides a comprehensive summary of the research, highlighting the key findings and contributions of the multimodal emotion recognition system. Reflecting on the effectiveness of the combined facial and audio emotion detection approaches, highlighting the strengths and limitations of the models used in both modalities. In addition, the chapter offers insights into areas for future improvement. These include enhancing emotion detection accuracy, optimising computational efficiency, and exploring deeper integration of large language models for more natural and responsive interactions. The conclusion also outlines possible future directions, such as user studies to validate the system's performance and further development of multimodal fusion techniques to achieve even more accurate emotion recognition.