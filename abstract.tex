
\begin{abstract}
This thesis explores the development and evaluation of a multimodal emotion recognition system, integrating both Facial Emotion Recognition (FER) and Audio Emotion Recognition (AER). The FER system employs various face detection algorithms, including Tiny YOLO, YOLO, dlib, and Haar Cascade, followed by emotion recognition models such as MobileNetV2, ResNet50, and VGG16, trained using transfer learning. These models are assessed for both accuracy and efficiency, and their real-time performance is tested on a robotic platform. For AER, off-the-shelf tools like OpenSMILE are considered alongside sentiment analysis systems such as IBM Watson and large language models, to evaluate emotion detection through vocal input. Both modalities are evaluated on metrics such as response time and emotion recognition accuracy, with a particular focus on their real-world applicability when deployed on a physical robot. The study also discusses the trade-off between accuracy and inference speed, highlighting the most effective combinations of models. Future work includes testing the system in live, uncontrolled environments with human participants, implementing on-robot AER systems, and exploring the fusion of facial and audio data for enhanced emotional understanding. These efforts aim to further optimise the system for real-time interaction and improved emotion recognition in human-robot interaction scenarios.\\

    \noindent
    \textit{Keywords}: Facial Emotion Recognition, Audio Emotion Recognition, Social Robot, Multimodal Emotion Recognition.
\end{abstract}

\vspace{5cm}
\begin{center}
\vspace{2cm}
\textbf {Author}\\ Joshua Bamforth \\
\vspace{2cm}
{\textbf {Supervisory team:} Prof. Alessandro Di Nuovo, Dr. Jing Wang}    
\end{center}
